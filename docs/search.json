[
  {
    "objectID": "syllabus.html",
    "href": "syllabus.html",
    "title": "STAT 155: Introduction to Statistical Modeling",
    "section": "",
    "text": "Section 06: M/W/F 09:40-10:40am, THTR 203\nSection 07: M/W/F 10:50-11:50pm, THTR 203\nWelcome to STAT 155! Whether in life or research, we‚Äôre often interested in relationships between 2+ variables. For example, how is one‚Äôs commute time to class related to their distance from campus and mode of transportation? Or, how is voter participation related to a person‚Äôs age and political affiliation? Statistical modeling is the art and science of turning data into information about such relationships of interest.\nBeing able to summarize, interpret, and communicate about data are crucial for navigating today‚Äôs information landscape, and these are precisely the skills that we‚Äôll build in this class. Throughout the semester, we‚Äôll study the fundamental methods that statisticians use to extract knowledge from data, emphasizing statistical literacy & intuition, real data applications, and modern computing over memorizing facts and formulas."
  },
  {
    "objectID": "syllabus.html#important-technical-concepts",
    "href": "syllabus.html#important-technical-concepts",
    "title": "STAT 155: Introduction to Statistical Modeling",
    "section": "Important technical concepts",
    "text": "Important technical concepts\nUpon completion of this course, students should be able to:\n\nBuild, use, and interpret graphical and numerical summaries of data.\nGiven a research question: identify an appropriate model, use sample data to fit the model in RStudio (free!), evaluate the model‚Äôs quality, and quantify our uncertainty in the model‚Äôs coefficients and predictions.\nUse a sample model to make predictions & inferences about a population, using prediction/confidence intervals & hypothesis tests.\nInterpret & communicate an analysis in context & using appropriate notation, argumentation, & evidence.\nDescribe potential advantages, limitations, and ethical considerations of a data set and statistical analysis.\nIdentify common pitfalls in statistical analyses (e.g., spurious correlation vs.¬†causal relationships, extrapolation, multicollinearity, multiple testing, practical vs.¬†statistical significance).\nAccurately describe methods and results in a way that is scientifically sound and widely accessible.\nWork productively and effectively in a group setting."
  },
  {
    "objectID": "syllabus.html#important-statistical-skills",
    "href": "syllabus.html#important-statistical-skills",
    "title": "STAT 155: Introduction to Statistical Modeling",
    "section": "Important statistical skills",
    "text": "Important statistical skills\nThe following skills are essential both within and beyond Statistics, and demonstrably improve your own learning and the learning of those around you:\n\nMove beyond a ‚Äúhomework only‚Äù study approach. Develop a deeper understanding of the material through continued review, reflection, and practice.\nThink creatively, and build confidence, applying course concepts in open-ended, novel settings.\nBe comfortable working through challenges and mistakes.\nContribute to a welcoming and engaged learning environment.\nWork effectively in a group setting."
  },
  {
    "objectID": "syllabus.html#about-your-professor",
    "href": "syllabus.html#about-your-professor",
    "title": "STAT 155: Introduction to Statistical Modeling",
    "section": "About your professor",
    "text": "About your professor\n\nMd Mutasim Billah, PhD\nPronunciation: listen here\nOffice: Olin-Rice 234\nEmail: mbillah@macalester.edu\n\n\n\n\n\n\nNotes from ‚Äúyour professor‚Äù\n\n\n\nGreetings! You can call me Bill or Professor Billah & I use he/him pronouns. Back when I was an undergrad student, I didn‚Äôt have the best experience in Intro Stat‚Äîthose courses often emphasized formulas over real understanding. That experience has shaped my teaching‚ÄîI concentrate on illustrating how statistical theories connect and can be applied in the real world. I‚Äôm excited to teach STAT 155 and to create a more meaningful experience‚Äîone that helps all students feel confident applying it beyond the classroom. My methodological research lies at the intersection of statistical genetics, biostatistics, and genomics. My current research interests include developing novel statistical methods and computationally efficient bioinformatics tools, leveraging modern machine- and deep-learning approaches analyze high-dimensional next-generation sequencing and multi-omics data to identify genes and regulatory mechanisms underlying complex diseases. Outside of my academic work, I enjoy spending time outdoors with family and friends or cooking variety of foods. If you can‚Äôt find me anywhere, I might be busy playing soccer or exploring new worlds on my PS5 Pro!\n\n\nOffice hours:\n\nLocation: My office (OLRI 234) and over zoom, password: 123456\nTimes: M/W: 12:00pm - 12:30pm (in-person), T/TR: 2pm - 3pm (Over zoom, password: 123456)\nBy Appointment: I‚Äôm also happy to meet one-on-one if my normal drop-in/virtual hours don‚Äôt work for you. Shoot me an email and we can arrange it in-person or over zoom, password: 123456.\nEmail Response Time: I do my best to reply to emails promptly during weekdays. Please note that messages sent after 3:00 pm or on weekends may take longer to receive a response."
  },
  {
    "objectID": "syllabus.html#about-your-preceptors",
    "href": "syllabus.html#about-your-preceptors",
    "title": "STAT 155: Introduction to Statistical Modeling",
    "section": "About your preceptors",
    "text": "About your preceptors\nWe have several wonderful STAT 155 preceptors this semester! Their role is to help students with content questions, assist in the navigation of available resources, advise on studying approaches, and assist with concepts, tools, and skills. Students are accountable for their own learning; as such, preceptors are not allowed to share answers to assignments (unless specifically directed by the instructor), they are not expected to immediately know the right approach to an exercise, and they do not provide assistance outside of office hours.\nIn hiring preceptors, we prioritize and emphasize kindness and respect. I expect the same of students in their interactions with the preceptors. Please utilize and respect their experience and commitment to supporting you in this course. Please check out some additional guidelines and expectations on how to interact with preceptors."
  },
  {
    "objectID": "syllabus.html#textbook-online-course-manual",
    "href": "syllabus.html#textbook-online-course-manual",
    "title": "STAT 155: Introduction to Statistical Modeling",
    "section": "Textbook & Online Course Manual",
    "text": "Textbook & Online Course Manual\nThere is no required textbook for this course. Throughout the course, readings may be assigned from these notes, or other sources.\nThe online course manual includes all in-class activities (with solutions).\nSee the rough schedule of the course, which is subject to change as needed."
  },
  {
    "objectID": "syllabus.html#moodle",
    "href": "syllabus.html#moodle",
    "title": "STAT 155: Introduction to Statistical Modeling",
    "section": "Moodle",
    "text": "Moodle\nMoodle includes announcements, general resources, a broad course calendar, submission links, feedback, and a forum for student questions."
  },
  {
    "objectID": "syllabus.html#statistical-software",
    "href": "syllabus.html#statistical-software",
    "title": "STAT 155: Introduction to Statistical Modeling",
    "section": "Statistical software",
    "text": "Statistical software\nWe will use the (completely free and open source) R programming language extensively throughout this course. RStudio (an interface for R) will facilitate our use of R. You may use RStudio in one of two ways:\n\nDesktop version: Download for Windows or Mac at https://posit.co/downloads/. Note: You first need to download and install R on your computer in order to use the desktop version of RStudio\nOnline: Go to https://rstudio.macalester.edu, and log in with your full Mac email address and your usual Mac password to get access. (Note that this is a shared resource across MSCS, and you may experience performance issues due to high traffic, server outages, etc.)\n\nMore detailed instructions on downloading, installing, and getting started with R, and RStudio are available on the R Resources tab."
  },
  {
    "objectID": "syllabus.html#office-hours-oh-and-r-support",
    "href": "syllabus.html#office-hours-oh-and-r-support",
    "title": "STAT 155: Introduction to Statistical Modeling",
    "section": "Office hours (OH) and R Support",
    "text": "Office hours (OH) and R Support\nOH: Across the instructor and preceptors, there are several office hours each week. Names, times, and locations are on the Moodle course calendar. IMPORTANT: Always check the calendar before attending OH.\nData & R Support: In addition to the course preceptors, there is support on campus for working with data and R / RStudio. This is a great resource for R setup and troubleshooting throughout the semester. See https://www.macalester.edu/mscs/data-support for more information."
  },
  {
    "objectID": "syllabus.html#asking-questionscommunicating",
    "href": "syllabus.html#asking-questionscommunicating",
    "title": "STAT 155: Introduction to Statistical Modeling",
    "section": "Asking questions/communicating",
    "text": "Asking questions/communicating\n\nOffice Hours\nOH are a great place to chat about the course, career planning, life,‚Ä¶ Please visit us!!\n\nOH times & locations are on the Moodle course calendar.\nOH are oriented around group discussion. They are not first come, first served appointments.\nSince it‚Äôs not an effective way to deepen your learning, OH are not a place to sit and do assignments with me or preceptors. It‚Äôs an opportunity to discuss concepts & specific questions.\n\n\n\nMoodle Forum: STAT 155 Discussion Board\nThis forum is where we‚Äôll communicate outside class. Students can post and answer comments / questions there. This is an informal way to converse, ask questions, share info, & connect. Do not rely on receiving responses outside weekdays between 9am & 5pm."
  },
  {
    "objectID": "syllabus.html#what-to-do-when-you-have-a-question-for-me",
    "href": "syllabus.html#what-to-do-when-you-have-a-question-for-me",
    "title": "STAT 155: Introduction to Statistical Modeling",
    "section": "What to do when you have a question for me?",
    "text": "What to do when you have a question for me?\n\nIf it‚Äôs non-private (e.g.¬†about policies, homework (Practice Problems), class activities, etc), you should post it on STAT 155 Discussion Board in Moodle. Remember- collaboration is the KEY!\nIf it‚Äôs personal (e.g.¬†about an absence), email me.\nIt‚Äôs good, professional practice to check whether your question is already answered in the provided resources. For example:\n\nInfo (what to do if you miss class): syllabus\nDue dates: course calendar at the top of Moodle + course schedule in the online manual\nQuiz dates: syllabus + course calendar at the top of Moodle + announcement on Moodle\nHomework policies & grading: homework policies & grading doc\nFinals week: syllabus + course calendar at the top of Moodle + announcement on Moodle"
  },
  {
    "objectID": "syllabus.html#thriving-in-stat-155",
    "href": "syllabus.html#thriving-in-stat-155",
    "title": "STAT 155: Introduction to Statistical Modeling",
    "section": "Thriving in STAT 155",
    "text": "Thriving in STAT 155\n\n\n\n\n\n\nüóìÔ∏è Plan Ahead\n\n\n\nYou should plan to spend ~10-12 hours on any 4-credit course, including class time.1 Stay up-to-date on the course calendar and carve out time for studying & doing homework.\n\n\n\n\n\n\n\n\n‚úÖ Do the Things\n\n\n\nAt minimum, thriving in this course requires the completion of some concrete tasks. Complete all assignments, regularly attend & engage in class, complete in-class activities (which might mean completing work outside of class), and check the activity solutions.\n\n\n\n\n\n\n\n\nüèóÔ∏è Build a Foundation\n\n\n\nIf your main focus is on checking off some boxes, you won‚Äôt get much out of this course (or college in general). Deeper, enduring learning requires more. Carve out time to rewrite, reflect upon, & review your notes. Summarize concepts in your own words.\n\n\n\n\n\n\n\n\nüéâ Engage, Ask Questions, Have Fun\n\n\n\nActively participate in the class & take ownership of your learning. PLEASE: Don‚Äôt be afraid to ask for help, make mistakes, and ask questions! These skills are critical to your well-being & learning. Finally, have some fun, be curious, and reflect upon what surprises you about the material and yourself"
  },
  {
    "objectID": "syllabus.html#flexibility",
    "href": "syllabus.html#flexibility",
    "title": "STAT 155: Introduction to Statistical Modeling",
    "section": "Flexibility",
    "text": "Flexibility\nI provide transparent accommodations to all students. It helps reduce stress and the ‚Äúhidden curriculum‚Äù (not everybody feels comfortable asking for flexibility).\n\nMissed Class: It‚Äôs okay to miss class in the case of an emergency. Please see the ‚ÄòAbsences‚Äô tab below for details.\nPractice Sets (PS): Limited extensions and limited mistakes without penalty. Please carefully go through all the sections from STAT 155 PS Policies & Grading doc.\nCheckpoints: Some class periods will have course videos and readings assigned ahead of time. For each class period where this is the case, a checkpoint quiz (on Moodle) must be completed by 09:00 am. Checkpoints may be attempted as many times as you‚Äôd like, but to earn completion credit for a given checkpoint you must score 100% by your final attempt. These short quizzes are designed to ensure that you stay on top of course material, since much of the content in this course builds on itself.\nQuizzes: Limited revisions. Additional flexibility will be provided in rare extenuating circumstances, upon discussion. Exceptions must be discussed with me (not assumed) early on (not after the fact).\n\n\n\n\n\n\n\nü§ù PLEASE REACH OUT WHEN YOU NEED HELP."
  },
  {
    "objectID": "syllabus.html#absences",
    "href": "syllabus.html#absences",
    "title": "STAT 155: Introduction to Statistical Modeling",
    "section": "Absences",
    "text": "Absences\nIt‚Äôs okay to miss the occasional class. Except in rare extenuating circumstances (which must be discussed in advance): - 3 or fewer absences will not impact your grade - 4-6 absences will impact your grade (see Calculating Final Grades section) - you cannot pass the course if you accrue 7+ absences (more than 25% of class sessions)\n\n\n\n\n\n\nWhat to Do If You Miss Class\n\n\n\n\nüìß Send me a quick email. You do not need to share a reason for your absence, especially if it‚Äôs personal. It‚Äôs just a simple courtesy & keeps communication lines open.\nüìÖ Check the Course Schedule in the online manual for what is happening in class that day.\nüìù Complete the in-class activity on your own & check the solutions posted in the online manual.\nüí¨ Ask any follow-up questions on the Moodle forum or in office hours (OH)."
  },
  {
    "objectID": "syllabus.html#artificial-intelligence-ai",
    "href": "syllabus.html#artificial-intelligence-ai",
    "title": "STAT 155: Introduction to Statistical Modeling",
    "section": "Artificial Intelligence (AI)",
    "text": "Artificial Intelligence (AI)\nUsing AI tools is an emerging skill. You may use AI (ChatGPT, Gemini, Grok, etc), with some caveats & limitations:\n\nAI is often wrong, thus is not a good resource on topics for which you don‚Äôt yet have expertise. Relatedly, though AI can be helpful with parts of a statistical analysis (eg: getting unstuck on code, checking grammar), you have to guide that process (eg: what questions are we trying to answer? what‚Äôs a reasonable approach?).\nWork on an exercise for at least 30 minutes before even thinking about AI. You will learn very little if you overly rely on AI, hence be unprepared for other interactions with the material (eg: in-class discussions, quizzes, future courses that build upon 155, etc). Learning comes from you doing the puzzling, not from you producing a correct answer.\nWhether or not you use AI, you must be able to defend/explain any code/discussion you hand in. You cannot simply use AI to bypass your own learning.\nYou may not use AI to generate entire arguments or discussions. Putting code and discussions into your own words is critical for your own deeper learning, independent thinking, and creativity. (For example, imagine how little you‚Äôd learn in a language course if you simply used AI to translate all text for you!!)\nAny use of AI must be cited, just like any other resource."
  },
  {
    "objectID": "syllabus.html#community-academic-integrity",
    "href": "syllabus.html#community-academic-integrity",
    "title": "STAT 155: Introduction to Statistical Modeling",
    "section": "Community & Academic Integrity",
    "text": "Community & Academic Integrity\nMSCS strives to provide a learning environment that is equitable, inclusive, welcoming, mutually respectful, and free of discrimination. You‚Äôre expected to follow the MSCS Community Guidelines. You‚Äôre also required to be familiar with & follow the college‚Äôs academic integrity & other academic policies. In addition to the examples listed there, academic violations in this course include but are not limited to the following:\n\nUsing any materials from any past STAT 155 course, at Mac or elsewhere. Relatedly, you should not provide any materials to any future 155 students.\nGaining access to, using, or distributing solution sets.\nPassing off others‚Äô work as your own. You must be able to defend / explain all work you hand in.\nUsing AI without citation, to generate entire discussions / code blocks, or without being able to defend the results. Policy violations will result in a score of 0 on the work & be reported to the Asst. Dean of Academic Programs & Advising."
  },
  {
    "objectID": "syllabus.html#engagement",
    "href": "syllabus.html#engagement",
    "title": "STAT 155: Introduction to Statistical Modeling",
    "section": "(1) Engagement",
    "text": "(1) Engagement\nEngagement is important to your own learning & to fostering a supportive learning community.\n\n\n\n\n\n\nüìå Expectations\n\n\n\n\nDo not miss more than 3 in-person class sessions. (4‚Äì6 absences will lower your grade. 7+ absences will result in a D/NC.)\nWhen attending class:\n\nBe on time & don‚Äôt leave early\n\nDo not use your phone (phones must be put away when you enter the course, even if class hasn‚Äôt started)\n\nDo not use your laptop for anything other than taking notes and in-class activities (e.g., no videos, no email, no messaging apps, etc.)\n\nBe actively present (e.g., don‚Äôt work alone, don‚Äôt work on other courses, etc.)\n\nOutside class:\n\nCheck your email for announcements (sent via Moodle) and stay updated on the Moodle forum\n\nWhen you have questions, or just want to chat, please stop by OH!"
  },
  {
    "objectID": "syllabus.html#collaboration",
    "href": "syllabus.html#collaboration",
    "title": "STAT 155: Introduction to Statistical Modeling",
    "section": "(2) Collaboration",
    "text": "(2) Collaboration\nCollaboration improves higher-level thinking, confidence, communication, community, & more. You will work in groups in and outside class. These groups may occasionally switch & may sometimes be assigned.\n\n\n\n\n\n\nü§ù Expectations\n\n\n\n\nFollow the MSCS Community Guidelines\nIn group settings, both in and outside class, you:\n\nUse your group members‚Äô correct names and pronouns\n\nActively contribute to discussions\n\nActively include all other group members in discussion\n\nCreate a space where others feel comfortable making mistakes & sharing their ideas\n\nEffectively communicate with your group about meeting times, etc."
  },
  {
    "objectID": "syllabus.html#preparation-checkpoints",
    "href": "syllabus.html#preparation-checkpoints",
    "title": "STAT 155: Introduction to Statistical Modeling",
    "section": "(3) Preparation (Checkpoints)",
    "text": "(3) Preparation (Checkpoints)\nRoughly half of our class sessions will require some prep work. Before class you will watch videos which introduce new concepts, then take a low-stakes checkpoint quiz (CP). This will help us prepare for class, build a common foundation, & maximize our time together ‚Äì just how readings & reading reflections might be used in another class!\n\n\n\n\n\n\nüìä Expectations\n\n\n\nComplete at least 13 (out of 17) CPs (‚âà80%) without affecting your final grades!\n\n\n\n\n\n\n\n\nüìú Policies\n\n\n\n\nCPs may be attempted as many times as you‚Äôd like, but to earn completion credit for a given checkpoint you must score 100% by your final attempt.\nIf you complete less than 13 (out of 17) CPs (‚âà80%) before the time they are due, your overall course grade will be lowered by 1/3 of a letter grade (i.e.¬†B ‚Äì&gt; B-, A- ‚Äì&gt; B+, etc.).\n\nIf you complete less than 8 (out of 17) CPs (‚âà50%) before the time they are due, your overall course grade will be lowered by 1 of a letter grade (i.e.¬†A ‚Äì&gt; B, etc.).\n\nCPs are due 09:00am on the assigned date. There are no extensions for CPs, as they are important preparation for the relevant class session."
  },
  {
    "objectID": "syllabus.html#practice-practice-sets",
    "href": "syllabus.html#practice-practice-sets",
    "title": "STAT 155: Introduction to Statistical Modeling",
    "section": "(4) Practice (Practice Sets)",
    "text": "(4) Practice (Practice Sets)\nIn 8 practice sets (PS), you will practice and explore the course material in more depth. The following flexibility is built in to help reduce stress and to facilitate deeper learning. Detailed directions and policies are here.\n\nGrading: PP grades will be based on Exercises & Presentation. You can make some mistakes without it chipping away at your score (e.g.¬†you will earn 5/5 points on an exercise if it‚Äôs at least 90% correct & complete).\nExtensions: Limited extensions will be provided.\nSTAT 155 Discussion Board: Use the Moodle board as your first stop for Practice Sets (PS):\n\nAsk first: Post questions about PP on Moodle. Include the problem number, a brief summary of your approach, and where you‚Äôre stuck. For coding, share the entire code chunk for the related problem.\nHelp each other: If you know (or suspect) the answer, reply! Explaining your reasoning helps everyone learn.\nShare alternatives: Multiple correct methods are welcome‚Äîpost yours with explanation.\nShow your work: To earn full collaboration credit, provide complete, well-explained solutions/steps. Partial or unexplained answers may lose points.\nBe professional: Be respectful, cite any resources you used, and write solutions in your own words.\nGoal: work together as a class so everyone can earn the PP points while learning deeply."
  },
  {
    "objectID": "syllabus.html#project-independence-application",
    "href": "syllabus.html#project-independence-application",
    "title": "STAT 155: Introduction to Statistical Modeling",
    "section": "(5) Project (Independence & Application)",
    "text": "(5) Project (Independence & Application)\nMore details will be provided later in the semester. Here are some basics:\n\nWe‚Äôll start working on projects in ~week 6, with the majority of the work happening later in the semester.\nThe projects are collaborative. You will be working in groups. Though you will work in assigned groups at various points throughout the semester, you will pick your own group for the project. This is something to think about as you meet other students in class and learn about common interests.\nProject grades will be based upon a final group written report (no oral presentation), multiple group and individual checkpoints, and individual contributions to the project (thus it‚Äôs possible for different group members to earn different grades)."
  },
  {
    "objectID": "syllabus.html#quizzes-content-expertise",
    "href": "syllabus.html#quizzes-content-expertise",
    "title": "STAT 155: Introduction to Statistical Modeling",
    "section": "(6) Quizzes (Content Expertise)",
    "text": "(6) Quizzes (Content Expertise)\nYour course engagement, collaboration, preparation, practice, and application will support your deeper understanding of the course material. This will be assessed through three in-person quizzes. You must schedule all travel and other commitments around them ‚Äî there will not be any alternative quiz times.\n\nQuiz 1: Monday, 02/16 in class\nQuiz 2: March, 03/30 in class\nQuiz 3: Finals week\n\nThe exam will be written to take ~1.25 hours, but you will have the full 2-hour period to complete it.\n\n09:40‚Äì10:40 am section 06: Sat. 5/9, 8:00am-10:00am in class\n10:50‚Äì11:50 pm section 07: Sat. 5/9, 10:30am-12:30pm in class\n\n\n\n\n\n\n\n\n\nüìú Quiz Policies\n\n\n\n\nAll quizzes will have the following format:\n\nTaken individually, using pen/pencil & paper\n\nYou will not need to write code or use a calculator, but you will need to read & interpret R output\n\nClosed notes, but you may use a 3x5 index card with writing on both sides. These can be handwritten or typed, but you may not include screenshots or share note cards. Making your own card is important to the review process- as you are required to submit the index card along with the answer paper.\n\nQuizzes 2 & 3 will be cumulative. This is unavoidable as the material builds upon itself.\nQuiz corrections:\nYou can earn up to 33% of missed points back on Quizzes 1 & 2 if you:\n\nWrite a reflection of how you prepared for the quiz and where you felt strongest or more uncertain in your understanding before taking the quiz; and\n\nSubmit your quiz corrections along with your reflection to the instructor, no later than one week after quizzes have been handed back.\nNote: Quiz 3 corrections are not allowed due to time constraints at the end of the semester."
  },
  {
    "objectID": "syllabus.html#grading-system",
    "href": "syllabus.html#grading-system",
    "title": "STAT 155: Introduction to Statistical Modeling",
    "section": "Grading system",
    "text": "Grading system\nThe grading system in this course is designed to help you achieve the learning objectives while allowing space to make and learn from mistakes along the way. Your final course grade will consist of three, evenly-weighted components (Quizzes, Practice Problems, and the Project), modified by your progress toward the Engagement, Collaboration, and Preparation (Checkpoint) goals:\n\n\n\nCourse Percentage\n25% Practice Sets +\n50% Quizzes +\n25% Project\n\n\n\nGrade\nCourse percentage\n\n\n\n\nA\n&gt; 93%\n\n\nA-\n87‚Äì93%\n\n\nB+\n84‚Äì87%\n\n\nB\n81‚Äì84%\n\n\nB-\n78‚Äì81%\n\n\nC+\n75‚Äì78%\n\n\nC\n72‚Äì75%\n\n\nC-\n69‚Äì72%\n\n\nD/NC\n&lt; 69% or 7+ absences\n\n\n\n\n\n¬† \n\n\nGrade Modifiers\nEngagement (including attendance) +\nPreparation (checkpoints)\n\n\n\n\n\n\n\nModifier\nScenario\n\n\n\n\nnone (e.g.¬†A ‚Üí A)\nMeets expectations in all two areas (Engagement and Preparation)\n\n\n‚Öì lower grade (e.g.¬†A ‚Üí A-)\nDemonstrates strong progress (e.g., 4 absences OR less than 13 (out of 17) CPs (‚âà80%))\n\n\n1 lower grade (e.g.¬†A ‚Üí B)\nDemonstrates moderate progress (e.g., 5‚Äì6 absences OR less than 8 (out of 17) CPs (‚âà50%))\n\n\n&gt;1 lower grade\nDemonstrates little progress toward expectations in two areas\n\n\nDrop to D/NC\nHas 7+ absences\n\n\n\nNOTE: The table presents general scenarios. Please reach out if you want to discuss progress in Engagement and/or Preparation.\n\n\n\n\n\n\n\n\n\nüìä Grading Caveats\n\n\n\n\nThe goal of sharing this specific information is to provide transparency around final grades, hence clear goals to work toward. That said, assigning grades is much more nuanced than any grading rubric / framework might suggest (for good reasons). What‚Äôs shared here is a worst case scenario ‚Äì it represents the lowest a grade might be if you meet the corresponding goals.\nMoodle does NOT correctly weight your grades, thus should not be used alone to monitor your progress."
  },
  {
    "objectID": "syllabus.html#what-to-do-if-you-miss-class",
    "href": "syllabus.html#what-to-do-if-you-miss-class",
    "title": "STAT 155: Introduction to Statistical Modeling",
    "section": "What to Do If You Miss Class",
    "text": "What to Do If You Miss Class\nIf you do miss class, I expect you to complete any in-class activities on your own. Check the solutions in the online manual and come to office hours with any follow-up questions."
  },
  {
    "objectID": "syllabus.html#late-work-on-practice-problems-pps",
    "href": "syllabus.html#late-work-on-practice-problems-pps",
    "title": "STAT 155: Introduction to Statistical Modeling",
    "section": "Late work on Practice Problems (PPs)",
    "text": "Late work on Practice Problems (PPs)\nThroughout the semester, you may use up to three, three-day extensions. These three extensions can be used on Practice Problems only, not quizzes. The purpose of deadlines (and extensions) are to keep you accountable for your own learning, to keep you on track with the pace of the course (which builds upon itself throughout the semester), and to provide preceptors and myself with the ability to provide you with timely feedback on assignments. Since the Problem Sets are due roughly every two weeks, you must begin working on them early if you want to succeed.\nExtensions can be used automatically, without letting me know in advance. The Moodle dropboxes for assignments will close exactly 3 days after the original deadline (i.e.¬†Mondays at 11:59pm), and I will not accept work submitted after that point unless there are extenuating circumstances that you have communicated with me about ahead of the original deadline. If you email me a completed assignment after a 3-day extension is up, I may have the preceptors provide you with feedback, but you will not receive credit for the assignment (equivalent to ‚ÄúNeeds Improvement‚Äù on every question of the relevant assignment).\nI expect you to keep track of how many extensions you‚Äôve used. I will do my best to email you a reminder if you have used all three of your extensions and have none remaining.\nIf you have run out of extensions and/or an extenuating circumstance occurs that impacts your ability to submit assignments on time, please email me to discuss the situation. I am happy to be flexible as long as you communicate!"
  },
  {
    "objectID": "syllabus.html#religious-observance",
    "href": "syllabus.html#religious-observance",
    "title": "STAT 155: Introduction to Statistical Modeling",
    "section": "Religious Observance",
    "text": "Religious Observance\nStudents may wish to take part in religious observances that occur during the semester. If you have a religious observance/practice that conflicts with your participation in the course, please contact me before the end of the second week of the semester to discuss appropriate accommodations.\nIn an effort to respect religious diversity, I request that students who plan to observe a religious holiday during scheduled class meetings/class requirements talk to me about reasonable consideration by the end of the second week of the course."
  },
  {
    "objectID": "syllabus.html#well-being",
    "href": "syllabus.html#well-being",
    "title": "STAT 155: Introduction to Statistical Modeling",
    "section": "Well-being",
    "text": "Well-being\nI want you to succeed. Both here at Macalester and beyond. To help make this happen, I am committed to the following.\nRespect: Everyone comes from a different path through life, and it is our moral duty as human beings to listen to each other without judgment and to respect one another. I have no tolerance for discrimination of any kind, in and out of the classroom. If you are seeking campus resources regarding discrimination, the Department of Multicultural Life and the Center for Religious and Spiritual Life are wonderful resources. We will also respect the MSCS Community Guidelines.\nSensitive Topics: Applications in this course span issues in science, policy, and society. As such, we may sometimes address sensitive topics. I will try to announce in class if an assignment or activity involves a potentially sensitive topic. If you have reservations about a particular topic, please come talk to me to discuss possible options.\nAccommodations: If you need accommodations for any reason, please contact Disability Services to discuss your needs, and speak with me as soon as possible afterwards so that we can discuss your accommodation plan. If you already have official accommodations, please discuss these with me within the first week of class so that you get off to a great start. Contact me if you have other special circumstances. I will find resources for you.\nTitle IX: You deserve a community free from discrimination, sexual harassment, hostility, sexual assault, domestic violence, dating violence, and stalking. If you or anyone you know has experienced harassment or discrimination, know that you are not alone. Macalester provides staff and resources to help you find support. Please be aware that as a faculty member, it is my responsibility to report disclosure about sexual harassment, sexual misconduct, relationship violence, and stalking to the Title IX Office. The purpose of this report is to ensure that anyone experiencing harm receives the resources and support they need. I will keep this information private, and it will not be shared beyond this required report.\nYou may also contact Macalester‚Äôs Title IX Coordinator directly (phone: 651-696-6258; e-mail: titleixcordinator@macalester.edu); they will provide you with supportive measures, resources, and referrals. Additional information about how to file a report (including anonymously) is available on the Title IX website.\nGeneral Health and Well-being: I care that you prioritize your well-being in this semester and beyond. Investing time into taking care of yourself will have profound impacts on all aspects of your life. Remember that beyond being a student, you are a human being carrying your own experiences, thoughts, emotions, and identities. It is important to acknowledge any stressors you may be facing, which can be mental, emotional, physical, cultural, financial, etc., and how they can have an impact on you. I encourage you to remember that you have a body with needs. In the classroom, eat when you are hungry, drink water, use the restroom, and step out if you are upset and need some air. Please do what is necessary so long as it does not impede your or others‚Äô ability to be mentally and emotionally present in the course. Outside of the classroom, sleeping well, moving your body, and connecting with others can be strategies can help nourish you. If you are having difficulties maintaining your well-being, please don‚Äôt hesitate to contact me and/or find support from physical and mental health resources here, here, and here."
  },
  {
    "objectID": "syllabus.html#footnotes",
    "href": "syllabus.html#footnotes",
    "title": "STAT 155: Introduction to Statistical Modeling",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nMacalester Academic Advising ‚Äì High School Preparation‚Ü©Ô∏é"
  },
  {
    "objectID": "project.html",
    "href": "project.html",
    "title": "Project",
    "section": "",
    "text": "Coming soon‚Ä¶."
  },
  {
    "objectID": "assignments.html",
    "href": "assignments.html",
    "title": "Practice Problems",
    "section": "",
    "text": "Please review the Practice Problems Policies document before proceeding!\n‚Ä¶due at 11:59 pm Central on Moodle!\n\nPractice Sets 1, due 02/04\nPractice Sets 2, due 02/09"
  },
  {
    "objectID": "activities/06-slr-transformations.html",
    "href": "activities/06-slr-transformations.html",
    "title": "Simple Linear Regression - Transformation",
    "section": "",
    "text": "You can download the .qmd file for this activity here and open in R-studio. The rendered version is posted in the course website (Activities tab). I often experiment with the class activities (and see it in live!) and make updates, but I always post the final version before class starts. To be sure you have the most up-to-date copy, please download it once you‚Äôve settled in before class begins.",
    "crumbs": [
      "Simple Linear Regression - Transformation"
    ]
  },
  {
    "objectID": "activities/06-slr-transformations.html#these-activities-on-slr-transformation-are-optional--enjoy",
    "href": "activities/06-slr-transformations.html#these-activities-on-slr-transformation-are-optional--enjoy",
    "title": "Simple Linear Regression - Transformation",
    "section": "These activities on ‚ÄòSLR: Transformation‚Äô are OPTIONAL- ENJOY!!!",
    "text": "These activities on ‚ÄòSLR: Transformation‚Äô are OPTIONAL- ENJOY!!!",
    "crumbs": [
      "Simple Linear Regression - Transformation"
    ]
  },
  {
    "objectID": "activities/06-slr-transformations.html#learning-goals",
    "href": "activities/06-slr-transformations.html#learning-goals",
    "title": "Simple Linear Regression - Transformation",
    "section": "Learning goals",
    "text": "Learning goals\nBy the end of this lesson, you should be able to:\n\nDistinguish between the different motivations for transformations of variables (interpretation, regression assumptions, etc.)\nDetermine when a particular transformation (center, scale, or log) may be appropriate\nInterpret regression coefficients after a transformation has taken place",
    "crumbs": [
      "Simple Linear Regression - Transformation"
    ]
  },
  {
    "objectID": "activities/06-slr-transformations.html#readings-and-videos",
    "href": "activities/06-slr-transformations.html#readings-and-videos",
    "title": "Simple Linear Regression - Transformation",
    "section": "Readings and videos",
    "text": "Readings and videos\nPlease watch the following video before class.\n\nVideo: Simple Linear Regression: Transformations\n\nThe following reading is optional.\n\nSection 3.8.4 in the STAT 155 Notes covers log transformations, and the ‚Äúladder of power,‚Äù which we will not cover in class.\n\nFile organization: Save this file in the ‚ÄúActivities‚Äù subfolder of your ‚ÄúSTAT155‚Äù folder.",
    "crumbs": [
      "Simple Linear Regression - Transformation"
    ]
  },
  {
    "objectID": "activities/06-slr-transformations.html#exercise-1-location-transformations",
    "href": "activities/06-slr-transformations.html#exercise-1-location-transformations",
    "title": "Simple Linear Regression - Transformation",
    "section": "Exercise 1: Location transformations",
    "text": "Exercise 1: Location transformations\nLocation transformations are ones that shift a predictor variable up or down by a fixed amount. Using a location transformation is sometimes also called centering a predictor.\nWe‚Äôll use the homes data in this exercise, which includes 2006 data on homes in Saratoga County, New York. Our goal is to understand the relationship of home Price ($) with Living.Area, the size of a home in square feet.\n\nFit a linear regression model of Price by Living.Area, and call this model home_mod.\n\n\n# Fit the model\n\n\n# Display model summary output\n\n\nInterpret the intercept and the coefficient for Living.Area. Is the interpretation of the intercept meaningful?\nWe can use a location transformation on Living.Area to ‚Äústart‚Äù it at a more reasonable value. We can see from the summarize() code below that the smallest house is 616 quare feet, so let‚Äôs center this predictor at 600 square feet. There is no code to fill in here, but make note of the mutate() syntax.\n\n\nhomes %&gt;% \n    summarize(min(Living.Area))\n## # A tibble: 1 √ó 1\n##   `min(Living.Area)`\n##                &lt;dbl&gt;\n## 1                616\n\n# What is mutate() doing???\nhomes &lt;- homes %&gt;%\n    mutate(Living.Area.Shifted = Living.Area-600)\n\n\nWe can actually determine the coefficients of the Price ~ Living.Area.Shifted model by hand.\n\nFirst, write out in general terms (without specific numbers) how we would interpret the intercept and slope in this model.\nUse these general interpretations as well as the summary output of home_mod to determine what these new coefficients should be.\n\nNow check your answer to part d by fitting the model.\n\n\n# Fit a model of Price vs. Living.Area.Shifted\n\n\n# Display model summary output\n\n\nSo we now have 2 models of Price by the size of a home, as measured by Living.Area and Living.Area.Shifted:\n\n\nE[Price | Living.Area] = 13439.394 + 113.123 Living.Area\nE[Price | Living.Area.Shifted] = 81312.919 + 113.123 Living.Area.Shifted\n\nAs indicated by the equal slopes but differing intercepts, these models simply have different locations:\n\n# Don't worry about the code!!\nhomes %&gt;% \n  select(Price, Living.Area, Living.Area.Shifted) %&gt;% \n  pivot_longer(cols = -Price, names_to = \"Predictor\", values_to = \"Size\") %&gt;% \n  ggplot(aes(y = Price, x = Size)) + \n  geom_point() + \n  geom_smooth(method = \"lm\", se = FALSE) + \n  facet_wrap(~ Predictor)\n\n\n\n\n\n\n\n\nImportantly, they produce the same predictions! For example, use both models to predict the price of a 1000 square foot home (without using the predict() function). These two predictions should be the same, within rounding.\n\n# Predicting price with the home_mod\n\n# Predicting price with the home_mod_2",
    "crumbs": [
      "Simple Linear Regression - Transformation"
    ]
  },
  {
    "objectID": "activities/06-slr-transformations.html#exercise-2-scale-transformations",
    "href": "activities/06-slr-transformations.html#exercise-2-scale-transformations",
    "title": "Simple Linear Regression - Transformation",
    "section": "Exercise 2: Scale transformations",
    "text": "Exercise 2: Scale transformations\nIn this exercise, we‚Äôll explore the relationship between four-year graduation rate and admissions rate of colleges using 2024 data from the College Scorecard. In the code chunk below, construct a visualization comparing graduation rate (our outcome variable) and admissions rate (our predictor of interest). Remember that your outcome variable should be on the y-axis, in general!\n\n# Scatterplot of graduation rate vs. admissions rate\n\n\nDescribe the relationship you observe between the two quantitative variables, in terms of correlation (weak/strong, positive/negative). Does the relationship appear to be roughly linear?\nWrite a linear regression model formula of the form E[Y | X] = ‚Ä¶ (filling in Y and X appropriately).\nFit this model in R, and report (don‚Äôt interpret yet!) the slope coefficient and intercept coefficient estimates.\n\n\n# Linear regression model with GradRate as the outcome, AdmisRate as predictor of interest\n\n\nIntercept Estimate: Your response here\n\n\nSlope Estimate: Your response here\n\n\nConsidering the units of AdmisRate, what does it mean for AdmisRate to change by one unit? What are the units for AdmisRate (and GradRate, for that matter!)?\nSuppose I want the interpretation of my slope coefficient for AdmisRate in my linear model to be in terms a ‚Äú1 percentage point increase in admissions rate.‚Äù To achieve this, we could mutate our AdmisRate variable to range from 0 to 100. Let‚Äôs do that for GradRate too (just because!):\n\n\n# Mutate\ncollege &lt;- college %&gt;%\n  mutate(AdmisRate = AdmisRate * ___,\n         GradRate = ___ * ___)\n## Error in parse(text = input): &lt;text&gt;:3:35: unexpected input\n## 2: college &lt;- college %&gt;%\n## 3:   mutate(AdmisRate = AdmisRate * __\n##                                      ^\n\n\nFit a new linear regression model with the updated AdmisRate and GradRate variables as your predictor of interest and outcome, respectively. Again, report the intercept and slope estimate from your model.\n\n\n# Linear regression model with updated GradRate as the outcome, updated AdmisRate as predictor of interest\n\n\nIntercept Estimate: Your response here\n\n\nSlope Estimate: Your response here\n\nHow have your intercept and slope estimates changed from the previous model, if at all?\n\nInterpret the regression coefficient that corresponds to the estimated linear relationship between admissions and graduation rates, in the context of the problem. Make sure to use non-causal language, include units, and talk about averages rather than individual cases.",
    "crumbs": [
      "Simple Linear Regression - Transformation"
    ]
  },
  {
    "objectID": "activities/06-slr-transformations.html#exercise-3-log-transformations",
    "href": "activities/06-slr-transformations.html#exercise-3-log-transformations",
    "title": "Simple Linear Regression - Transformation",
    "section": "Exercise 3: Log transformations",
    "text": "Exercise 3: Log transformations\nThe Big Mac Index has been published by The Economist since 1986 as a metric for comparing purchasing power between countries, giving rise to the phrase Burgernomics. It was developed (sort of jokingly) as a way to explain exchange rates in digestible terms.\nAs an example, suppose a Big Mac in Switzerland costs 6.70 Swiss franc, and in the U.S. a Big Mac costs 5.58 USD. Then the Big Mac Index is 6.70/5.58 = 1.20, and is the implied exchange rate between Swiss franc and USD.\nIf you‚Äôd like to read more about the Big Mac index, here‚Äôs an article in The Economist (this may be behind a pay-wall for you, you can read up to 5 free articles in the Economist per month).\nFor this exercise, we‚Äôll use the bigmac data (collected in 2006) to explore the relationship between average teaching salary in a country and the amount of time someone needs to work to be able to afford a Big Mac. The variables we‚Äôll consider are:\n\nbigmac_mins: average minutes to earn 1 Big Mac\ngross_annual_teacher_income: average gross teacher salary in 1 year (USD)\n\n\nCreate an appropriate visualization that displays the relationship between average minutes to earn a Big Mac and gross annual, average teaching salary, and describe what you observe.\n\n\n# Visualization: Big Mac minutes vs. gross annual teacher income\n\n\nExplain why correlation might not be an appropriate numerical summary for the relationship between the two variables you plotted above.\nFit a linear regression model with bigmac_mins as the outcome and gross_annual_teacher_income as the predictor of interest, and interpret the coefficient for gross_annual_teacher_income, in context. Make sure to use non-causal language, include units, and talk about averages rather than individual cases.\n\n\n# Linear regression code\n\n\nPlot residuals vs.¬†fitted values for the model you fit, and describe what you observe. Are there any noticeable patterns in the residuals? Describe them!\n\n\n# Residuals vs. fitted values plot\n\n\nFor which observations do the residuals from the linear regression model appear to be relatively large (i.e.¬†for which observations would predictions fall farthest from observed outcomes)? What possible consequences would this have for people using this model to predict the amount of time it takes for them to earn enough money to afford a Big Mac?\n\nWe‚Äôll now consider a log transformation of teaching salary. In the code chunk below, create a new variable called log_sal that contains the logged values of gross_annual_teacher_income.\n\n# Creating new variable log_sal\nbigmac &lt;- bigmac %&gt;%\n  mutate(log_sal = log(___))\n## Error in parse(text = input): &lt;text&gt;:3:25: unexpected input\n## 2: bigmac &lt;- bigmac %&gt;%\n## 3:   mutate(log_sal = log(__\n##                            ^\n\n\nCreate an appropriate visualization that displays the relationship between average minutes to earn a Big Mac and logged gross annual, average teaching salary, and describe what you observe. Does correlation seem like it may be an appropriate numerical summary for the relationship between these two variables? Explain why or why not.\nFit a linear regression model with bigmac_mins as the outcome and log_sal as the predictor of interest, and interpret the coefficient for log_sal, in context. Make sure to use non-causal language, include units, and talk about averages rather than individual cases.\nPlot residuals vs.¬†fitted values for the model you fit, and describe what you observe. Are there any noticeable patterns in the residuals? Describe them!\n\n\n# Residuals vs. fitted values plot",
    "crumbs": [
      "Simple Linear Regression - Transformation"
    ]
  },
  {
    "objectID": "activities/06-slr-transformations.html#reflection",
    "href": "activities/06-slr-transformations.html#reflection",
    "title": "Simple Linear Regression - Transformation",
    "section": "Reflection",
    "text": "Reflection\nTwo of the main motivations for transforming variables in our regression models is to (1) intentionally change the interpretation of regression coefficients, and (2) to better satisfy linear regression assumptions (e.g.¬†remove ‚Äúpatterns‚Äù from our residual plots). The first is nearly always justified by the scientific context of the research questions you are trying to answer, while the second is a bit more muddy.\nThink about the pros and cons of transforming your variables to satisfy linear regression assumptions. Is there a limit to how much you would be willing to transform your variables? Would transforming too much leave you with un-interpretable regression coefficients?\n\nResponse: Put your response here.",
    "crumbs": [
      "Simple Linear Regression - Transformation"
    ]
  },
  {
    "objectID": "activities/06-slr-transformations.html#exercise-1-location-transformations-1",
    "href": "activities/06-slr-transformations.html#exercise-1-location-transformations-1",
    "title": "Simple Linear Regression - Transformation",
    "section": "Exercise 1: Location transformations",
    "text": "Exercise 1: Location transformations\nLocation transformations are ones that shift a predictor variable up or down by a fixed amount. Using a location transformation is sometimes also called centering a predictor.\nWe‚Äôll use the homes data in this exercise.\n\nFit a linear regression model of Price as a function of Living.Area, and call this model home_mod.\n\n\n# Fit the model\nhome_mod &lt;- lm(Price ~ Living.Area, data = homes)\n\n# Display model summary output\ncoef(summary(home_mod))\n##               Estimate  Std. Error   t value      Pr(&gt;|t|)\n## (Intercept) 13439.3940 4992.352849  2.691996  7.171207e-03\n## Living.Area   113.1225    2.682341 42.173065 9.486240e-268\n\n\n\nInterpretation of slope: Every 1 square foot increase in living area is associated with a $113.12 increase in average house price.\nInterpretation of intercept: The average/expected house price for a house with zero square feet is $13,439.39. Can a house ever be zero square feet??? Nope! The intercept is meaningless in this case.\n\n\n\n\nhomes %&gt;% \n    summarize(min(Living.Area))\n## # A tibble: 1 √ó 1\n##   `min(Living.Area)`\n##                &lt;dbl&gt;\n## 1                616\n\n# mutate() creates a new variable called Living.Area.Shifted that is equal to Living.Area - 600\nhomes &lt;- homes %&gt;%\n    mutate(Living.Area.Shifted = Living.Area-600)\n\n\n\nIn general terms, the intercept in this model should represent the average house price when Living.Area.Shifted is 0‚Äîin other words when Living.Area is 600 square feet. From the coefficient estimates in home_mod, we can calculate the expected / predicted house price for 600 square foot homes: 13439.394 + (113.123*600) = 81312.89. So we‚Äôre expecting the new intercept to be $81312.89.\nThe slope in this model represents the average price change for each unit change in Living.Area.Shifted (which is the same as a unit change in Living.Area). Based on this, the slope should be the same as in home_mod ($113.12 per square foot).\n\nLines up with work in part d!\n\n\n# Fit a model of Price vs. Living.Area.Shifted\nhome_mod_centered &lt;- lm(Price ~ Living.Area.Shifted, data = homes)\n\n# Display model summary output\ncoef(summary(home_mod_centered))\n##                       Estimate  Std. Error  t value      Pr(&gt;|t|)\n## (Intercept)         81312.9191 3515.879467 23.12733 2.638371e-103\n## Living.Area.Shifted   113.1225    2.682341 42.17307 9.486240e-268\n\n\n\n\n\n# Predicting price with the home_mod\n13439.394 + 113.123*1000\n## [1] 126562.4\n\n# Predicting price with the home_mod_2\n81312.919 + 113.123*(1000 - 600)\n## [1] 126562.1",
    "crumbs": [
      "Simple Linear Regression - Transformation"
    ]
  },
  {
    "objectID": "activities/06-slr-transformations.html#exercise-2-scale-transformations-1",
    "href": "activities/06-slr-transformations.html#exercise-2-scale-transformations-1",
    "title": "Simple Linear Regression - Transformation",
    "section": "Exercise 2: Scale transformations",
    "text": "Exercise 2: Scale transformations\nIn the code chunk below, construct a visualization comparing graduation rate (our outcome variable) and admissions rate (our predictor of interest). Remember that your outcome variable should be on the y-axis, in general!\n\n# Scatterplot of graduation rate vs. admissions rate\ncollege %&gt;%\n  ggplot(aes(x = AdmisRate, y = GradRate)) +\n  geom_point()\n\n\n\n\n\n\n\n\n\nThe correlation between admissions and graduation rates appears to be weakly negative. Notably, there are hard boundaries to admissions and graduation rates, since both must fall between 0 and 100%! A few colleges hit up against these boundaries. I would say that, with the exception of the observations that have either 0% graduation rates or 0% admission rates, the relationship does appear to be roughly linear.\nE[GradRate | AdmisRate] = \\(\\beta_0\\) + \\(\\beta_1\\) AdmisRate\n\n\n\n# Linear regression model with GradRate as the outcome, AdmisRate as predictor of interest\nmod &lt;- lm(GradRate ~ AdmisRate, data = college)\nsummary(mod)\n## \n## Call:\n## lm(formula = GradRate ~ AdmisRate, data = college)\n## \n## Residuals:\n##      Min       1Q   Median       3Q      Max \n## -0.68409 -0.13681  0.01296  0.15550  0.66204 \n## \n## Coefficients:\n##             Estimate Std. Error t value Pr(&gt;|t|)    \n## (Intercept)  0.68409    0.01759   38.89   &lt;2e-16 ***\n## AdmisRate   -0.34613    0.02330  -14.85   &lt;2e-16 ***\n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n## \n## Residual standard error: 0.2088 on 1649 degrees of freedom\n## Multiple R-squared:  0.118,  Adjusted R-squared:  0.1175 \n## F-statistic: 220.6 on 1 and 1649 DF,  p-value: &lt; 2.2e-16\n\n\nIntercept Estimate: 0.68409\n\n\nSlope Estimate: -0.34613\n\n\nOne unit of AdmisRate corresponds to a 100% change in admissions rates! The same goes for graduation rate. This is a huge change (in fact, the largest change possible).\nSuppose I want the interpretation of my slope coefficient for AdmisRate in my linear model to be in terms a ‚Äú1% increase in admissions rate.‚Äù To achieve this, we could mutate our AdmisRate variable to range from 0 to 100. Let‚Äôs do that for GradRate too (just because!):\n\n\n# Mutate\ncollege &lt;- college %&gt;%\n  mutate(AdmisRate = AdmisRate * 100,\n         GradRate = GradRate * 100)\n\n\nFit a new linear regression model with the updated AdmisRate and GradRate variables as your predictor of interest and outcome, respectively. Again, report the intercept and slope estimate from your model.\n\n\n# Linear regression model with updated GradRate as the outcome, updated AdmisRate as predictor of interest\nmod_new &lt;- lm(GradRate ~ AdmisRate, data = college)\nsummary(mod_new)\n## \n## Call:\n## lm(formula = GradRate ~ AdmisRate, data = college)\n## \n## Residuals:\n##     Min      1Q  Median      3Q     Max \n## -68.409 -13.681   1.296  15.550  66.204 \n## \n## Coefficients:\n##             Estimate Std. Error t value Pr(&gt;|t|)    \n## (Intercept)  68.4088     1.7592   38.89   &lt;2e-16 ***\n## AdmisRate    -0.3461     0.0233  -14.85   &lt;2e-16 ***\n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n## \n## Residual standard error: 20.88 on 1649 degrees of freedom\n## Multiple R-squared:  0.118,  Adjusted R-squared:  0.1175 \n## F-statistic: 220.6 on 1 and 1649 DF,  p-value: &lt; 2.2e-16\n\n\nIntercept Estimate: 68.4088\n\n\nSlope Estimate: -0.3461\n\nOur intercept estimate is now 100x larger, and our slope estimate has remained the same! The slope remained the same because we multiplied our outcome and our predictor of interest by the same value, and the intercept is 100x larger because we multiplied our outcome by 100 (recall that the intercept is the average expected outcome when ‚Äúx‚Äù is zero).\n\nOn average, we expect colleges that differ in admissions rate by 1% to have 0.35% different graduation rates, with colleges with higher admissions rates having lower graduation rates.",
    "crumbs": [
      "Simple Linear Regression - Transformation"
    ]
  },
  {
    "objectID": "activities/06-slr-transformations.html#exercise-3-log-transformations-1",
    "href": "activities/06-slr-transformations.html#exercise-3-log-transformations-1",
    "title": "Simple Linear Regression - Transformation",
    "section": "Exercise 3: Log transformations",
    "text": "Exercise 3: Log transformations\n\n\n\n\n# Visualization: Big Mac minutes vs. gross annual teacher income\nbigmac %&gt;%\n  ggplot(aes(x = gross_annual_teacher_income, y = bigmac_mins)) +\n  geom_point() \n\n\n\n\n\n\n\n\nAs annual teacher income gets higher, time it takes in minutes to earn a Big Mac decreases, though the relationship does not appear linear. The amount of time it takes to earn a Big Mac is very high when income is below about 10,000 where it sharply decreases, and then decreases at a much lower rate when income is above around 20,000.\n\nCorrelation is a summary of the linear relationship between two quantitative variables, and this relationship does not appear to be linear!\n\n\n\n# Linear regression code\nmod &lt;- lm(bigmac_mins ~ gross_annual_teacher_income, data = bigmac)\nsummary(mod)\n## \n## Call:\n## lm(formula = bigmac_mins ~ gross_annual_teacher_income, data = bigmac)\n## \n## Residuals:\n##     Min      1Q  Median      3Q     Max \n## -29.649  -9.556  -1.784   4.512  43.715 \n## \n## Coefficients:\n##                               Estimate Std. Error t value Pr(&gt;|t|)    \n## (Intercept)                  5.801e+01  3.104e+00   18.69  &lt; 2e-16 ***\n## gross_annual_teacher_income -9.092e-04  9.591e-05   -9.48 6.16e-14 ***\n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n## \n## Residual standard error: 15.4 on 66 degrees of freedom\n##   (2 observations deleted due to missingness)\n## Multiple R-squared:  0.5766, Adjusted R-squared:  0.5701 \n## F-statistic: 89.86 on 1 and 66 DF,  p-value: 6.164e-14\n\nA one dollar increase in gross annual teacher income is associated with a 9 x 10^(-4) minutes decrease in the average number of minutes it takes to earn a Big Mac. Stated differently, a ten-thousand dollar increase in gross annual teacher income is associated with a 9 minute decrease in the average number of minutes it takes to earn a Big Mac (note that here I did a scale transformation of gross annual teacher income to get this interpretation, which might make more sense when looking at the scale of salary!).\n\n\n\n\n# Residuals vs. fitted values plot\nggplot(mod, aes(x = .fitted, y = .resid)) + \n    geom_point() + \n    geom_hline(yintercept = 0) +\n    geom_smooth(se = FALSE)\n\n\n\n\n\n\n\n\nThe residuals vs.¬†fitted values plot shows a very clear, nonlinear pattern! As fitted values increase, residuals decrease for a while, and then sharply increase once fitted values are higher than around 40 minutes. The spread of residuals around zero also varies, with greater spread for higher fitted values.\n\nThe residuals appear to be large for people with negative fitted values and those with very high fitted values. Recall that a linear model does not ‚Äúknow‚Äù that number of minutes to earn a Big Mac can‚Äôt be negative, in context. If we look at the fitted line from our linear model on a scatterplot (see below)‚Ä¶\n\n\nbigmac %&gt;%\n  ggplot(aes(x = gross_annual_teacher_income, y = bigmac_mins)) +\n  geom_point() +\n  geom_smooth(method = \"lm\", se = FALSE)\n\n\n\n\n\n\n\n\nWe observe that negative fitted values and very large fitted values occur when annual teacher income is greater than around 70,000 and less than 10,000, respectively. This implies that the model does a worse job at predicting the number of minutes to earn a Big Mac in countries where annual teacher income is either very high or very low.\nWe‚Äôll now consider a log transformation of teaching salary. In the code chunk below, create a new variable called log_sal that contains the logged values of gross_annual_teacher_income.\n\n# Creating new variable log_sal\nbigmac &lt;- bigmac %&gt;%\n  mutate(log_sal = log(gross_annual_teacher_income))\n\n\n\n\n\nbigmac %&gt;%\n  ggplot(aes(log_sal, bigmac_mins)) +\n  geom_point()\n\n\n\n\n\n\n\n\nThe relationship between logged annual teaching salary and minutes to earn a Big Mac appears roughly linear, with a weakly negative relationship. Correlation is likely an appropriate numerical summary for the relationship between these two quantitative variables, as the relationship is roughly linear!\n\n\n\n\nmod_log &lt;- lm(bigmac_mins ~ log_sal, data = bigmac)\nsummary(mod_log)\n## \n## Call:\n## lm(formula = bigmac_mins ~ log_sal, data = bigmac)\n## \n## Residuals:\n##     Min      1Q  Median      3Q     Max \n## -36.817  -6.951  -1.241   6.032  41.357 \n## \n## Coefficients:\n##             Estimate Std. Error t value Pr(&gt;|t|)    \n## (Intercept)  210.875     14.687   14.36   &lt;2e-16 ***\n## log_sal      -18.142      1.502  -12.08   &lt;2e-16 ***\n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n## \n## Residual standard error: 13.21 on 66 degrees of freedom\n##   (2 observations deleted due to missingness)\n## Multiple R-squared:  0.6886, Adjusted R-squared:  0.6838 \n## F-statistic: 145.9 on 1 and 66 DF,  p-value: &lt; 2.2e-16\n\nEach 1 unit increase in logged salary is associated with a 18.14 minute decrease in time to earn a Big Mac on average.\nWe can also use a property of logarithms to interpret the slope of -18.14 in a different way. Suppose we have two salaries: Salary1 and Salary2. If Salary2 is 10% higher than Salary1, then Salary2/Salary1 = 1.1. It is a property of logarithms that log(Salary2/Salary1) = log(Salary2) - log(Salary1). In this case log(Salary2/Salary1) = log(Salary2) - log(Salary1) = log(1.1) = 0.09531018. So a 10% increase in salary is a 0.09 unit increase in the log scale:\n\n# Multiplicative difference of 1.1, or 10% between salaries gives us the \nlog(1.1) * -18.142\n## [1] -1.729117\n\nWhile a 1 unit increase in log salary is associated with an average decrease of 18 Big Mac minutes, a 0.0953 unit increase in log salary (which corresponds to a 10% multiplicative increase), is associated with a 1.7 minute decrease in Big Mac minutes.\nUnderlying math:\nCase 1: Salary = x\n   E[bigmacmin_1] = beta0 + beta1 log(x)\nCase 2: Salary = m*x\n   E[bigmacmin_2] = beta0 + beta1 log(m*x)\n\nE[bigmacmin_2] - E[bigmacmin_1] = beta1 log(m)\n\n\n\n\n# Residuals vs. fitted values plot\nggplot(mod_log, aes(x = .fitted, y = .resid)) + \n    geom_point() + \n    geom_hline(yintercept = 0) +\n    geom_smooth(se = FALSE)\n\n\n\n\n\n\n\n\nThe residuals seem to lie roughly around zero for all possible fitted values, though the spread is still noticably larger for larger fitted values compared to smaller ones. This implies that the linearity assumption is likely satisfied for this model, but equal variance may be a concern.",
    "crumbs": [
      "Simple Linear Regression - Transformation"
    ]
  },
  {
    "objectID": "activities/03_04-slr-intro-formalization.html",
    "href": "activities/03_04-slr-intro-formalization.html",
    "title": "Simple Linear Regression - Intro+Formalization",
    "section": "",
    "text": "You can download the .qmd file for this activity here and open in R-studio. The rendered version is posted in the course website (Activities tab). I often experiment with the class activities (and see it in live!) and make updates, but I always post the final version before class starts. To be sure you have the most up-to-date copy, please download it once you‚Äôve settled in before class begins.",
    "crumbs": [
      "Simple Linear Regression - Intro+Formalization"
    ]
  },
  {
    "objectID": "activities/03_04-slr-intro-formalization.html#learning-goals",
    "href": "activities/03_04-slr-intro-formalization.html#learning-goals",
    "title": "Simple Linear Regression - Intro+Formalization",
    "section": "Learning goals",
    "text": "Learning goals\nBy the end of this lesson, you should be able to:\n\nVisualize and describe the relationship between two quantitative variables using a scatterplot\nWrite R code to create a scatterplot and compute the linear correlation between two quantitative variables\nDescribe/identify weak / strong, and positive / negative correlation from a point cloud\nBuild intuition for fitting lines to quantify the relationship between two quantitative variables\nDifferentiate between a response / outcome variable and a predictor / explanatory variable\nWrite a model formula for a simple linear regression model with a quantitative predictor\nWrite R code to fit a linear regression model\nInterpret the intercept and slope coefficients in a simple linear regression model with a quantitative predictor\nCompute expected / predicted / fitted values and residuals from a linear regression model formula\nInterpret predicted values and residuals in the context of the data\nExplain the connection between residuals and the least squares criterion",
    "crumbs": [
      "Simple Linear Regression - Intro+Formalization"
    ]
  },
  {
    "objectID": "activities/03_04-slr-intro-formalization.html#readings-and-videos",
    "href": "activities/03_04-slr-intro-formalization.html#readings-and-videos",
    "title": "Simple Linear Regression - Intro+Formalization",
    "section": "Readings and videos",
    "text": "Readings and videos\nChoose either the reading or the videos to go through before class. CP Quiz due at 09:00 am on these topics\n\nReading: Sections 2.8, 3.1-3.3, 3.6 in the STAT 155 Notes\nVideos:\n\nSimple linear regression Part 1: motivation & scatterplots\nSimple linear regression Part 2: correlation\nSimple linear regression Part 3: simple linear regression models\nR Code for Fitting a Linear Model (Time: 11:07)\n\n\nFile organization: Save this file in the ‚ÄúActivities‚Äù subfolder of your ‚ÄúSTAT155‚Äù folder.",
    "crumbs": [
      "Simple Linear Regression - Intro+Formalization"
    ]
  },
  {
    "objectID": "activities/03_04-slr-intro-formalization.html#exercise-1-get-to-know-the-data",
    "href": "activities/03_04-slr-intro-formalization.html#exercise-1-get-to-know-the-data",
    "title": "Simple Linear Regression - Intro+Formalization",
    "section": "Exercise 1: Get to know the data",
    "text": "Exercise 1: Get to know the data\nCreate a new code chunk to look at the first few rows of the data and learn how much data (in terms of cases and variables) we have.\n\ndim(bikes)\n## [1] 731  15\nhead(bikes)\n## # A tibble: 6 √ó 15\n##   date       season  year month day_of_week weekend holiday temp_actual\n##   &lt;date&gt;     &lt;chr&gt;  &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt;       &lt;lgl&gt;   &lt;chr&gt;         &lt;dbl&gt;\n## 1 2011-01-01 winter  2011 Jan   Sat         TRUE    no             57.4\n## 2 2011-01-02 winter  2011 Jan   Sun         TRUE    no             58.8\n## 3 2011-01-03 winter  2011 Jan   Mon         FALSE   no             46.5\n## 4 2011-01-04 winter  2011 Jan   Tue         FALSE   no             46.8\n## 5 2011-01-05 winter  2011 Jan   Wed         FALSE   no             48.7\n## 6 2011-01-06 winter  2011 Jan   Thu         FALSE   no             47.1\n## # ‚Ñπ 7 more variables: temp_feel &lt;dbl&gt;, humidity &lt;dbl&gt;, windspeed &lt;dbl&gt;,\n## #   weather_cat &lt;chr&gt;, riders_casual &lt;dbl&gt;, riders_registered &lt;dbl&gt;,\n## #   riders_total &lt;dbl&gt;\n\n\nWhat does a case represent?\nHow many and what kinds of variables do we have?",
    "crumbs": [
      "Simple Linear Regression - Intro+Formalization"
    ]
  },
  {
    "objectID": "activities/03_04-slr-intro-formalization.html#exercise-2-get-to-know-the-outcomeresponse-variable",
    "href": "activities/03_04-slr-intro-formalization.html#exercise-2-get-to-know-the-outcomeresponse-variable",
    "title": "Simple Linear Regression - Intro+Formalization",
    "section": "Exercise 2: Get to know the outcome/response variable",
    "text": "Exercise 2: Get to know the outcome/response variable\nLet‚Äôs get acquainted with the riders_registered variable.\n\nConstruct an appropriate plot to visualize the distribution of this variable, and compute appropriate numerical summaries.\n\n\nggplot(bikes, aes(x = riders_registered)) +\n    geom_histogram()\n\n\n\n\n\n\n\n\nggplot(bikes, aes(y = riders_registered)) +\n    geom_boxplot()\n\n\n\n\n\n\n\n\nsummary(bikes$riders_registered)\n##    Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n##      20    2497    3662    3656    4776    6946\n\nbikes %&gt;% \n    summarize(sd(riders_registered))\n## # A tibble: 1 √ó 1\n##   `sd(riders_registered)`\n##                     &lt;dbl&gt;\n## 1                   1560.\n\n\nWrite a good paragraph interpreting the plot and numerical summaries after the class.\n\n\nType your answer",
    "crumbs": [
      "Simple Linear Regression - Intro+Formalization"
    ]
  },
  {
    "objectID": "activities/03_04-slr-intro-formalization.html#before-starting-the-next-exercise-lets-do-the-followings-first",
    "href": "activities/03_04-slr-intro-formalization.html#before-starting-the-next-exercise-lets-do-the-followings-first",
    "title": "Simple Linear Regression - Intro+Formalization",
    "section": "Before starting the next Exercise, let‚Äôs do the followings first!",
    "text": "Before starting the next Exercise, let‚Äôs do the followings first!\nMost of you should have figured out the followings by now, still let‚Äôs skim through the followings!\n\nFirst, save this activity in your device as STAT 155 -&gt; activities -&gt; copy-paste 03_04-slr-intro-formalization.qmd\nClick on Render button! What happens? ::: {.callout-tip title=‚ÄúAnswers‚Äù} The html has saved in the same location (folder) where you initially saved the .qmd file! You need to submit .html file like this for the PP! :::\nNow, click on the +C at the top right side of RStudio and then choose R. What happens?\n\n\n\n\n\n\n\nAnswers\n\n\n\nIt creates code chunks",
    "crumbs": [
      "Simple Linear Regression - Intro+Formalization"
    ]
  },
  {
    "objectID": "activities/03_04-slr-intro-formalization.html#exercise-3-explore-the-relationship-between-ridership-and-temperature",
    "href": "activities/03_04-slr-intro-formalization.html#exercise-3-explore-the-relationship-between-ridership-and-temperature",
    "title": "Simple Linear Regression - Intro+Formalization",
    "section": "Exercise 3: Explore the relationship between ridership and temperature",
    "text": "Exercise 3: Explore the relationship between ridership and temperature\nWe‚Äôd like to understand how daily ridership among registered users relates with the temperature that it feels like that day (temp_feel).\n\nWhat type of plot would be appropriate to visualize this relationship? Sketch and describe what you expect this plot to look like.\n\n\nScatterplot (outcome and predictor are both quantitative)\n\n\nCreate an appropriate plot using ggplot(). How does the plot compare to what you predicted?\n\n\nggplot(bikes, aes(x = temp_feel, y = riders_registered)) +\n    geom_point()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nComments\n\n\n\nTrend: Linear (?); Direction/Association: Positive/Negative, Strength: spread of the points (dispersed? close together? moderately close together?); Outlier?\n\n\n\nAdd the following two lines after your plot to add a linear (blue) and curved (red) smoothing line. What do you notice? Is a simple linear regression model appropriate for this data?\n\n\n# Add a red straight line of best fit and a blue curve of best fit\nYOUR_PLOT +\n    geom_smooth(method = \"lm\", color = \"red\", se = FALSE) +\n    geom_smooth(color = \"blue\", se = FALSE)\n\nWhat do you think ‚Äúeval = TRUE/FALSE‚Äù doing here {r eval = TRUE/FALSE}?\n\n# Add a red straight line of best fit and a blue curve of best fit\nggplot(bikes, aes(x = temp_feel, y = riders_registered)) +\n    geom_point() +\n    geom_smooth(method = \"lm\", color = \"red\", se = FALSE) +\n    geom_smooth(color = \"blue\", se = FALSE)\n\n\n\n\n\n\n\n\nComments: If we only displayed the red line of best fit on the plot, we might miss the slight downward trend at the highest temperatures that we can see more clearly with the blue curve of best fit. A linear model is not appropriate if fit to the whole range of the data, but there does seem to be a linear relationship between ridership and temperature below 80 degrees Fahrenheit.\n\nCompute Correlation of temp_feel and riders_registered.\n\n\nCorrelation\nWe can quantify the linear relationship between two quantitative variables using a numerical summary known as correlation (sometimes known as a ‚Äúcorrelation coefficient‚Äù or ‚ÄúPearson‚Äôs correlation‚Äù). Correlation can range from -1 to 1, where a correlation of 0 indicates that there is no linear relationship between the two quantitative variables.\nBelow is an example of a ‚ÄúMath Box‚Äù. You‚Äôll see these occasionally throughout the activities. You are not required to memorize, nor will you be assessed on, anything in the math boxes. If you plan on continuing with Statistics courses at Macalester (or are interested in the math behind everything!), these math boxes are for you!\n\n\n\n\n\n\nCorrelation\n\n\n\n\n\nThe Pearson correlation coefficient, \\(r_{x, y}\\), of \\(x\\) and \\(y\\) is the (almost) average of products of the z-scores of variables \\(x\\) and \\(y\\):\n\\[\nr_{x, y} = \\frac{\\sum z_x z_y}{n - 1}\n\\]\n\n\n\nIn general, we will want to be able to describe (qualitatively) two aspects of correlation:\n\nStrength\n\n\nIs the correlation between x and y strong, or weak, i.e.¬†how closely do the points fit around a line? This has to do with how dispersed our point clouds are.\n\n\nDirection\n\n\nIs the correlation between x and y positive or negative, i.e.¬†does y go ‚Äúup‚Äù when x goes ‚Äúup‚Äù (positive), or does y go ‚Äúdown‚Äù when x goes ‚Äúup‚Äù (negative)?\n\nStronger correlations will be further from 0 (closer to -1 or 1), and positive and negative correlations will have the appropriate respective sign (above or below zero).\nWhiteboard Time (a little-bit!)\n\n# correlation\n\n# Note: the order in which you put your two quantitative variables into the cor\n# function doesn't matter! Try switching them around to confirm this for yourself\nbikes %&gt;%\n    summarize(cor(temp_feel, riders_registered))\n## # A tibble: 1 √ó 1\n##   `cor(temp_feel, riders_registered)`\n##                                 &lt;dbl&gt;\n## 1                               0.544",
    "crumbs": [
      "Simple Linear Regression - Intro+Formalization"
    ]
  },
  {
    "objectID": "activities/03_04-slr-intro-formalization.html#exercise-4-filtering-our-data",
    "href": "activities/03_04-slr-intro-formalization.html#exercise-4-filtering-our-data",
    "title": "Simple Linear Regression - Intro+Formalization",
    "section": "Exercise 4: Filtering our data",
    "text": "Exercise 4: Filtering our data\nThe relationship between registered riders and temperature looks linear below 80 degrees. We can use the filter() function from the dplyr package to subset our cases. (We‚Äôll learn techniques soon for handling this nonlinear relationship.)\nIf we wanted to only keep cases where registered ridership was greater than 2000, we would use the following code:\n\n# The %&gt;% is called a \"pipe\" and feeds what comes before it\n# into what comes after (bikes data is \"fed into\" the filter() function)\nNEW_DATASET_NAME &lt;- bikes %&gt;% \n    filter(riders_registered &gt; 2000)\n\nAdapt the example above to create a new dataset called bikes_sub that only keeps cases where the felt temperature is less than 80 degrees.\n\n# The %&gt;% is called a \"pipe\" and feeds what comes before it\n# into what comes after (bikes data is \"fed into\" the filter() function)\nbikes_sub &lt;- bikes %&gt;% \n    filter(temp_feel &lt; 80)\n\nDid it work? Check the dimensions of bikes and bikes_sub!\n\ndim(bikes_sub)\n## [1] 430  15",
    "crumbs": [
      "Simple Linear Regression - Intro+Formalization"
    ]
  },
  {
    "objectID": "activities/03_04-slr-intro-formalization.html#exercise-5-model-fitting-and-coefficient-interpretation",
    "href": "activities/03_04-slr-intro-formalization.html#exercise-5-model-fitting-and-coefficient-interpretation",
    "title": "Simple Linear Regression - Intro+Formalization",
    "section": "Exercise 5: Model fitting and coefficient interpretation",
    "text": "Exercise 5: Model fitting and coefficient interpretation\nLet‚Äôs fit a simple linear regression model and examine the results. Step through code chunk slowly, and make note of new code.\n\n# Construct and save the model as bike_mod\n# What's the purpose of \"riders_registered ~ temp_feel\"?\n# What's the purpose of \"data = bikes_sub\"?\nbike_mod &lt;- lm(riders_registered ~ temp_feel, data = bikes_sub)\n\n\n# A long summary of the model stored in bike_mod\nsummary(bike_mod)\n## \n## Call:\n## lm(formula = riders_registered ~ temp_feel, data = bikes_sub)\n## \n## Residuals:\n##     Min      1Q  Median      3Q     Max \n## -3681.8  -928.3   -98.6   904.9  3496.7 \n## \n## Coefficients:\n##              Estimate Std. Error t value Pr(&gt;|t|)    \n## (Intercept) -2486.412    421.379  -5.901 7.37e-09 ***\n## temp_feel      86.493      6.464  13.380  &lt; 2e-16 ***\n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n## \n## Residual standard error: 1267 on 428 degrees of freedom\n## Multiple R-squared:  0.2949, Adjusted R-squared:  0.2933 \n## F-statistic:   179 on 1 and 428 DF,  p-value: &lt; 2.2e-16\n\n\n# A simplified model summary\ncoef(summary(bike_mod))\n##                Estimate Std. Error   t value     Pr(&gt;|t|)\n## (Intercept) -2486.41180 421.379174 -5.900652 7.368345e-09\n## temp_feel      86.49251   6.464247 13.380135 2.349753e-34\n\n\nUsing the model summary output, complete the following model formula:\nE[riders_registered | temp_feel] = -2486.41180 + 86.49251 * temp_feel\nIntercept interpretation: On days that feel like 0 degrees Fahrenheit, we can expect an average of -2486.41180 riders‚Äîa negative number of riders doesn‚Äôt make sense! This results because of extrapolation‚Äî0 degrees is so far below the minimum temperature in the data. We only have information on the relationship between ridership and temperature in the ~40-100 degree range and have no idea what that relationship looks like outside that range.\nSlope interpretation: Every 1 degree Fahrenheit increase in feeling temperature is associated with an average of about 86 more riders.",
    "crumbs": [
      "Simple Linear Regression - Intro+Formalization"
    ]
  },
  {
    "objectID": "activities/03_04-slr-intro-formalization.html#exercise-6-predictions-and-residuals",
    "href": "activities/03_04-slr-intro-formalization.html#exercise-6-predictions-and-residuals",
    "title": "Simple Linear Regression - Intro+Formalization",
    "section": "Exercise 6: Predictions and residuals",
    "text": "Exercise 6: Predictions and residuals\nOn August 17, 2012, the temp_feel was 53.816 degrees and there were 5665 riders. We can get data for this day using the filter() and select() dplyr functions. Note, but don‚Äôt worry about the syntax ‚Äì we haven‚Äôt learned this yet:\n\nbikes_sub %&gt;% \n    filter(date == \"2012-08-17\") %&gt;% \n    select(riders_registered, temp_feel) \n## # A tibble: 1 √ó 2\n##   riders_registered temp_feel\n##               &lt;dbl&gt;     &lt;dbl&gt;\n## 1              5665      53.8\n\n\nPeak back at the scatterplot.\n\n\nggplot(bikes_sub, aes(x = temp_feel, y = riders_registered)) +\n    geom_point() +\n    geom_smooth(method = \"lm\", color = \"red\", se = FALSE) \n\n\n\n\n\n\n\n\n\nUse your model formula from the previous exercise to predict the ridership on August 17, 2012 from the temperature on that day. (That is, where do days with this temperature fall on the model trend line? How many registered riders would we expect on a 53.816 degree day?)\n\n\n\n\n\n\n\nAnswer\n\n\n\n-2486.41180 + 86.49251 * 53.816 = 2168.269\n\n\n\nCheck your part b calculation using the predict() function. Take careful note of the syntax ‚Äì there‚Äôs a lot going on!\n\n\n# What is the purpose of newdata = ___???\npredict(bike_mod, newdata = data.frame(temp_feel = 53.816))\n##        1 \n## 2168.269\n\n\nCalculate the residual or prediction error. How far does the observed ridership fall from the model prediction?\n\n\n\n\n\n\n\nAnswer\n\n\n\nresidual = observed y - predicted y = 5665 - 2168.269 = 3496.731\n\n\n\nAre positive residuals above or below the trend line? When we have positive residuals, does the model over- or under-estimate ridership? Repeat these questions for negative residuals.\n\n\n\n\n\n\n\nAnswer\n\n\n\n\nPositive residuals are above the trend line‚Äîwe under-estimate ridership.\nNegative residuals are below the trend line‚Äîwe over-estimate ridership.\n\n\n\n\nFor an 85 degree day, how many registered riders would we expect? Do you think it‚Äôs a good idea to make this prediction? (Revisit the visualization and filtering we did in Exercises 3 and 4.) [Complete after the class!]",
    "crumbs": [
      "Simple Linear Regression - Intro+Formalization"
    ]
  },
  {
    "objectID": "activities/03_04-slr-intro-formalization.html#exercise-7-changing-temperature-units-challenge-complete-after-the-class",
    "href": "activities/03_04-slr-intro-formalization.html#exercise-7-changing-temperature-units-challenge-complete-after-the-class",
    "title": "Simple Linear Regression - Intro+Formalization",
    "section": "Exercise 7: Changing temperature units (CHALLENGE) [Complete after the class!]",
    "text": "Exercise 7: Changing temperature units (CHALLENGE) [Complete after the class!]\nSuppose we had measured temperature in degrees Celsius rather than degrees Fahrenheit. How do you think our intercept and slope estimates, and their coefficient interpretations, would change?",
    "crumbs": [
      "Simple Linear Regression - Intro+Formalization"
    ]
  },
  {
    "objectID": "activities/03_04-slr-intro-formalization.html#render-your-work-again-this-is-a-good-practice-to-render-often--to-see-which-part-of-the-paragraph-might-cause-non-rendering-if-applicable-complete-after-the-class",
    "href": "activities/03_04-slr-intro-formalization.html#render-your-work-again-this-is-a-good-practice-to-render-often--to-see-which-part-of-the-paragraph-might-cause-non-rendering-if-applicable-complete-after-the-class",
    "title": "Simple Linear Regression - Intro+Formalization",
    "section": "Render your work Again (this is a good practice to render often- to see which part of the paragraph might cause non-rendering, if applicable!) [Complete after the class!]",
    "text": "Render your work Again (this is a good practice to render often- to see which part of the paragraph might cause non-rendering, if applicable!) [Complete after the class!]\n\nClick the ‚ÄúRender‚Äù button in the menu bar for this pane (blue arrow pointing right). This will create an HTML file containing all of the directions, code, and responses from this activity. A preview of the HTML will appear in the browser.\nScroll through and inspect the document to check that your work translated to the HTML format correctly.\nClose the browser tab.\nGo to the ‚ÄúBackground Jobs‚Äù pane in RStudio and click the Stop button to end the rendering process.\nNavigate to your ‚ÄúActivities‚Äù subfolder within your ‚ÄúSTAT155‚Äù folder and locate the HTML file. You can open it again in your browser to double check.",
    "crumbs": [
      "Simple Linear Regression - Intro+Formalization"
    ]
  },
  {
    "objectID": "activities/03_04-slr-intro-formalization.html#exercise-8-ridership-and-windspeed",
    "href": "activities/03_04-slr-intro-formalization.html#exercise-8-ridership-and-windspeed",
    "title": "Simple Linear Regression - Intro+Formalization",
    "section": "Exercise 8: Ridership and windspeed",
    "text": "Exercise 8: Ridership and windspeed\nLet‚Äôs pull together everything that you‚Äôve practiced in the preceding exercises to investigate the relationship between riders_registered and windspeed. Go back to using the bikes dataset (instead of bikes_sub) because we no longer need to only keep days less than 80 degrees.\n\n# Construct and interpret a visualization of this relationship\n# Include a representation of the relationship trend\n\n\n# Use lm to construct a model of riders_registered vs windspeed\n# Save this as bike_mod2\n\n\n# Get a short summary of this model\n\n\nSummarize your observations from the visualizations.\nWrite out a formula for the model trend.\nInterpret both the intercept and the windspeed coefficient. (Note: What does a negative slope indicate?)\nUse this model to predict the ridership on August 17, 2012 and calculate the corresponding residual. (Note: You‚Äôll first need to find the windspeed on this date!)",
    "crumbs": [
      "Simple Linear Regression - Intro+Formalization"
    ]
  },
  {
    "objectID": "activities/03_04-slr-intro-formalization.html#exercise-9-data-drills-filter-select-summarize-complete-after-class",
    "href": "activities/03_04-slr-intro-formalization.html#exercise-9-data-drills-filter-select-summarize-complete-after-class",
    "title": "Simple Linear Regression - Intro+Formalization",
    "section": "Exercise 9: Data drills (filter, select, summarize) [Complete after class]",
    "text": "Exercise 9: Data drills (filter, select, summarize) [Complete after class]\nThis exercise is designed to help you keep building your dplyr skills. These skills are important to data cleaning and digging, which in turn is important to really making meaning of our data. We‚Äôll work with a simpler set of 10 data points:\n\nnew_bikes &lt;- bikes %&gt;% \n    select(date, temp_feel, humidity, riders_registered, day_of_week) %&gt;% \n    head(10)\n\n\nVerb 1: summarize\nThus far, in the dplyr grammar you‚Äôve seen 3 verbs or action words: summarize(), select(), filter(). Try out the following code and then summarize the point of the summarize() function:\n\nnew_bikes %&gt;% \n    summarize(mean(temp_feel), mean(humidity))\n## # A tibble: 1 √ó 2\n##   `mean(temp_feel)` `mean(humidity)`\n##               &lt;dbl&gt;            &lt;dbl&gt;\n## 1              52.0            0.544\n\n\n\nVerb 2: select\nTry out the following code and then summarize the point of the select() function:\n\nnew_bikes %&gt;%\n    select(date, temp_feel)\n## # A tibble: 10 √ó 2\n##    date       temp_feel\n##    &lt;date&gt;         &lt;dbl&gt;\n##  1 2011-01-01      64.7\n##  2 2011-01-02      63.8\n##  3 2011-01-03      49.0\n##  4 2011-01-04      51.1\n##  5 2011-01-05      52.6\n##  6 2011-01-06      53.0\n##  7 2011-01-07      50.8\n##  8 2011-01-08      46.6\n##  9 2011-01-09      42.5\n## 10 2011-01-10      45.6\n\n\nnew_bikes %&gt;% \n    select(-date, -temp_feel)\n## # A tibble: 10 √ó 3\n##    humidity riders_registered day_of_week\n##       &lt;dbl&gt;             &lt;dbl&gt; &lt;chr&gt;      \n##  1    0.806               654 Sat        \n##  2    0.696               670 Sun        \n##  3    0.437              1229 Mon        \n##  4    0.590              1454 Tue        \n##  5    0.437              1518 Wed        \n##  6    0.518              1518 Thu        \n##  7    0.499              1362 Fri        \n##  8    0.536               891 Sat        \n##  9    0.434               768 Sun        \n## 10    0.483              1280 Mon\n\n\n\nVerb 3: filter\nTry out the following code and then summarize the point of the filter() function:\n\nnew_bikes %&gt;% \n    filter(riders_registered &gt; 850)\n## # A tibble: 7 √ó 5\n##   date       temp_feel humidity riders_registered day_of_week\n##   &lt;date&gt;         &lt;dbl&gt;    &lt;dbl&gt;             &lt;dbl&gt; &lt;chr&gt;      \n## 1 2011-01-03      49.0    0.437              1229 Mon        \n## 2 2011-01-04      51.1    0.590              1454 Tue        \n## 3 2011-01-05      52.6    0.437              1518 Wed        \n## 4 2011-01-06      53.0    0.518              1518 Thu        \n## 5 2011-01-07      50.8    0.499              1362 Fri        \n## 6 2011-01-08      46.6    0.536               891 Sat        \n## 7 2011-01-10      45.6    0.483              1280 Mon\n\n\nnew_bikes %&gt;% \n    filter(day_of_week == \"Sat\")\n## # A tibble: 2 √ó 5\n##   date       temp_feel humidity riders_registered day_of_week\n##   &lt;date&gt;         &lt;dbl&gt;    &lt;dbl&gt;             &lt;dbl&gt; &lt;chr&gt;      \n## 1 2011-01-01      64.7    0.806               654 Sat        \n## 2 2011-01-08      46.6    0.536               891 Sat\n\n\nnew_bikes %&gt;% \n    filter(riders_registered &gt; 850, day_of_week == \"Sat\")\n## # A tibble: 1 √ó 5\n##   date       temp_feel humidity riders_registered day_of_week\n##   &lt;date&gt;         &lt;dbl&gt;    &lt;dbl&gt;             &lt;dbl&gt; &lt;chr&gt;      \n## 1 2011-01-08      46.6    0.536               891 Sat",
    "crumbs": [
      "Simple Linear Regression - Intro+Formalization"
    ]
  },
  {
    "objectID": "activities/03_04-slr-intro-formalization.html#exercise-10-your-turn-complete-after-the-class",
    "href": "activities/03_04-slr-intro-formalization.html#exercise-10-your-turn-complete-after-the-class",
    "title": "Simple Linear Regression - Intro+Formalization",
    "section": "Exercise 10: Your turn [Complete after the class!]",
    "text": "Exercise 10: Your turn [Complete after the class!]\nUse dplyr verbs to complete each task below.\n\n# Keep only information about the humidity and day of week\n\n# Keep only information about the humidity and day of week using a different approach\n\n# Keep only information for Sundays\n\n# Keep only information for Sundays with temperatures below 50\n\n# Calculate the maximum and minimum temperatures",
    "crumbs": [
      "Simple Linear Regression - Intro+Formalization"
    ]
  },
  {
    "objectID": "activities/03_04-slr-intro-formalization.html#exercise-1-get-to-know-the-data-1",
    "href": "activities/03_04-slr-intro-formalization.html#exercise-1-get-to-know-the-data-1",
    "title": "Simple Linear Regression - Intro+Formalization",
    "section": "Exercise 1: Get to know the data",
    "text": "Exercise 1: Get to know the data\n\nCreate a new code chunk by clicking the green ‚ÄúC‚Äù button with a green + sign in the top right of the menu bar. In this code chunk, use an appropriate function to look at the first few rows of the data.\nCreate a new code chunk, and use an appropriate function to learn how much data we have (in terms of cases and variables).\nWhat does a case represent?\nNavigate to the FAQ page and read the response to the ‚ÄúHow does this site work? Do you just download results from the federations?‚Äù question. What do you learn about data quality and completeness from this response?",
    "crumbs": [
      "Simple Linear Regression - Intro+Formalization"
    ]
  },
  {
    "objectID": "activities/03_04-slr-intro-formalization.html#exercise-2-mutating-our-data",
    "href": "activities/03_04-slr-intro-formalization.html#exercise-2-mutating-our-data",
    "title": "Simple Linear Regression - Intro+Formalization",
    "section": "Exercise 2: Mutating our data",
    "text": "Exercise 2: Mutating our data\nStrength-to-weight ratio (SWR) is defined as TotalKg/BodyweightKg. We can use the mutate() function from the dplyr package to create a new variable in our dataframe for SWR using the following code:\n\n# The %&gt;% is called a \"pipe\" and feeds what comes before it\n# into what comes after (lifts data is \"fed into\" the mutate() function).\n# When creating a new variable, we often reassign the data frame to itself,\n# which updates the existing columns in lifts with the additional \"new\" column(s)\n# in lifts!\nlifts &lt;- lifts %&gt;% \n    mutate(NEW_VARIABLE_NAME = Age/BestSquatKg)\n## Error in `mutate()`:\n## ‚Ñπ In argument: `NEW_VARIABLE_NAME = Age/BestSquatKg`.\n## Caused by error:\n## ! object 'BestSquatKg' not found\n\nAdapt the example above to create a new variable called SWR, where SWR is defined as TotalKg/BodyweightKg.\n\nlifts &lt;- lifts %&gt;% \n    mutate(SWR = TotalKg / BodyweightKg)",
    "crumbs": [
      "Simple Linear Regression - Intro+Formalization"
    ]
  },
  {
    "objectID": "activities/03_04-slr-intro-formalization.html#exercise-3-get-to-know-the-outcomeresponse-variable",
    "href": "activities/03_04-slr-intro-formalization.html#exercise-3-get-to-know-the-outcomeresponse-variable",
    "title": "Simple Linear Regression - Intro+Formalization",
    "section": "Exercise 3: Get to know the outcome/response variable",
    "text": "Exercise 3: Get to know the outcome/response variable\nLet‚Äôs get acquainted with the SWR variable.\n\nConstruct an appropriate plot to visualize the distribution of this variable, and compute appropriate numerical summaries.\nWrite a good paragraph interpreting the plot and numerical summaries.",
    "crumbs": [
      "Simple Linear Regression - Intro+Formalization"
    ]
  },
  {
    "objectID": "activities/03_04-slr-intro-formalization.html#exercise-4-data-visualization---two-quantitative-variables",
    "href": "activities/03_04-slr-intro-formalization.html#exercise-4-data-visualization---two-quantitative-variables",
    "title": "Simple Linear Regression - Intro+Formalization",
    "section": "Exercise 4: Data visualization - two quantitative variables",
    "text": "Exercise 4: Data visualization - two quantitative variables\nWe‚Äôd like to visualize the relationship between body weight and the strength-to-weight ratio. A scatterplot (or informally, a ‚Äúpoint cloud‚Äù) allows us to do this! The code below creates a scatterplot of body weight vs.¬†SWR using ggplot().\n\n# scatterplot\n\n# The alpha = 0.5 in geom_point() adds transparency to the points\n# to make them easier to see. You can make this smaller for more transparency\nlifts %&gt;%\n  ggplot(aes(x = BodyweightKg, y = SWR)) +\n  geom_point(alpha = 0.5)\n\n\n\n\n\n\n\n\n\nThis is your second (!) bivariate data visualization (visualization for two variables)! What differences do you notice in the code structure when creating a bivariate visualization, compared to univariate visualizations we‚Äôve worked with before?\nWhat similarities do you notice in the code structure?\nDoes there appear to be some sort of pattern in the structure of the point cloud? Describe it, in no more than three sentences! Comment on the direction of the relationship between the two variables (positive? negative?) and the spread of the points (are they dispersed? close together?).",
    "crumbs": [
      "Simple Linear Regression - Intro+Formalization"
    ]
  },
  {
    "objectID": "activities/03_04-slr-intro-formalization.html#exercise-5-scatterplots---patterns-in-point-clouds",
    "href": "activities/03_04-slr-intro-formalization.html#exercise-5-scatterplots---patterns-in-point-clouds",
    "title": "Simple Linear Regression - Intro+Formalization",
    "section": "Exercise 5: Scatterplots - patterns in point clouds",
    "text": "Exercise 5: Scatterplots - patterns in point clouds\nSometimes, it can be easier to see a pattern in a point cloud by adding a smoothing line to our scatterplots. The code below adapts the code in Exercise 4 to do this:\n\n# scatterplot with smoothing line\nlifts %&gt;%\n  ggplot(aes(x = BodyweightKg, y = SWR)) +\n  geom_point(alpha = 0.5) +\n  geom_smooth()\n\n\n\n\n\n\n\n\n\nLook back at your answer to Exercise 4 (c). Does the smoothing line assist you in seeing a pattern, or change your answer at all? Why or why not?\nBased on the scatterplot with the smoothing line added above, does there appear to be a linear relationship between body weight and SWR (i.e.¬†would a straight line do a decent job at summarizing the relationship between these two variables)? Why or why not?",
    "crumbs": [
      "Simple Linear Regression - Intro+Formalization"
    ]
  },
  {
    "objectID": "activities/03_04-slr-intro-formalization.html#exercise-6-correlation",
    "href": "activities/03_04-slr-intro-formalization.html#exercise-6-correlation",
    "title": "Simple Linear Regression - Intro+Formalization",
    "section": "Exercise 6: Correlation",
    "text": "Exercise 6: Correlation\nWe can quantify the linear relationship between two quantitative variables using a numerical summary known as correlation (sometimes known as a ‚Äúcorrelation coefficient‚Äù or ‚ÄúPearson‚Äôs correlation‚Äù). Correlation can range from -1 to 1, where a correlation of 0 indicates that there is no linear relationship between the two quantitative variables.\nBelow is an example of a ‚ÄúMath Box‚Äù. You‚Äôll see these occasionally throughout the activities. You are not required to memorize, nor will you be assessed on, anything in the math boxes. If you plan on continuing with Statistics courses at Macalester (or are interested in the math behind everything!), these math boxes are for you!\n\n\n\n\n\n\nCorrelation\n\n\n\n\n\nThe Pearson correlation coefficient, \\(r_{x, y}\\), of \\(x\\) and \\(y\\) is the (almost) average of products of the z-scores of variables \\(x\\) and \\(y\\):\n\\[\nr_{x, y} = \\frac{\\sum z_x z_y}{n - 1}\n\\]\n\n\n\nIn general, we will want to be able to describe (qualitatively) two aspects of correlation:\n\nStrength\n\n\nIs the correlation between x and y strong, or weak, i.e.¬†how closely do the points fit around a line? This has to do with how dispersed our point clouds are.\n\n\nDirection\n\n\nIs the correlation between x and y positive or negative, i.e.¬†does y go ‚Äúup‚Äù when x goes ‚Äúup‚Äù (positive), or does y go ‚Äúdown‚Äù when x goes ‚Äúup‚Äù (negative)?\n\nStronger correlations will be further from 0 (closer to -1 or 1), and positive and negative correlations will have the appropriate respective sign (above or below zero).\n\nRather than a smooth trend line, we can force the line we add to our scatterplots to be linear using geom_smooth(method = 'lm'), as below:\n\n\n# scatterplot with linear trend line\nlifts %&gt;%\n  ggplot(aes(x = BodyweightKg, y = SWR)) +\n  geom_point(alpha = 0.5) +\n  geom_smooth(method = \"lm\")\n\n\n\n\n\n\n\n\n\nBased on the above scatterplot, how would you describe the correlation between body weight and SWR, in terms of strength and direction?\nMake a guess as to what numerical value the correlation between body weight and SWR will have, based on your response to part (b).",
    "crumbs": [
      "Simple Linear Regression - Intro+Formalization"
    ]
  },
  {
    "objectID": "activities/03_04-slr-intro-formalization.html#exercise-7-computing-correlation-in-r",
    "href": "activities/03_04-slr-intro-formalization.html#exercise-7-computing-correlation-in-r",
    "title": "Simple Linear Regression - Intro+Formalization",
    "section": "Exercise 7: Computing correlation in R",
    "text": "Exercise 7: Computing correlation in R\nWe can compute the correlation between body weight and SWR using summarize and cor functions:\n\n# correlation\n\n# Note: the order in which you put your two quantitative variables into the cor\n# function doesn't matter! Try switching them around to confirm this for yourself\n# Because of the missing data, we need to include the use = \"complete.obs\" - otherwise the correlation would be computed as NA\nlifts %&gt;%\n    summarize(cor(SWR, BodyweightKg, use = \"complete.obs\"))\n## # A tibble: 1 √ó 1\n##   `cor(SWR, BodyweightKg, use = \"complete.obs\")`\n##                                            &lt;dbl&gt;\n## 1                                        -0.0392\n\nIs the computed correlation close to what you guessed in Exercise 6 part (c)?",
    "crumbs": [
      "Simple Linear Regression - Intro+Formalization"
    ]
  },
  {
    "objectID": "activities/03_04-slr-intro-formalization.html#exercise-8-limitations-of-correlation",
    "href": "activities/03_04-slr-intro-formalization.html#exercise-8-limitations-of-correlation",
    "title": "Simple Linear Regression - Intro+Formalization",
    "section": "Exercise 8: Limitations of correlation",
    "text": "Exercise 8: Limitations of correlation\nWe previously noted that correlation was a numerical summary of the linear relationship between two variables. We‚Äôll now go through some examples of relationships between quantitative variables to demonstrate why it is incredibly important to visualize our data in addition to just computing numerical summaries!\nFor this exercise, we‚Äôll be working with the anscombe dataset, which is built in to R. To load this dataset into our environment, we run the following code:\n\n# load anscombe data\ndata(\"anscombe\")\n\nThe anscombe dataset contains four different pairs of quantitative variables:\n\nx1, y1\nx2, y2\nx3, y3\nx4, y4\n\nAdapt the code we used in Exercise 7 to compute the correlation between each of these four pairs of variables, below:\n\n# correlation between x1, y1\n\n# correlation between x2, y2\n\n# correlation between x3, y3\n\n# correlation between x4, y4\n\n\nWhat do you notice about each of these correlations (if the answer to this isn‚Äôt obvious, double-check your code)?\nDescribe these correlations in terms of strength and direction, using only the numerical summary to assist you in your description.\nDraw an example on the white board or at your tables of what you think the point clouds for these pairs of variables might look like. There are only 11 observations, so you can draw all 11 points if you‚Äôd like!\nAdapt the code for scatterplots given previously in this activity to make four distinct scatterplots for each pair of quantitative variables in the anscombe dataset. You do not need to add a smooth trend line or a linear trend line to these plots.\n\n\n# scatterplot: x1, y1\n\n# scatterplot: x2, y2\n\n# scatterplot: x3, y3\n\n# scatterplot: x4, y4\n\n\nBased on the correlations you calculated and scatterplots you made, what is the message of this last exercise as it relates to the limits of correlation?",
    "crumbs": [
      "Simple Linear Regression - Intro+Formalization"
    ]
  },
  {
    "objectID": "activities/03_04-slr-intro-formalization.html#reflection",
    "href": "activities/03_04-slr-intro-formalization.html#reflection",
    "title": "Simple Linear Regression - Intro+Formalization",
    "section": "Reflection",
    "text": "Reflection\nMuch of statistics is about making (hopefully) reasonable assumptions in attempt to summarize observed relationships in data. Today we started considering assumptions of linear relationships between quantitative variables.\nReview the learning objectives at the top of this file and today‚Äôs activity. How do you imagine assumptions of linearity might be useful in terms of quantifying relationships between quantitative variables? How do you imagine these assumptions could sometimes fall short, or even be unethical in certain cases?\n\nResponse: Put your response here.",
    "crumbs": [
      "Simple Linear Regression - Intro+Formalization"
    ]
  },
  {
    "objectID": "activities/03_04-slr-intro-formalization.html#exercise-9-lines-of-best-fit",
    "href": "activities/03_04-slr-intro-formalization.html#exercise-9-lines-of-best-fit",
    "title": "Simple Linear Regression - Intro+Formalization",
    "section": "Exercise 9: Lines of best fit",
    "text": "Exercise 9: Lines of best fit\nIn this activity, we‚Äôve learned how to fit straight lines to data, to help us visualize the relationship between two quantitative variables. So far, ggplot has chosen the line for us. How does it know which line is ‚Äúbest‚Äù, and what does ‚Äúbest‚Äù even mean?\nFor this exercise, we‚Äôll consider the relationship between x1 and y1 in the anscombe dataset. Run the following code, which creates a scatterplot with a fitted line to our data using the function geom_abline:\n\n# scatterplot with a fitted line, whose slope is 0.4 and intercept is 3\nanscombe %&gt;%\n  ggplot(aes(x = x1, y = y1)) +\n  geom_point() +\n  geom_abline(slope = 0.4, intercept = 3, col = \"blue\", size = 1)\n\n\n\n\n\n\n\n\nDescribe the line that you see. Do you think the line is ‚Äúgood‚Äù? What are you using to define ‚Äúgood‚Äù?\nSome things to think about:\n\nHow many points are above the line?\nHow many points are below the line?\nAre the distances of the points above and below the line roughly similar, or is there meaningful difference?\n\nNow we‚Äôll add another line to our plot. Which line do you think is better suited for this data? Why? Be specific!\n\n# scatterplot with a fitted line, whose slope is 0.4 and intercept is 3\nanscombe %&gt;%\n  ggplot(aes(x = x1, y = y1)) +\n  geom_point() +\n  geom_abline(slope = 0.4, intercept = 3, col = \"blue\", size = 1) +\n  geom_abline(slope = 0.5, intercept = 4, col = \"orange\", size = 1)\n\n\n\n\n\n\n\n\nIt‚Äôs usually quite simple to note when a line is bad, but more difficult to quantify when a line is a good fit for our data. Consider the following line:\n\n# scatterplot with a fitted line, whose slope is 0.4 and intercept is 3\nanscombe %&gt;%\n  ggplot(aes(x = x1, y = y1)) +\n  geom_point() +\n  geom_abline(slope = -0.5, intercept = 10, col = \"red\", size = 1) \n\n\n\n\n\n\n\n\nIn the next activity, we‚Äôll formalize the principle of least squares, which will give us one particular definition of a line of best fit that is commonly used in statistics! We‚Äôll take advantage of the vertical distances between each point and the fitted line (residuals), which will help us define (mathematically) a line that best fits our data:\n\nlibrary(broom)\nanscombe %&gt;%\n  lm(y1 ~ x1, data = .) %&gt;%\n  augment() %&gt;%\n  ggplot(aes(x = x1, y = y1)) +\n  geom_smooth(method = \"lm\", se = FALSE) +\n  geom_segment(aes(xend = x1, yend = .fitted), col = \"red\") +\n  geom_point()",
    "crumbs": [
      "Simple Linear Regression - Intro+Formalization"
    ]
  },
  {
    "objectID": "activities/03_04-slr-intro-formalization.html#exercise-10-correlation-and-extreme-values",
    "href": "activities/03_04-slr-intro-formalization.html#exercise-10-correlation-and-extreme-values",
    "title": "Simple Linear Regression - Intro+Formalization",
    "section": "Exercise 10: Correlation and extreme values",
    "text": "Exercise 10: Correlation and extreme values\nIn this exercise, we‚Äôll explore how correlation changes with the addition of extreme values, or observations. We‚Äôll begin by generating a toy dataset called dat with two quantitative variables, x and y. Run the code below to create the dataset.\nwhile not required, recall that you can look up function documentation in R using the ? in front of a function name to figure out what that function is doing!\n\n# create a toy dataset\nset.seed(1234)\nx &lt;- rnorm(100, mean = 5, sd = 2)\ny &lt;- -3 * x + rnorm(100, sd = 4)\ndat &lt;- data.frame(x = x, y = y)\n\n\nMake a scatterplot of x vs.¬†y.\n\n\n# scatterplot\n\n\nBased on your scatterplot, describe the correlation between x and y in terms of strength and direction.\nGuess the correlation (the numerical value) between x and y.\nCompute the correlation between x and y. Was your guess from part (c) close?\n\n\n# correlation\n\n\nSuppose we observe an additional observation with x = 15 and y = -45. We can create a new data frame, dat_new1, that contains this observation in addition to the original ones as follows:\n\n\n# creating dat_new1\nx1 &lt;- c(x, 15)\ny1 &lt;- c(y, -45)\ndat_new1 &lt;- data.frame(x = x1, y = y1)\n\n\nMake a scatterplot of x vs.¬†y for this new data frame, and compute the correlation between x and y. Did your correlation change very much with the addition of this observation? Hypothesize why or why not.\n\n\n# scatterplot\n\n# correlation\n\n\nSuppose instead of our additional observation having values x = 15 and y = -45, we instead observe x = 15 and y = -15. We can create a new data frame, dat_new2, that contains this observation in addition to the original ones as follows:\n\n\n# creating dat_new1\nx2 &lt;- c(x, 15)\ny2 &lt;- c(y, 45)\ndat_new2 &lt;- data.frame(x = x2, y = y2)\n\n\nMake a scatterplot of x vs.¬†y for this new data frame, and compute the correlation between x and y. Did your correlation change very much with the addition of this observation? Hypothesize why or why not.\n\n\n# scatterplot\n\n# correlation\n\n\nWhat do you think the takeaway message is of this exercise?\n\n\nChallenge Add linear trend lines to your scatterplots from parts (f) and (h). Does this give you any additional insight into why the correlations may have changed in different ways with the addition of a new observation?",
    "crumbs": [
      "Simple Linear Regression - Intro+Formalization"
    ]
  },
  {
    "objectID": "activities/03_04-slr-intro-formalization.html#exercise-1-get-to-know-the-data-2",
    "href": "activities/03_04-slr-intro-formalization.html#exercise-1-get-to-know-the-data-2",
    "title": "Simple Linear Regression - Intro+Formalization",
    "section": "Exercise 1: Get to know the data",
    "text": "Exercise 1: Get to know the data\n\ndim(bikes)\n## [1] 731  15\n\nhead(bikes)\n## # A tibble: 6 √ó 15\n##   date       season  year month day_of_week weekend holiday temp_actual\n##   &lt;date&gt;     &lt;chr&gt;  &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt;       &lt;lgl&gt;   &lt;chr&gt;         &lt;dbl&gt;\n## 1 2011-01-01 winter  2011 Jan   Sat         TRUE    no             57.4\n## 2 2011-01-02 winter  2011 Jan   Sun         TRUE    no             58.8\n## 3 2011-01-03 winter  2011 Jan   Mon         FALSE   no             46.5\n## 4 2011-01-04 winter  2011 Jan   Tue         FALSE   no             46.8\n## 5 2011-01-05 winter  2011 Jan   Wed         FALSE   no             48.7\n## 6 2011-01-06 winter  2011 Jan   Thu         FALSE   no             47.1\n## # ‚Ñπ 7 more variables: temp_feel &lt;dbl&gt;, humidity &lt;dbl&gt;, windspeed &lt;dbl&gt;,\n## #   weather_cat &lt;chr&gt;, riders_casual &lt;dbl&gt;, riders_registered &lt;dbl&gt;,\n## #   riders_total &lt;dbl&gt;\n\n\nA case represents a day of the year.\nWe have 15 variables broadly concerning weather, day of week information, whether the day is a holiday.\nLots of answers are reasonable here! When and where seem to be particularly relevant because this is for a rideshare based in Washington DC with data from 2011-2012. Ridership likely changes a lot from city to city and over time.",
    "crumbs": [
      "Simple Linear Regression - Intro+Formalization"
    ]
  },
  {
    "objectID": "activities/03_04-slr-intro-formalization.html#exercise-2-get-to-know-the-outcomeresponse-variable-1",
    "href": "activities/03_04-slr-intro-formalization.html#exercise-2-get-to-know-the-outcomeresponse-variable-1",
    "title": "Simple Linear Regression - Intro+Formalization",
    "section": "Exercise 2: Get to know the outcome/response variable",
    "text": "Exercise 2: Get to know the outcome/response variable\nThe distribution of the riders_registered variable looks fairly symmetric. On average there are about 3600 registered riders per day (mean = 3656, median = 3662). On any given day, the number of registered riders is about 1560 from the mean. There seem to be a small number of low outliers (minimum ridership was 20).\n\nggplot(bikes, aes(x = riders_registered)) +\n    geom_histogram()\n\n\n\n\n\n\n\n\nggplot(bikes, aes(y = riders_registered)) +\n    geom_boxplot()\n\n\n\n\n\n\n\n\nsummary(bikes$riders_registered)\n##    Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n##      20    2497    3662    3656    4776    6946\n\nbikes %&gt;% \n    summarize(sd(riders_registered))\n## # A tibble: 1 √ó 1\n##   `sd(riders_registered)`\n##                     &lt;dbl&gt;\n## 1                   1560.",
    "crumbs": [
      "Simple Linear Regression - Intro+Formalization"
    ]
  },
  {
    "objectID": "activities/03_04-slr-intro-formalization.html#exercise-3-explore-the-relationship-between-ridership-and-temperature-1",
    "href": "activities/03_04-slr-intro-formalization.html#exercise-3-explore-the-relationship-between-ridership-and-temperature-1",
    "title": "Simple Linear Regression - Intro+Formalization",
    "section": "Exercise 3: Explore the relationship between ridership and temperature",
    "text": "Exercise 3: Explore the relationship between ridership and temperature\nWe‚Äôd like to understand how daily ridership among registered users relates with the temperature that it feels like that day (temp_feel).\n\nScatterplot (outcome and predictor are both quantitative)\n\n\n\nggplot(bikes, aes(x = temp_feel, y = riders_registered)) +\n    geom_point()\n\n\n\n\n\n\n\n\n\nIf we only displayed the red line of best fit on the plot, we might miss the slight downward trend at the highest temperatures that we can see more clearly with the blue curve of best fit. A linear model is not appropriate if fit to the whole range of the data, but there does seem to be a linear relationship between ridership and temperature below 80 degrees Fahrenheit.\n\n\n# Add a red straight line of best fit and a blue curve of best fit\nggplot(bikes, aes(x = temp_feel, y = riders_registered)) +\n    geom_point() +\n    geom_smooth(method = \"lm\", color = \"red\", se = FALSE) +\n    geom_smooth(color = \"blue\", se = FALSE)",
    "crumbs": [
      "Simple Linear Regression - Intro+Formalization"
    ]
  },
  {
    "objectID": "activities/03_04-slr-intro-formalization.html#exercise-4-filtering-our-data-1",
    "href": "activities/03_04-slr-intro-formalization.html#exercise-4-filtering-our-data-1",
    "title": "Simple Linear Regression - Intro+Formalization",
    "section": "Exercise 4: Filtering our data",
    "text": "Exercise 4: Filtering our data\n\n# The %&gt;% is called a \"pipe\" and feeds what comes before it\n# into what comes after (bikes data is \"fed into\" the filter() function)\nbikes_sub &lt;- bikes %&gt;% \n    filter(temp_feel &lt; 80)",
    "crumbs": [
      "Simple Linear Regression - Intro+Formalization"
    ]
  },
  {
    "objectID": "activities/03_04-slr-intro-formalization.html#exercise-5-model-fitting-and-coefficient-interpretation-1",
    "href": "activities/03_04-slr-intro-formalization.html#exercise-5-model-fitting-and-coefficient-interpretation-1",
    "title": "Simple Linear Regression - Intro+Formalization",
    "section": "Exercise 5: Model fitting and coefficient interpretation",
    "text": "Exercise 5: Model fitting and coefficient interpretation\nLet‚Äôs fit a simple linear regression model and examine the results. Step through code chunk slowly, and make note of new code.\n\n# Construct and save the model as bike_mod\n# What's the purpose of \"riders_registered ~ temp_feel\"?\n# What's the purpose of \"data = bikes_sub\"?\nbike_mod &lt;- lm(riders_registered ~ temp_feel, data = bikes_sub)\n\n\n# A long summary of the model stored in bike_mod\nsummary(bike_mod)\n## \n## Call:\n## lm(formula = riders_registered ~ temp_feel, data = bikes_sub)\n## \n## Residuals:\n##     Min      1Q  Median      3Q     Max \n## -3681.8  -928.3   -98.6   904.9  3496.7 \n## \n## Coefficients:\n##              Estimate Std. Error t value Pr(&gt;|t|)    \n## (Intercept) -2486.412    421.379  -5.901 7.37e-09 ***\n## temp_feel      86.493      6.464  13.380  &lt; 2e-16 ***\n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n## \n## Residual standard error: 1267 on 428 degrees of freedom\n## Multiple R-squared:  0.2949, Adjusted R-squared:  0.2933 \n## F-statistic:   179 on 1 and 428 DF,  p-value: &lt; 2.2e-16\n\n\n# A simplified model summary\ncoef(summary(bike_mod))\n##                Estimate Std. Error   t value     Pr(&gt;|t|)\n## (Intercept) -2486.41180 421.379174 -5.900652 7.368345e-09\n## temp_feel      86.49251   6.464247 13.380135 2.349753e-34\n\n\nE[riders_registered | temp_feel] = -2486.41180 + 86.49251 * temp_feel\nIntercept interpretation: On days that feel like 0 degrees Fahrenheit, we can expect an average of -2486.41180 riders‚Äîa negative number of riders doesn‚Äôt make sense! This results because of extrapolation‚Äî0 degrees is so far below the minimum temperature in the data. We only have information on the relationship between ridership and temperature in the ~40-100 degree range and have no idea what that relationship looks like outside that range.\nSlope interpretation: Every 1 degree increase in feeling temperature is associated with an average of about 86 more riders.",
    "crumbs": [
      "Simple Linear Regression - Intro+Formalization"
    ]
  },
  {
    "objectID": "activities/03_04-slr-intro-formalization.html#exercise-6-predictions-and-residuals-1",
    "href": "activities/03_04-slr-intro-formalization.html#exercise-6-predictions-and-residuals-1",
    "title": "Simple Linear Regression - Intro+Formalization",
    "section": "Exercise 6: Predictions and residuals",
    "text": "Exercise 6: Predictions and residuals\nOn August 17, 2012, the temp_feel was 53.816 degrees and there were 5665 riders. We can get data for this day using the filter() and select() dplyr functions. Note, but don‚Äôt worry about the syntax ‚Äì we haven‚Äôt learned this yet:\n\nbikes_sub %&gt;% \n    filter(date == \"2012-08-17\") %&gt;% \n    select(riders_registered, temp_feel) \n## # A tibble: 1 √ó 2\n##   riders_registered temp_feel\n##               &lt;dbl&gt;     &lt;dbl&gt;\n## 1              5665      53.8\n\n\nMore riders than expected ‚Äì the point is far above the trend line\n-2486.41180 + 86.49251 * 53.816 = 2168.269\nWe get the same result with predict():\n\n\n# What is the purpose of newdata = ___???\npredict(bike_mod, newdata = data.frame(temp_feel = 53.816))\n##        1 \n## 2168.269\n\n\nresidual = 5665 - 2168.269 = 3496.731. On August 17, 2012, there were 3496.731 more riders than would be expected from our model.\n\nPositive residuals are above the trend line‚Äîwe under-estimate ridership.\nNegative residuals are below the trend line‚Äîwe over-estimate ridership.\n\nOn an 85 degree day, we would predict 4865.452 riders. Even though we can compute this prediction, it‚Äôs not a good idea because of extrapolation‚Äìthe data that we used to fit our model was filtered to days less than 80 degrees.\n\n\n-2486.41180 + 86.49251 * 85\n## [1] 4865.452\npredict(bike_mod, newdata = data.frame(temp_feel = 85))\n##        1 \n## 4865.451",
    "crumbs": [
      "Simple Linear Regression - Intro+Formalization"
    ]
  },
  {
    "objectID": "activities/03_04-slr-intro-formalization.html#exercise-7-changing-temperature-units-challenge",
    "href": "activities/03_04-slr-intro-formalization.html#exercise-7-changing-temperature-units-challenge",
    "title": "Simple Linear Regression - Intro+Formalization",
    "section": "Exercise 7: Changing temperature units (CHALLENGE)",
    "text": "Exercise 7: Changing temperature units (CHALLENGE)\nIf we had measured temperature in degrees Celsius rather than degrees Fahrenheit, both the intercept and slope should change. The intercept would now represent 0 degrees Celsius (32 degrees Fahrenheit) and a one unit change in temperature is now 1 degree Celsius (1.8 degrees Fahrenheit).",
    "crumbs": [
      "Simple Linear Regression - Intro+Formalization"
    ]
  },
  {
    "objectID": "activities/03_04-slr-intro-formalization.html#exercise-8-ridership-and-windspeed-1",
    "href": "activities/03_04-slr-intro-formalization.html#exercise-8-ridership-and-windspeed-1",
    "title": "Simple Linear Regression - Intro+Formalization",
    "section": "Exercise 8: Ridership and windspeed",
    "text": "Exercise 8: Ridership and windspeed\nLet‚Äôs pull together everything that you‚Äôve practiced in the preceding exercises to investigate the relationship between riders_registered and windspeed. Go back to using the bikes dataset (instead of bikes_sub) because we no longer need to only keep days less than 80 degrees.\n\n# Construct and interpret a visualization of this relationship\n# Include a representation of the relationship trend\nggplot(bikes, aes(x = windspeed, y = riders_registered)) + \n    geom_point() + \n    geom_smooth(method = \"lm\", color = \"red\", se = FALSE) +\n    geom_smooth(color = \"blue\", se = FALSE)\n\n\n\n\n\n\n\n\n# Use lm to construct a model of riders_registered vs windspeed\n# Save this as bike_mod2\nbike_mod2 &lt;- lm(riders_registered ~ windspeed, data = bikes)\n\n# Get a short summary of this model\ncoef(summary(bike_mod2))\n##               Estimate Std. Error   t value      Pr(&gt;|t|)\n## (Intercept) 4490.09761  149.65992 30.002005 2.023179e-129\n## windspeed    -65.34145   10.86299 -6.015053  2.844453e-09\n\n\nThere‚Äôs a weak, negative relationship ‚Äì ridership tends to be smaller on windier days.\nE[riders_registered | windspeed] = 4490.09761 - 65.34145 windspeed\n\nIntercept: On days with no wind, we‚Äôd expect around 4490 riders. (0 is a little below the minimum of the observed data, but not by much! So extrapolation in interpreting the intercept isn‚Äôt a huge concern.)\nSlope: Every 1mph increase in windspeed is associated with a ridership decrease of 65 riders on average.\n\nSee the code below to predict ridership on August 17, 2012 and calculate the corresponding residual. Note that this residual is smaller than the residual from the temperature model (that residual was 3496.731). This indicates that August 17 was more of an outlier in ridership given the temperature than the windspeed.\n\n\nbikes %&gt;% \n    filter(date == \"2012-08-17\") %&gt;% \n    select(riders_registered, windspeed)\n## # A tibble: 1 √ó 2\n##   riders_registered windspeed\n##               &lt;dbl&gt;     &lt;dbl&gt;\n## 1              5665      15.5\n\n# prediction\n4490.09761 - 65.34145 * 15.50072\n## [1] 3477.258\n\n# residual \n5665 - 3477.258\n## [1] 2187.742",
    "crumbs": [
      "Simple Linear Regression - Intro+Formalization"
    ]
  },
  {
    "objectID": "activities/03_04-slr-intro-formalization.html#exercise-9-data-drills-filter-select-summarize",
    "href": "activities/03_04-slr-intro-formalization.html#exercise-9-data-drills-filter-select-summarize",
    "title": "Simple Linear Regression - Intro+Formalization",
    "section": "Exercise 9: Data drills (filter, select, summarize)",
    "text": "Exercise 9: Data drills (filter, select, summarize)\nThis exercise is designed to help you keep building your dplyr skills. These skills are important to data cleaning and digging, which in turn is important to really making meaning of our data. We‚Äôll work with a simpler set of 10 data points:\n\nnew_bikes &lt;- bikes %&gt;% \n    select(date, temp_feel, humidity, riders_registered, day_of_week) %&gt;% \n    head(10)\n\n\nVerb 1: summarize\nsummarize() calculates numerical summaries of variables (columns).\n\nnew_bikes %&gt;% \n    summarize(mean(temp_feel), mean(humidity))\n## # A tibble: 1 √ó 2\n##   `mean(temp_feel)` `mean(humidity)`\n##               &lt;dbl&gt;            &lt;dbl&gt;\n## 1              52.0            0.544\n\n\n\nVerb 2: select\nselect() selects variables (columns).\n\nnew_bikes %&gt;%\n    select(date, temp_feel)\n## # A tibble: 10 √ó 2\n##    date       temp_feel\n##    &lt;date&gt;         &lt;dbl&gt;\n##  1 2011-01-01      64.7\n##  2 2011-01-02      63.8\n##  3 2011-01-03      49.0\n##  4 2011-01-04      51.1\n##  5 2011-01-05      52.6\n##  6 2011-01-06      53.0\n##  7 2011-01-07      50.8\n##  8 2011-01-08      46.6\n##  9 2011-01-09      42.5\n## 10 2011-01-10      45.6\n\n\nnew_bikes %&gt;% \n    select(-date, -temp_feel)\n## # A tibble: 10 √ó 3\n##    humidity riders_registered day_of_week\n##       &lt;dbl&gt;             &lt;dbl&gt; &lt;chr&gt;      \n##  1    0.806               654 Sat        \n##  2    0.696               670 Sun        \n##  3    0.437              1229 Mon        \n##  4    0.590              1454 Tue        \n##  5    0.437              1518 Wed        \n##  6    0.518              1518 Thu        \n##  7    0.499              1362 Fri        \n##  8    0.536               891 Sat        \n##  9    0.434               768 Sun        \n## 10    0.483              1280 Mon\n\n\n\nVerb 3: filter\nfilter() keeps only days (rows) that meet the given condition(s).\n\nnew_bikes %&gt;% \n    filter(riders_registered &gt; 850)\n## # A tibble: 7 √ó 5\n##   date       temp_feel humidity riders_registered day_of_week\n##   &lt;date&gt;         &lt;dbl&gt;    &lt;dbl&gt;             &lt;dbl&gt; &lt;chr&gt;      \n## 1 2011-01-03      49.0    0.437              1229 Mon        \n## 2 2011-01-04      51.1    0.590              1454 Tue        \n## 3 2011-01-05      52.6    0.437              1518 Wed        \n## 4 2011-01-06      53.0    0.518              1518 Thu        \n## 5 2011-01-07      50.8    0.499              1362 Fri        \n## 6 2011-01-08      46.6    0.536               891 Sat        \n## 7 2011-01-10      45.6    0.483              1280 Mon\n\n\nnew_bikes %&gt;% \n    filter(day_of_week == \"Sat\")\n## # A tibble: 2 √ó 5\n##   date       temp_feel humidity riders_registered day_of_week\n##   &lt;date&gt;         &lt;dbl&gt;    &lt;dbl&gt;             &lt;dbl&gt; &lt;chr&gt;      \n## 1 2011-01-01      64.7    0.806               654 Sat        \n## 2 2011-01-08      46.6    0.536               891 Sat\n\n\nnew_bikes %&gt;% \n    filter(riders_registered &gt; 850, day_of_week == \"Sat\")\n## # A tibble: 1 √ó 5\n##   date       temp_feel humidity riders_registered day_of_week\n##   &lt;date&gt;         &lt;dbl&gt;    &lt;dbl&gt;             &lt;dbl&gt; &lt;chr&gt;      \n## 1 2011-01-08      46.6    0.536               891 Sat",
    "crumbs": [
      "Simple Linear Regression - Intro+Formalization"
    ]
  },
  {
    "objectID": "activities/03_04-slr-intro-formalization.html#exercise-10-your-turn",
    "href": "activities/03_04-slr-intro-formalization.html#exercise-10-your-turn",
    "title": "Simple Linear Regression - Intro+Formalization",
    "section": "Exercise 10: Your turn",
    "text": "Exercise 10: Your turn\nUse dplyr verbs to complete each task below.\n\n# Keep only information about the humidity and day of week\nnew_bikes %&gt;% \n    select(humidity, day_of_week)\n## # A tibble: 10 √ó 2\n##    humidity day_of_week\n##       &lt;dbl&gt; &lt;chr&gt;      \n##  1    0.806 Sat        \n##  2    0.696 Sun        \n##  3    0.437 Mon        \n##  4    0.590 Tue        \n##  5    0.437 Wed        \n##  6    0.518 Thu        \n##  7    0.499 Fri        \n##  8    0.536 Sat        \n##  9    0.434 Sun        \n## 10    0.483 Mon\n\n# Keep only information about the humidity and day of week using a different approach\nnew_bikes %&gt;% \n    select(-date, -temp_feel, -riders_registered)\n## # A tibble: 10 √ó 2\n##    humidity day_of_week\n##       &lt;dbl&gt; &lt;chr&gt;      \n##  1    0.806 Sat        \n##  2    0.696 Sun        \n##  3    0.437 Mon        \n##  4    0.590 Tue        \n##  5    0.437 Wed        \n##  6    0.518 Thu        \n##  7    0.499 Fri        \n##  8    0.536 Sat        \n##  9    0.434 Sun        \n## 10    0.483 Mon\n\n# Keep only information for Sundays\nnew_bikes %&gt;% \n    filter(day_of_week == \"Sun\")\n## # A tibble: 2 √ó 5\n##   date       temp_feel humidity riders_registered day_of_week\n##   &lt;date&gt;         &lt;dbl&gt;    &lt;dbl&gt;             &lt;dbl&gt; &lt;chr&gt;      \n## 1 2011-01-02      63.8    0.696               670 Sun        \n## 2 2011-01-09      42.5    0.434               768 Sun\n\n# Keep only information for Sundays with temperatures below 50\nnew_bikes %&gt;% \n    filter(day_of_week == \"Sun\", temp_feel &lt; 50)\n## # A tibble: 1 √ó 5\n##   date       temp_feel humidity riders_registered day_of_week\n##   &lt;date&gt;         &lt;dbl&gt;    &lt;dbl&gt;             &lt;dbl&gt; &lt;chr&gt;      \n## 1 2011-01-09      42.5    0.434               768 Sun\n\n# Calculate the maximum and minimum temperatures\nnew_bikes %&gt;% \n    summarize(min(temp_feel), max(temp_feel))\n## # A tibble: 1 √ó 2\n##   `min(temp_feel)` `max(temp_feel)`\n##              &lt;dbl&gt;            &lt;dbl&gt;\n## 1             42.5             64.7",
    "crumbs": [
      "Simple Linear Regression - Intro+Formalization"
    ]
  },
  {
    "objectID": "activities/03_04-slr-intro-formalization.html#exercise-1-get-to-know-the-data-3",
    "href": "activities/03_04-slr-intro-formalization.html#exercise-1-get-to-know-the-data-3",
    "title": "Simple Linear Regression - Intro+Formalization",
    "section": "Exercise 1: Get to know the data",
    "text": "Exercise 1: Get to know the data\n\nUse an appropriate function to look at the first few rows of the data.\n\n\nhead(lifts)\n## # A tibble: 6 √ó 21\n##   Name        Sex   Event Equipment   Age BodyweightKg Best3SquatKg Best3BenchKg\n##   &lt;chr&gt;       &lt;chr&gt; &lt;chr&gt; &lt;chr&gt;     &lt;dbl&gt;        &lt;dbl&gt;        &lt;dbl&gt;        &lt;dbl&gt;\n## 1 Natalya Po‚Ä¶ F     D     Raw        37           58.4          NA          NA  \n## 2 Fatima Rod‚Ä¶ F     SBD   Single-p‚Ä¶  NA           74.8          NA          NA  \n## 3 Josh Kelley M     SBD   Single-p‚Ä¶  NA           72.4         147.         97.5\n## 4 Timothy Ca‚Ä¶ M     D     Raw        16           72.9          NA          NA  \n## 5 M Moynihan  M     B     Raw        NA           67.5          NA         100  \n## 6 Lucas Wegr‚Ä¶ M     B     Raw        23.5        103.           NA         188. \n## # ‚Ñπ 13 more variables: Best3DeadliftKg &lt;dbl&gt;, TotalKg &lt;dbl&gt;, Place &lt;chr&gt;,\n## #   Dots &lt;dbl&gt;, Wilks &lt;dbl&gt;, Glossbrenner &lt;dbl&gt;, Goodlift &lt;dbl&gt;, Tested &lt;chr&gt;,\n## #   Country &lt;chr&gt;, State &lt;chr&gt;, Date &lt;date&gt;, MeetCountry &lt;chr&gt;, MeetState &lt;chr&gt;\n\n\nCreate a new code chunk, and use an appropriate function to learn how much data we have (in terms of cases and variables).\n\n\ndim(lifts)\n## [1] 100000     21\n\n\nA case represents an individual lifter at a single weightlifting competition.\nIt looks like some meets may be missing if they weren‚Äôt detected by the web scraper used by the maintainers of the Open Powerlifting database. They don‚Äôt describe in detail the process used for transferring PDFs of results to their database, so it‚Äôs unclear what errors in transcription might have resulted. Still, it‚Äôs worth taking a moment to appreciate the labor they put into making these results available for passionate powerlifters to explore.",
    "crumbs": [
      "Simple Linear Regression - Intro+Formalization"
    ]
  },
  {
    "objectID": "activities/03_04-slr-intro-formalization.html#exercise-2-mutating-our-data-1",
    "href": "activities/03_04-slr-intro-formalization.html#exercise-2-mutating-our-data-1",
    "title": "Simple Linear Regression - Intro+Formalization",
    "section": "Exercise 2: Mutating our data",
    "text": "Exercise 2: Mutating our data\nStrength-to-weight ratio (SWR) is defined as TotalKg/BodyweightKg. We can use the mutate() function from the dplyr package to create a new variable in our dataframe for SWR using the following code:\n\nlifts &lt;- lifts %&gt;% \n    mutate(SWR = TotalKg / BodyweightKg)\n\nAdapt the example above to create a new variable called SWR, where SWR is defined as TotalKg/BodyweightKg.",
    "crumbs": [
      "Simple Linear Regression - Intro+Formalization"
    ]
  },
  {
    "objectID": "activities/03_04-slr-intro-formalization.html#exercise-3-get-to-know-the-outcomeresponse-variable-1",
    "href": "activities/03_04-slr-intro-formalization.html#exercise-3-get-to-know-the-outcomeresponse-variable-1",
    "title": "Simple Linear Regression - Intro+Formalization",
    "section": "Exercise 3: Get to know the outcome/response variable",
    "text": "Exercise 3: Get to know the outcome/response variable\nLet‚Äôs get acquainted with the SWR variable.\n\nConstruct an appropriate plot to visualize the distribution of this variable, and compute appropriate numerical summaries.\n\n\nlifts %&gt;%\n  ggplot(aes(SWR)) +\n  geom_histogram(bins = 10, col = \"black\")\n\n\n\n\n\n\n\n\nlifts %&gt;% summarize(mean(SWR, na.rm = TRUE), min(SWR, na.rm = TRUE), max(SWR, na.rm = TRUE), sd(SWR, na.rm = TRUE))\n## # A tibble: 1 √ó 4\n##   `mean(SWR, na.rm = TRUE)` `min(SWR, na.rm = TRUE)` `max(SWR, na.rm = TRUE)`\n##                       &lt;dbl&gt;                    &lt;dbl&gt;                    &lt;dbl&gt;\n## 1                      4.42                    0.183                     12.5\n## # ‚Ñπ 1 more variable: `sd(SWR, na.rm = TRUE)` &lt;dbl&gt;\n\n\nWrite a good paragraph interpreting the plot and numerical summaries.\n\nStrength-to-weight (SWR) ratio ranges from 0.18 to 12.46, with a mean SWR of 4.4. SWR varies about 2.08 units above and below the mean. We observe that most SWRs appear to be centered between 4 and 7, with a slight right-skew to the data. The distribution of SWRs appears to be unimodal.",
    "crumbs": [
      "Simple Linear Regression - Intro+Formalization"
    ]
  },
  {
    "objectID": "activities/03_04-slr-intro-formalization.html#exercise-4-data-visualization---two-quantitative-variables-1",
    "href": "activities/03_04-slr-intro-formalization.html#exercise-4-data-visualization---two-quantitative-variables-1",
    "title": "Simple Linear Regression - Intro+Formalization",
    "section": "Exercise 4: Data visualization - two quantitative variables",
    "text": "Exercise 4: Data visualization - two quantitative variables\nWe‚Äôd like to visualize the relationship between body weight and the strength-to-weight ratio. A scatterplot (or informally, a ‚Äúpoint cloud‚Äù) allows us to do this! The code below creates a scatterplot of body weight vs.¬†SWR using ggplot().\n\n# scatterplot\n\n# The alpha = 0.5 in geom_point() adds transparency to the points\n# to make them easier to see. You can make this smaller for more transparency\nlifts %&gt;%\n  ggplot(aes(x = BodyweightKg, y = SWR)) +\n  geom_point(alpha = 0.5)\n\n\n\n\n\n\n\n\na & b. In our plot aesthetics, we now have two variables listed (an ‚Äúx‚Äù and a ‚Äúy‚Äù) as opposed to just a single variable. The ‚Äúgeom‚Äù for a scatterplot is geom_point. Otherwise, the code structure remains very similar!\n\nIn general, it seems as though higher body weights are associated with lower SWRs. Once body weight (in kg) is greater than 50, the relationship between body weight and SWR appears to be weakly negative, and roughly linear. The points are very dispersed, indicating that there is a good amount of variation in this relationship (hence the term ‚Äúweak‚Äù).",
    "crumbs": [
      "Simple Linear Regression - Intro+Formalization"
    ]
  },
  {
    "objectID": "activities/03_04-slr-intro-formalization.html#exercise-5-scatterplots---patterns-in-point-clouds-1",
    "href": "activities/03_04-slr-intro-formalization.html#exercise-5-scatterplots---patterns-in-point-clouds-1",
    "title": "Simple Linear Regression - Intro+Formalization",
    "section": "Exercise 5: Scatterplots - patterns in point clouds",
    "text": "Exercise 5: Scatterplots - patterns in point clouds\nSometimes, it can be easier to see a pattern in a point cloud by adding a smoothing line to our scatterplots. The code below adapts the code in Exercise 4 to do this:\n\n# scatterplot with smoothing line\nlifts %&gt;%\n  ggplot(aes(x = BodyweightKg, y = SWR)) +\n  geom_point(alpha = 0.5) +\n  geom_smooth()\n\n\n\n\n\n\n\n\n\nThis doesn‚Äôt change my answer much (but it may have changed yours, and that‚Äôs okay!). It does appear as though there is a weakly negative relationship between body weight and SWR, particularly once body weight is above a certain value.\nI would say that yes, a linear relationship here seems reasonable! Even though there is some curvature in the smoothed trend line early on, that is based on very few data points. Those data points with low body weights aren‚Äôt enough to convince me that the relationship couldn‚Äôt be roughly linear between body weight and SWR.",
    "crumbs": [
      "Simple Linear Regression - Intro+Formalization"
    ]
  },
  {
    "objectID": "activities/03_04-slr-intro-formalization.html#exercise-6-correlation-1",
    "href": "activities/03_04-slr-intro-formalization.html#exercise-6-correlation-1",
    "title": "Simple Linear Regression - Intro+Formalization",
    "section": "Exercise 6: Correlation",
    "text": "Exercise 6: Correlation\n\nI would describe the correlation between body weight and SWR as weak and negative.\nI‚Äôll guess -0.1, since the line is negative, and the points are very dispersed around the line!",
    "crumbs": [
      "Simple Linear Regression - Intro+Formalization"
    ]
  },
  {
    "objectID": "activities/03_04-slr-intro-formalization.html#exercise-7-computing-correlation-in-r-1",
    "href": "activities/03_04-slr-intro-formalization.html#exercise-7-computing-correlation-in-r-1",
    "title": "Simple Linear Regression - Intro+Formalization",
    "section": "Exercise 7: Computing correlation in R",
    "text": "Exercise 7: Computing correlation in R\n\n# correlation\n\n# Note: the order in which you put your two quantitative variables into the cor\n# function doesn't matter! Try switching them around to confirm this for yourself\n# Because of the missing data, we need to include the use = \"complete.obs\" - otherwise the correlation would be computed as NA\nlifts %&gt;%\n    summarize(cor(SWR, BodyweightKg, use = \"complete.obs\"))\n## # A tibble: 1 √ó 1\n##   `cor(SWR, BodyweightKg, use = \"complete.obs\")`\n##                                            &lt;dbl&gt;\n## 1                                        -0.0392\n\nSo close to our guess!",
    "crumbs": [
      "Simple Linear Regression - Intro+Formalization"
    ]
  },
  {
    "objectID": "activities/03_04-slr-intro-formalization.html#exercise-8-limitations-of-correlation-1",
    "href": "activities/03_04-slr-intro-formalization.html#exercise-8-limitations-of-correlation-1",
    "title": "Simple Linear Regression - Intro+Formalization",
    "section": "Exercise 8: Limitations of correlation",
    "text": "Exercise 8: Limitations of correlation\n\n# correlation between x1, y1\nanscombe %&gt;% summarize(cor(x1, y1))\n##   cor(x1, y1)\n## 1   0.8164205\n\n# correlation between x2, y2\nanscombe %&gt;% summarize(cor(x2, y2))\n##   cor(x2, y2)\n## 1   0.8162365\n\n# correlation between x3, y3\nanscombe %&gt;% summarize(cor(x3, y3))\n##   cor(x3, y3)\n## 1   0.8162867\n\n# correlation between x4, y4\nanscombe %&gt;% summarize(cor(x4, y4))\n##   cor(x4, y4)\n## 1   0.8165214\n\n\nEach of these correlations are nearly the same!\nEach of these correlations is relatively strong, and positive, since 0.8 is positive and closer to 1 than 0.\n\n\n\n# scatterplot: x1, y1\nanscombe %&gt;%\n  ggplot(aes(x = x1, y = y1)) +\n  geom_point()\n\n\n\n\n\n\n\n\n# scatterplot: x2, y2\nanscombe %&gt;%\n  ggplot(aes(x = x2, y = y2)) +\n  geom_point()\n\n\n\n\n\n\n\n\n# scatterplot: x3, y3\nanscombe %&gt;%\n  ggplot(aes(x = x3, y = y3)) +\n  geom_point()\n\n\n\n\n\n\n\n\n# scatterplot: x4, y4\nanscombe %&gt;%\n  ggplot(aes(x = x4, y = y4)) +\n  geom_point()\n\n\n\n\n\n\n\n\n\nThe message of this exercise is that data visualization is important in addition to numerical summaries! Many different sets of points can have nearly the same correlation, but display very different patterns in point clouds upon closer inspection. Reporting correlation alone is not enough to summarize the relationship between two quantitative variables, and should be accompanied by a scatter plot!",
    "crumbs": [
      "Simple Linear Regression - Intro+Formalization"
    ]
  },
  {
    "objectID": "activities/03_04-slr-intro-formalization.html#exercise-10-correlation-and-extreme-values-1",
    "href": "activities/03_04-slr-intro-formalization.html#exercise-10-correlation-and-extreme-values-1",
    "title": "Simple Linear Regression - Intro+Formalization",
    "section": "Exercise 10: Correlation and extreme values",
    "text": "Exercise 10: Correlation and extreme values\n\n# create a toy dataset\nset.seed(1234)\nx &lt;- rnorm(100, mean = 5, sd = 2)\ny &lt;- -3 * x + rnorm(100, sd = 4)\ndat &lt;- data.frame(x = x, y = y)\n\n\n\n\n\n# scatterplot\ndat %&gt;% \n  ggplot(aes(x = x, y = y)) +\n  geom_point()\n\n\n\n\n\n\n\n\n\nThe correlation between x and y is moderately strong and negative.\nI‚Äôll guess -0.6, since the relationship is negative and is sort of in-between weak and strong.\n\n\n\n# correlation\ndat %&gt;% summarize(cor(x, y))\n##    cor(x, y)\n## 1 -0.8295483\n\n\n\n\n\n# creating dat_new1\nx1 &lt;- c(x, 15)\ny1 &lt;- c(y, -45)\ndat_new1 &lt;- data.frame(x = x1, y = y1)\n\n\n\n\n\n# scatterplot\ndat_new1 %&gt;%\n  ggplot(aes(x1, y1)) +\n  geom_point()\n\n\n\n\n\n\n\n\n# correlation\ndat %&gt;% summarize(cor(x1, y1))\n##   cor(x1, y1)\n## 1  -0.8573567\n\nOur correlation stayed roughly the same with the addition of this new point!\n\n\n\n\n# creating dat_new1\nx2 &lt;- c(x, 15)\ny2 &lt;- c(y, 45)\ndat_new2 &lt;- data.frame(x = x2, y = y2)\n\n\n\n\n\n# scatterplot\ndat_new2 %&gt;%\n  ggplot(aes(x2, y2)) +\n  geom_point()\n\n\n\n\n\n\n\n\n# correlation\ndat_new2 %&gt;% summarize(cor(x2, y2))\n##   cor(x2, y2)\n## 1  -0.2924792\n\nThe correlation changes quite a bit with the addition of this new point! Something to note is that this new point does not follow the rough linear trend that the original points had, that the first point we considered adding also had. This line seems way off base, comparatively!\n\nThe takeaway message here is that even though both of these additional points might be considered ‚Äúoutliers‚Äù because they have extreme x values, one changes the relationship between x and y much more than the other. In this case, the second point we considered would be influential because it changes the observed relationship between all x‚Äôs and y‚Äôs much more than the first point we considered. Not all ‚Äúoutliers‚Äù are considered equal!\n\n\n\n\n\ndat_new1 %&gt;%\n  ggplot(aes(x1, y1)) +\n  geom_point() +\n  geom_smooth(method = \"lm\", se = FALSE)\n\n\n\n\n\n\n\n\ndat_new2 %&gt;%\n  ggplot(aes(x2, y2)) +\n  geom_point() +\n  geom_smooth(method = \"lm\", se = FALSE)",
    "crumbs": [
      "Simple Linear Regression - Intro+Formalization"
    ]
  },
  {
    "objectID": "activities/01_foundations_welcome.html",
    "href": "activities/01_foundations_welcome.html",
    "title": "Collecting and Summarizing Data",
    "section": "",
    "text": "You can download the .qmd file for this activity here and open in R-studio. The rendered version is posted in the course website (Activities tab). I often experiment with the class activities (and see it in live!) and make updates, but I always post the final version before class starts. To be sure you have the most up-to-date copy, please download it once you‚Äôve settled in before class begins.",
    "crumbs": [
      "Collecting and Summarizing Data"
    ]
  },
  {
    "objectID": "activities/01_foundations_welcome.html#see-class-notes",
    "href": "activities/01_foundations_welcome.html#see-class-notes",
    "title": "Collecting and Summarizing Data",
    "section": "See class notes",
    "text": "See class notes\nGo to your class-notes to see what we covered in live-note taking!\n\n\n\nEXAMPLE 1: Tidy data \nRefering to the questions from the Fun Survey!\n\nHow many hours of sleep did you get last night?\nHow many cups of coffee did you drink this morning?\nWhat is your declared or potential major? (If you are a double major, just pick whichever one you think of first.)\nWhat is your anticipated graduation year?\nHow many stats/data science courses have you taken in the past?\nOn a scale of 1 (get me out of here) to 10 (yay!), how excited are you about this course?\nIs it your birthday this semester? (yes/no)\nHow many unread emails do you have in your inbox right now?",
    "crumbs": [
      "Collecting and Summarizing Data"
    ]
  },
  {
    "objectID": "activities/01_foundations_welcome.html#example-1-use-r-as-a-calculator",
    "href": "activities/01_foundations_welcome.html#example-1-use-r-as-a-calculator",
    "title": "Collecting and Summarizing Data",
    "section": "Example 1: Use R as a calculator",
    "text": "Example 1: Use R as a calculator\nType the following lines in the console (bottom left), one by one, hitting Return/Enter after each line. In some cases you might even get an error! This error is important to learning how R code does and doesn‚Äôt work.\n\n4 + 2\n\n\n4^2\n\n\n4*2\n\n\n4(2)",
    "crumbs": [
      "Collecting and Summarizing Data"
    ]
  },
  {
    "objectID": "activities/01_foundations_welcome.html#example-2-functions-and-arguments",
    "href": "activities/01_foundations_welcome.html#example-2-functions-and-arguments",
    "title": "Collecting and Summarizing Data",
    "section": "Example 2: Functions and arguments",
    "text": "Example 2: Functions and arguments\nWe can also use built-in functions to perform common tasks. These functions have names and require information about arguments in order to run:\nfunction(argument) Cheatcode: RiceCooker(Rice)\nTry out the following functions one by one in the RStudio console. For each function, note its‚Ä¶\n\nname\nthe argument or information it needs to run\nwhat output it produces (what the function does)\nhow the name connects to what the function does\n\n\nsqrt(9)\n\n\nnchar(\"macalester\")\n\n\nsqrt(nchar(\"snow\"))\n\nSome functions have more than 1 argument, separated by commas:\nfunction(argument1 = ___, argument2 = ___) Cheatcode: RiceCooker(Rice,Chicken)\nTry out the following, one by one.\n\nrep(x = 2, times = 5)\n\n\nrep(times = 5, x = 2)\n\n\nrep(2, 5)\n\n\nrep(5, 2)\n\nFinally, R is case sensitive. Try using Rep() instead of rep(). Take time to read the error message!\n\nRep(5, 2)",
    "crumbs": [
      "Collecting and Summarizing Data"
    ]
  },
  {
    "objectID": "activities/01_foundations_welcome.html#example-3-save-it-for-later",
    "href": "activities/01_foundations_welcome.html#example-3-save-it-for-later",
    "title": "Collecting and Summarizing Data",
    "section": "Example 3: Save it for later",
    "text": "Example 3: Save it for later\nWe‚Äôll often want to store some R output for later use. In R:\nname &lt;- output\nwhere name is the name under which to store a result, output is the result we wish to store, and &lt;- is the assignment operator (I think of this as an arrow pointing the output into the name).\nIMPORTANT: Try out each line one at a time. Why doesn‚Äôt the first line produce any output?\n\ndegrees_c &lt;- -13\n\n\ndegrees_c\n\n\ndegrees_c * (9/5) + 32",
    "crumbs": [
      "Collecting and Summarizing Data"
    ]
  },
  {
    "objectID": "activities/01_foundations_welcome.html#example-4-import-data",
    "href": "activities/01_foundations_welcome.html#example-4-import-data",
    "title": "Collecting and Summarizing Data",
    "section": "Example 4: Import data",
    "text": "Example 4: Import data\nNext, let‚Äôs work with some data!! The first step is importing our data into RStudio. How we do this depends on:\n\nfile format (eg: .xls Excel spreadsheet, .csv, .txt)\nfile location (eg: online, on your desktop, built into RStudio itself).\n\nThe data from the similar survey collected in fall 2024 stored as a .csv file online. Import this data using the read_csv() function, and store it as survey using the code below:\nFirst, in the Console pane of RStudio, run the following command to install some necessary packages (you will need to do this any time you are installing a new package):\ninstall.packages(\"tidyverse\")\n\n# Load the \"tidyverse\" package which contains the read_csv() function\nlibrary(tidyverse)\n\n# Import the data\nsurvey &lt;- read_csv(\"https://mac-stat.github.io/data/112_fall_2024_survey.csv\")\n\n\n\nCheck out the data\nIn the Environment tab in the upper right pane of RStudio, click on survey. What happens?!\n\n\nIn the modern era, datasets often contain hundreds of variables and millions of observations. We need more effective ways to explore such data.",
    "crumbs": [
      "Collecting and Summarizing Data"
    ]
  },
  {
    "objectID": "activities/01_foundations_welcome.html#example-5-get-to-know-the-data",
    "href": "activities/01_foundations_welcome.html#example-5-get-to-know-the-data",
    "title": "Collecting and Summarizing Data",
    "section": "Example 5: Get to know the data",
    "text": "Example 5: Get to know the data\nPAUSE: Make sure you‚Äôre sync with your group.\nBefore we can learn anything from our data, we must understand its structure. For each function below:\n\ntry it out\ndiscuss with your group what the function does\ndiscuss with your group how the function‚Äôs name connects to what it does\n\n\ndim(survey) # (Number of row (case/obs.), Number of column (variable))\n\n\nnrow(survey) # Number of case/obs.\n\n\nncol(survey) # Number of variables\n\n\nhead(survey) # View first few rows of the dataset (6 rows, by default)\n\n\nhead(survey, 3) # Controlling the view of first few rows of the dataset\n\n\ntail(survey) # View first few rows of the dataset (6 rows, by default)\n\n\nnames(survey) # Get all column (variable) names\n\n\nstr(survey) # Overall info about data",
    "crumbs": [
      "Collecting and Summarizing Data"
    ]
  },
  {
    "objectID": "activities/01_foundations_welcome.html#example-6-code-communication",
    "href": "activities/01_foundations_welcome.html#example-6-code-communication",
    "title": "Collecting and Summarizing Data",
    "section": "Example 6 : Code = communication",
    "text": "Example 6 : Code = communication\nIt‚Äôs important to recognize from day 1 that code is a form of communication, both to yourself and others!!!!! Code structure and details are important to readability and clarity, just as grammar, punctuation, spelling, paragraphs, and line spacing are important in written essays. All of the code below works, but has bad structure. With your group, discuss what is unfortunate about each line, then make it better.\n\nseq(from=1, to=9, by=2)\nseq(from = 1, to=9, by=2)\ntemp_cel &lt;- -13\nthisisthetemperaturetodayincelsius &lt;- -13\nthis_is_the_temperature_today_in_celsius &lt;- -13",
    "crumbs": [
      "Collecting and Summarizing Data"
    ]
  },
  {
    "objectID": "activities/01_foundations_welcome.html#example-7-you-will-make-so-many-mistakes",
    "href": "activities/01_foundations_welcome.html#example-7-you-will-make-so-many-mistakes",
    "title": "Collecting and Summarizing Data",
    "section": "Example 7: You will make so many mistakes!",
    "text": "Example 7: You will make so many mistakes!\nMistakes are common when, and even important to, learning any new language. You‚Äôll get better and better at interpreting error messages, finding help, and fixing errors. In addition to finding help online, R has built-in help files. For example:\n\nIn the console, type ?rep and press Return/Enter.\nCheck out the documentation file that pops up in the Help tab (lower right).\nQuickly scroll through, noting the type of information provided.\nPause at the ‚ÄúExamples‚Äù section at the bottom ‚Äì perhaps the most useful section! Try out a couple of the provided examples in your console.",
    "crumbs": [
      "Collecting and Summarizing Data"
    ]
  },
  {
    "objectID": "activities/01_foundations_welcome.html#example-8-make-a-cheat-sheet",
    "href": "activities/01_foundations_welcome.html#example-8-make-a-cheat-sheet",
    "title": "Collecting and Summarizing Data",
    "section": "Example 8: Make a ‚Äúcheat sheet‚Äù",
    "text": "Example 8: Make a ‚Äúcheat sheet‚Äù\nYou will continue to pick up new R code and ideas. You‚Äôre highly encouraged to start tracking this in a cheat sheet (eg: in a Google doc). The cheat sheet will be a handy reference for you, and the act of making it will help deepen your understanding and retention.",
    "crumbs": [
      "Collecting and Summarizing Data"
    ]
  },
  {
    "objectID": "activities/01_foundations_welcome.html#exercise-complete-this-after-the-class",
    "href": "activities/01_foundations_welcome.html#exercise-complete-this-after-the-class",
    "title": "Collecting and Summarizing Data",
    "section": "Exercise: Complete this after the class",
    "text": "Exercise: Complete this after the class\nComplete this exercise after class. First, try it on your own (or with your group), and then check your work against the solution provided at the end of this .qmd file.\nUse R code to do the following:\n\nImport & name data on different Himalayan peaks from the url below:\nhttps://raw.githubusercontent.com/rfordatascience/tidytuesday/master/data/2020/2020-09-22/peaks.csv NOTE: A codebook, i.e.¬†a description of the data, is here.\nUse a function to show which variables are recorded on each peak.\nHow many peaks are included in the dataset? Answer this using a function, not by counting up the rows yourself.\nShow the first 6 rows of the dataset. NOTE: This gives us a quick glimpse without having to print out the entire dataset!",
    "crumbs": [
      "Collecting and Summarizing Data"
    ]
  },
  {
    "objectID": "activities/01_foundations_welcome.html#exercise-your-turn",
    "href": "activities/01_foundations_welcome.html#exercise-your-turn",
    "title": "Collecting and Summarizing Data",
    "section": "Exercise: Your turn",
    "text": "Exercise: Your turn\n\n# a\npeaks &lt;- read_csv(\"https://raw.githubusercontent.com/rfordatascience/tidytuesday/master/data/2020/2020-09-22/peaks.csv\")\n## Error in read_csv(\"https://raw.githubusercontent.com/rfordatascience/tidytuesday/master/data/2020/2020-09-22/peaks.csv\"): could not find function \"read_csv\"\n\n# b\nnames(peaks)\n## Error: object 'peaks' not found\n\n# c\ndim(peaks)\n## Error: object 'peaks' not found\nnrow(peaks)\n## Error: object 'peaks' not found\n\n# d\nhead(peaks)\n## Error: object 'peaks' not found",
    "crumbs": [
      "Collecting and Summarizing Data"
    ]
  },
  {
    "objectID": "R_Resources.html",
    "href": "R_Resources.html",
    "title": "R and RStudio Resources",
    "section": "",
    "text": "Crowd-sourced R functions\nInteractive RStudio tutorials (R basics, data viz, and more)\nMarkdown (Quarto) basics, and Quarto/RStudio tutorial\nRStudio cheatsheets\nGoogle it! If you have a question about R, someone else probably asked it. Add ‚ÄúR tidyverse‚Äù to your search term.\nExample: scatterplot R tidyverse\nNote about ChatGPT for code: Please avoid using ChatGPT as a first resort for R code. It often suggests overly wordy or unfamiliar approaches. Our goal is for you to understand the code you write; sticking to tools and syntax we use in class will support your learning best."
  },
  {
    "objectID": "R_Resources.html#getting-help",
    "href": "R_Resources.html#getting-help",
    "title": "R and RStudio Resources",
    "section": "",
    "text": "Crowd-sourced R functions\nInteractive RStudio tutorials (R basics, data viz, and more)\nMarkdown (Quarto) basics, and Quarto/RStudio tutorial\nRStudio cheatsheets\nGoogle it! If you have a question about R, someone else probably asked it. Add ‚ÄúR tidyverse‚Äù to your search term.\nExample: scatterplot R tidyverse\nNote about ChatGPT for code: Please avoid using ChatGPT as a first resort for R code. It often suggests overly wordy or unfamiliar approaches. Our goal is for you to understand the code you write; sticking to tools and syntax we use in class will support your learning best."
  },
  {
    "objectID": "R_Resources.html#getting-started",
    "href": "R_Resources.html#getting-started",
    "title": "R and RStudio Resources",
    "section": "Getting Started",
    "text": "Getting Started\nThe videos below introduce R and RStudio and show how to install them on your computer. Start with the overview, then watch the step-by-step video for your operating system.\nDownload links (mentioned in the videos): - R: http://cran.r-project.org/ - RStudio: https://www.rstudio.com/products/rstudio/download/#download\n\n\n\n\n\n\nImportant\n\n\n\nVersion numbers in the videos are out-of-date. Please download the latest versions of both R and RStudio. As of January 19, 2026, the latest versions are: R 4.5.2 and RStudio RStudio-2026.01.0-392.\n\n\n\nInstallation / Overview\n\nInstalling R and RStudio (Overview) (Length: 4:30)\n\nPick one of the following: - Mac ‚Äî Installing R and RStudio Step-by-Step (Length: 3:24) - Windows ‚Äî Installing R and RStudio Step-by-Step (Length: 5:43)\nIf you run into installation issues, contact me right away. If we can‚Äôt get RStudio working locally, you can use Macalester‚Äôs online RStudio server:\nQuick intro video"
  },
  {
    "objectID": "R_Resources.html#intro-videos-to-r-watch-in-order",
    "href": "R_Resources.html#intro-videos-to-r-watch-in-order",
    "title": "R and RStudio Resources",
    "section": "Intro Videos to R (watch in order)",
    "text": "Intro Videos to R (watch in order)\n\nIntro to R and RStudio (11:31)\nR Data Types (8:05)\nR Error Messages and Troubleshooting (Optional) (7:52)\nIntro to RMarkdown / Quarto (9:33)\nFile Structure & Organization (Optional)\n\nMac (5:13)\nWindows (7:44)\n\nTips on File Naming & Organization (Optional)\nR Packages (7:30)\n\n\nYou can adjust video speed and pause as needed. If anything doesn‚Äôt work, please let me know so we can fix it quickly."
  },
  {
    "objectID": "activities/02-foundations-univariate.html",
    "href": "activities/02-foundations-univariate.html",
    "title": "Univariate Visualization & Summaries",
    "section": "",
    "text": "You can download the .qmd file for this activity here and open in R-studio. The rendered version is posted in the course website (Activities tab). I often experiment with the class activities (and see it in live!) and make updates, but I always post the final version before class starts. To be sure you have the most up-to-date copy, please download it once you‚Äôve settled in before class begins.",
    "crumbs": [
      "Univariate Visualization & Summaries"
    ]
  },
  {
    "objectID": "activities/02-foundations-univariate.html#learning-goals",
    "href": "activities/02-foundations-univariate.html#learning-goals",
    "title": "Univariate Visualization & Summaries",
    "section": "Learning goals",
    "text": "Learning goals\nBy the end of this activity, you should be able to:\n\nWhat summarizations and visualizations are appropriate for categorical or quantitative variables\nWrite R code to read in data and to summarize and visualize a single variable at a time.\nInterpret key features of barplots, boxplots, histograms, and density plots\nDescribe information about the distribution of a quantitative variable using the concepts of shape, center, spread, and outliers\nRelate summary statistics of data to the concepts of shape, center, spread, and outliers",
    "crumbs": [
      "Univariate Visualization & Summaries"
    ]
  },
  {
    "objectID": "activities/02-foundations-univariate.html#readings-and-videos",
    "href": "activities/02-foundations-univariate.html#readings-and-videos",
    "title": "Univariate Visualization & Summaries",
    "section": "Readings and videos",
    "text": "Readings and videos\nYou should have gone through the followings before the class and finished checkpoint quizzes!\n\nReading: Sections 2.1-2.4, 2.6 in the STAT 155 Notes\nVideos:\n\nUnivariate summaries (slides)\n\nPart 1\nPart 2\n\nR Code for Categorical Visualization and Summarization\nR Code for Quantitative Visualization and Summarization\nQuarto docs\n\n\n\nFile organization: Save this file in the ‚ÄúActivities‚Äù subfolder of your ‚ÄúSTAT155‚Äù folder.",
    "crumbs": [
      "Univariate Visualization & Summaries"
    ]
  },
  {
    "objectID": "activities/02-foundations-univariate.html#exercise-1-importing-and-getting-to-know-the-data",
    "href": "activities/02-foundations-univariate.html#exercise-1-importing-and-getting-to-know-the-data",
    "title": "Univariate Visualization & Summaries",
    "section": "Exercise 1: Importing and getting to know the data",
    "text": "Exercise 1: Importing and getting to know the data\nFirst, in the Console pane of RStudio, run the following command to install some necessary packages (you will need to do this any time you are installing a new package):\ninstall.packages(\"tidyverse\") # Have you installed it?\nNow, in the Quarto pane, run the following code chunk to load the package and load a dataset (you can either click the green arrow in the top right of the code chunk, put your cursor in the code chunk and hit Ctrl+Alt+C [on Windows/Linux] or Command+Option+C [on Mac]).\n\n# Load package\nlibrary(tidyverse)\n\n# Read in the Dear Abby data\nabby &lt;- read_csv(\"https://mac-stat.github.io/data/dear_abby.csv\")\n\nThroughout this activity, we‚Äôll work only with the most recent year of data, from 2017. Run the following chunk:\n\n# Wrangle the Dear Abby data\n# Ignore this code for now!\nabby &lt;- abby %&gt;% \n  filter(year == 2017) %&gt;% \n  mutate(month = month(month, label = TRUE)) %&gt;%\n  mutate(\n    parents = str_detect(question_only, \"mother|mama|mom|father|papa|dad\"),\n    marriage = str_detect(question_only, \"marriage|marry|married\"),\n    money = str_detect(question_only, \"money|finance\")\n  ) %&gt;%\n  rowwise() %&gt;%\n  mutate(\n    themes = c(\n      if (parents) \"parents\",\n      if (marriage) \"marriage\",\n      if (money) \"money\"\n    ) %&gt;% paste(collapse = \", \"),\n    themes = ifelse(themes == \"\", \"other\", themes)\n  ) %&gt;%\n  ungroup() %&gt;% \n  select(year, month, day, question_only, bing_pos, afinn_overall, afinn_pos, afinn_neg, themes)\n\n\nClick on the Environment tab (generally in the upper right hand pane in RStudio). Then click the abby line. The abby data will pop up as a separate pane (like viewing a spreadsheet) ‚Äì check it out.\nIn this tidy dataset, what is the unit of observation? That is, what is represented in each row of the dataset?\nWhat term do we use for the columns of the dataset?\nTry out each function below. Identify what each function tells you about the abby data and note this in the ???:\n\n\n# ??? [what do both numbers mean?]\ndim(abby)\n## [1] 514   9\n\n\n# ???\nnrow(abby)\n## [1] 514\n\n\n# ???\nncol(abby)\n## [1] 9\n\n\n# ???\nhead(abby)\n## # A tibble: 6 √ó 9\n##    year month day   question_only     bing_pos afinn_overall afinn_pos afinn_neg\n##   &lt;dbl&gt; &lt;ord&gt; &lt;chr&gt; &lt;chr&gt;                &lt;dbl&gt;         &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;\n## 1  2017 Aug   30    \"i moved to the ‚Ä¶    0.75             14        16         2\n## 2  2017 Aug   30    \"under what circ‚Ä¶   NA                NA        NA        NA\n## 3  2017 Aug   28    \"i'm not a dog p‚Ä¶    0.333             5         5         0\n## 4  2017 Aug   28    \"my 62-year-old ‚Ä¶    0.143           -11         8        19\n## 5  2017 Aug   27    \"i have a friend‚Ä¶    0.222             0         7         7\n## 6  2017 Aug   27    \"i have been sel‚Ä¶    0.333            -5         2         7\n## # ‚Ñπ 1 more variable: themes &lt;chr&gt;\n\n\n# ???\nnames(abby)\n## [1] \"year\"          \"month\"         \"day\"           \"question_only\"\n## [5] \"bing_pos\"      \"afinn_overall\" \"afinn_pos\"     \"afinn_neg\"    \n## [9] \"themes\"\n\n\n[OPTIONAL] If you‚Äôre not sure how exactly to use a function, you can pull up a built-in help page with information about the arguments a function takes (i.e., what goes inside the parentheses), and the output it produces. To do this, click inside the Console pane, and enter ?function_name. For example, to pull up a help page for the dim() function, we can type ?dim and hit Enter. Try pulling up the help page for the read_csv() function we used to load the dataset.",
    "crumbs": [
      "Univariate Visualization & Summaries"
    ]
  },
  {
    "objectID": "activities/02-foundations-univariate.html#exercise-2-preparing-to-summarize-and-visualize-the-data",
    "href": "activities/02-foundations-univariate.html#exercise-2-preparing-to-summarize-and-visualize-the-data",
    "title": "Univariate Visualization & Summaries",
    "section": "Exercise 2: Preparing to summarize and visualize the data",
    "text": "Exercise 2: Preparing to summarize and visualize the data\nIn the next exercises, we will be exploring themes in the Dear Abby questions and the overall ‚Äúmood‚Äù or sentiment of the questions. Before continuing, read the codebook for this dataset for some context about sentiment analysis, which gives us a measure of the mood/sentiment of a text.\n\nWhat sentiment variables do we have in the dataset? Are they quantitative or categorical?\nCheck out the theme variable. Is this quantitative or categorical?\nWhat visualizations are appropriate for looking at the distribution of a single quantitative variable? What about a single categorical variable?",
    "crumbs": [
      "Univariate Visualization & Summaries"
    ]
  },
  {
    "objectID": "activities/02-foundations-univariate.html#exercise-3-exploring-themes-in-the-letters",
    "href": "activities/02-foundations-univariate.html#exercise-3-exploring-themes-in-the-letters",
    "title": "Univariate Visualization & Summaries",
    "section": "Exercise 3: Exploring themes in the letters",
    "text": "Exercise 3: Exploring themes in the letters\n\nThe code below makes a barplot of the themes variable using the ggplot2 visualization package. Before making the plot, make note of what you expect the plot might look like. (This might be hard‚Äìjust do your best!) Then compare to what you observe when you run the code chunk to make the plot. (Clearly defining your expectations first is good scientific practice to avoid confirmation bias.)\n\n\n# Load package\n# install.packages(\"ggplot2\") # Run in R Console first time to install (copy without '#')\nlibrary(ggplot2)\n\n# barplot\nggplot(abby, aes(x = themes)) +\n    geom_bar() +\n    theme(axis.text.x = element_text(angle = 90, vjust = 0.5, hjust=1))\n\n\n\n\n\n\n\n\n\nWe can follow up on the barplot with a simple numerical summary. Whereas the ggplot2 package is great for visualizations, dplyr is great for numerical summaries. The code below constructs a table of the number of questions with each theme. Make sure that these numerical summaries match up with what you saw in the barplot.\n\n\n# install.packages(\"dplyr\")\n# Construct a table of counts\nabby %&gt;% \n    count(themes)\n## # A tibble: 8 √ó 2\n##   themes                       n\n##   &lt;chr&gt;                    &lt;int&gt;\n## 1 marriage                    75\n## 2 marriage, money              5\n## 3 money                       21\n## 4 other                      234\n## 5 parents                    127\n## 6 parents, marriage           33\n## 7 parents, marriage, money     4\n## 8 parents, money              15\n\n\nBefore proceeding, let‚Äôs break down the plotting code above. Run each chunk to see how the two lines of code above build up the plot in ‚Äúlayers‚Äù.\n\n\n# sets up the \"canvas\" of the plot with axis labels\nggplot(abby, aes(x = themes)) \n\n\n\n\n\n\n\n\n\n# Adds the bars- Any Problem you notice?\nggplot(abby, aes(x = themes)) +\n    geom_bar() \n\n\n\n\n\n\n\n\n\n# Rotates the x axis labels\nggplot(abby, aes(x = themes)) +\n    geom_bar() +\n    theme(axis.text.x = element_text(angle = 90, vjust = 0.5, hjust=1)) \n\n\n\n\n\n\n\n\n\n# Changes the visual theme of the plot with a white background and removes gridlines\nggplot(abby, aes(x = themes)) +\n    geom_bar() +\n    theme_classic() +\n    theme(axis.text.x = element_text(angle = 90, vjust = 0.5, hjust=1))",
    "crumbs": [
      "Univariate Visualization & Summaries"
    ]
  },
  {
    "objectID": "activities/02-foundations-univariate.html#live-note-taking",
    "href": "activities/02-foundations-univariate.html#live-note-taking",
    "title": "Univariate Visualization & Summaries",
    "section": "Live Note-Taking!",
    "text": "Live Note-Taking!",
    "crumbs": [
      "Univariate Visualization & Summaries"
    ]
  },
  {
    "objectID": "activities/02-foundations-univariate.html#exercise-4-exploring-sentiment",
    "href": "activities/02-foundations-univariate.html#exercise-4-exploring-sentiment",
    "title": "Univariate Visualization & Summaries",
    "section": "Exercise 4: Exploring sentiment",
    "text": "Exercise 4: Exploring sentiment\nWe‚Äôll look at the distribution of the bing_pos sentiment variable and associated summary statistics.\n\n\n\n\nThe code below creates a boxplot of this variable. In the comment, make note of how this code is similar to the code for the barplot above.\n\n\n# ???\nggplot(abby, aes(x = bing_pos)) +\n    geom_boxplot()\n\n\n\n\n\n\n\n\n\nChallenge: Using the code for the barplot and boxplot as a guide, try to make a histogram and a density plot of the overall average ratings.\n\nWhat information is given by the tallest bar of the histogram?\nHow would you describe the shape of the distribution?\n\n\n\n# Histogram\nggplot(abby, aes(x = bing_pos)) +\n    geom_histogram()\n\n\n\n\n\n\n\n\n# Density plot\nggplot(abby, aes(x = bing_pos)) +\n    geom_density()\n\n\n\n\n\n\n\n\n\nWe can compute summary statistics (numerical summaries) for a quantitative variable using the summary() function or with the summarize() function from the dplyr package. (1st Qu. and 3rd Qu. stand for first and third quartile.) After inspecting these summaries, look back to your boxplot, histogram, and density plot. Which plots show which summaries most clearly?\n\n\n# Summary statistics\n# Using summary() - convenient for computing many summaries in one command\n# Does not show the standard deviation\nabby %&gt;% \n    select(bing_pos) %&gt;% \n    summary()\n##     bing_pos     \n##  Min.   :0.0000  \n##  1st Qu.:0.1667  \n##  Median :0.3333  \n##  Mean   :0.3650  \n##  3rd Qu.:0.5000  \n##  Max.   :1.0000  \n##  NA's   :19\n\n# Using summarize() from dplyr\n# Note that we use %&gt;% to pipe the data into the summarize() function\n# We need to use na.rm = TRUE because there are missing values (NAs)\nabby %&gt;% \n    summarize(mean(bing_pos, na.rm = TRUE), median(bing_pos, na.rm = TRUE), sd(bing_pos, na.rm = TRUE))\n## # A tibble: 1 √ó 3\n##   `mean(bing_pos, na.rm = TRUE)` median(bing_pos, na.rm‚Ä¶¬π sd(bing_pos, na.rm =‚Ä¶¬≤\n##                            &lt;dbl&gt;                    &lt;dbl&gt;                  &lt;dbl&gt;\n## 1                          0.365                    0.333                  0.279\n## # ‚Ñπ abbreviated names: ¬π‚Äã`median(bing_pos, na.rm = TRUE)`,\n## #   ¬≤‚Äã`sd(bing_pos, na.rm = TRUE)`\n\n\nWrite a good paragraph describing the information in the histogram (or density plot) by discussing shape, center, spread, and outliers. Incorporate the numerical summaries from part c.",
    "crumbs": [
      "Univariate Visualization & Summaries"
    ]
  },
  {
    "objectID": "activities/02-foundations-univariate.html#pause-math-box",
    "href": "activities/02-foundations-univariate.html#pause-math-box",
    "title": "Univariate Visualization & Summaries",
    "section": "Pause: Math box",
    "text": "Pause: Math box\nBelow is an example of a ‚Äúmath box‚Äù which summarizes the formulas for some of the numerical summaries above. You are not required to memorize, nor will you be assessed on, any formulas presented in this or any future math box. They serve 3 purposes:\n\nTo emphasize that there‚Äôs ‚Äúmath‚Äù / a formal structure behind what we‚Äôre doing.\nTo provide students that plan to continue studying Statistics a glimpse into the formal statistical theory they‚Äôll explore in later courses.\nTo make happy the students that are simply interested in math!\n\n\n\n\n\n\n\n\n\n\nMATH BOX: Univariate numerical summaries\n\n\n\nLet \\((y_1, y_2, ..., y_n)\\) be a sample of \\(n\\) data points.\nmean: \\[\\overline{y} = \\frac{y_1 + y_2 + \\cdots + y_n}{n} = \\frac{\\sum_{i=1}^n y_i}{n}\\]\nvariance: \\[\\text{var}(y) = \\frac{(y_1 - \\overline{y})^2 + (y_2 - \\overline{y})^2 + \\cdots + (y_n - \\overline{y})^2}{n - 1} = \\frac{\\sum_{i=1}^n (y_i - \\overline{y})^2}{n - 1}\\]\nstandard deviation: \\[\\text{sd}(y) = \\sqrt{\\text{var}(y)}\\]",
    "crumbs": [
      "Univariate Visualization & Summaries"
    ]
  },
  {
    "objectID": "activities/02-foundations-univariate.html#exercise-5-box-plots-vs.-histograms-vs.-density-plots",
    "href": "activities/02-foundations-univariate.html#exercise-5-box-plots-vs.-histograms-vs.-density-plots",
    "title": "Univariate Visualization & Summaries",
    "section": "Exercise 5: Box plots vs.¬†histograms vs.¬†density plots",
    "text": "Exercise 5: Box plots vs.¬†histograms vs.¬†density plots\nWe took 3 different approaches to plotting the quantitative average course variable above. They all have pros and cons.\n\nWhat is one pro about the boxplot in comparison to the histogram and density plot?\nWhat is one con about the boxplot in comparison to the histogram and density plots?\nIn this example, which plot do you prefer and why?",
    "crumbs": [
      "Univariate Visualization & Summaries"
    ]
  },
  {
    "objectID": "activities/02-foundations-univariate.html#exercise-6-returning-to-our-context-looking-ahead",
    "href": "activities/02-foundations-univariate.html#exercise-6-returning-to-our-context-looking-ahead",
    "title": "Univariate Visualization & Summaries",
    "section": "Exercise 6: Returning to our context, looking ahead",
    "text": "Exercise 6: Returning to our context, looking ahead\nIn this activity, we explored data on Dear Abby question, with a focus on exploring a single variable at a time.\n\nIn big picture terms, what have we learned about Dear Abby questions?\nWhat further curiosities do you have about the data?",
    "crumbs": [
      "Univariate Visualization & Summaries"
    ]
  },
  {
    "objectID": "activities/02-foundations-univariate.html#exercise-7-different-ways-to-think-about-data-visualization",
    "href": "activities/02-foundations-univariate.html#exercise-7-different-ways-to-think-about-data-visualization",
    "title": "Univariate Visualization & Summaries",
    "section": "Exercise 7: Different ways to think about data visualization",
    "text": "Exercise 7: Different ways to think about data visualization\nIn working with and visualizing data, it‚Äôs important to keep in mind what a data point represents. It can reflect the experience of a real person. It might reflect the sentiment in a piece of art. It might reflect history. We‚Äôve taken one very narrow and technical approach to data visualization. Check out the following examples, and write some notes about anything you find interesting.\n\nDear Data\nW.E.B. DuBois\nDecolonizing Data Viz",
    "crumbs": [
      "Univariate Visualization & Summaries"
    ]
  },
  {
    "objectID": "activities/02-foundations-univariate.html#exercise-8-rendering-your-work",
    "href": "activities/02-foundations-univariate.html#exercise-8-rendering-your-work",
    "title": "Univariate Visualization & Summaries",
    "section": "Exercise 8: Rendering your work",
    "text": "Exercise 8: Rendering your work\nSave this file, and then click the ‚ÄúRender‚Äù button in the menu bar for this pane (blue arrow pointing right). This will create an HTML file containing all of the directions, code, and responses from this activity. A preview of the HTML will appear in the browser.\n\nScroll through and inspect the document to see how your work was translated into this HTML format. Neat!\nClose the browser tab.\nGo to the ‚ÄúBackground Jobs‚Äù pane in RStudio and click the Stop button to end the rendering process.\nNavigate to your ‚ÄúActivities‚Äù subfolder within your ‚ÄúSTAT155‚Äù folder and locate the HTML file. You can open it again in your browser to double check.",
    "crumbs": [
      "Univariate Visualization & Summaries"
    ]
  },
  {
    "objectID": "activities/02-foundations-univariate.html#reflection",
    "href": "activities/02-foundations-univariate.html#reflection",
    "title": "Univariate Visualization & Summaries",
    "section": "Reflection",
    "text": "Reflection\nGo to the top of this file and review the learning objectives for this lesson. Which objectives do you have a good handle on, are at least familiar with, or are struggling with? What feels challenging right now? What are some wins from the day?\n\nResponse: Put your response here.",
    "crumbs": [
      "Univariate Visualization & Summaries"
    ]
  },
  {
    "objectID": "activities/02-foundations-univariate.html#advice-make-an-r-code-cheat-sheet",
    "href": "activities/02-foundations-univariate.html#advice-make-an-r-code-cheat-sheet",
    "title": "Univariate Visualization & Summaries",
    "section": "Advice: make an R code ‚Äúcheat sheet‚Äù!",
    "text": "Advice: make an R code ‚Äúcheat sheet‚Äù!\nYou will continue to pick up new R code and ideas. You‚Äôre highly encouraged to start tracking this in a cheat sheet (eg: in a Google doc). The cheat sheet will be a handy reference for you, and the act of making it will help deepen your understanding and retention.",
    "crumbs": [
      "Univariate Visualization & Summaries"
    ]
  },
  {
    "objectID": "activities/02-foundations-univariate.html#exercise-9-read-in-and-get-to-know-the-weather-data",
    "href": "activities/02-foundations-univariate.html#exercise-9-read-in-and-get-to-know-the-weather-data",
    "title": "Univariate Visualization & Summaries",
    "section": "Exercise 9: Read in and get to know the weather data",
    "text": "Exercise 9: Read in and get to know the weather data\nDaily weather data are available for 3 locations in Perth, Australia.\n\nView the codebook here.\nComplete the code below to read in the data.\n\n\n# Replace the ??? with your own name for the weather data\n# Replace the ___ with the correct function\n??? &lt;- ___(\"https://mac-stat.github.io/data/weather_3_locations.csv\")\n## Error in parse(text = input): &lt;text&gt;:3:5: unexpected assignment\n## 2: # Replace the ___ with the correct function\n## 3: ??? &lt;-\n##        ^",
    "crumbs": [
      "Univariate Visualization & Summaries"
    ]
  },
  {
    "objectID": "activities/02-foundations-univariate.html#exercise-10-exploring-the-data-structure",
    "href": "activities/02-foundations-univariate.html#exercise-10-exploring-the-data-structure",
    "title": "Univariate Visualization & Summaries",
    "section": "Exercise 10: Exploring the data structure",
    "text": "Exercise 10: Exploring the data structure\nCheck out the basic features of the weather data.\n\n# Examine the first six cases\n\n# Find the dimensions of the data\n\nWhat does a case represent in this data?",
    "crumbs": [
      "Univariate Visualization & Summaries"
    ]
  },
  {
    "objectID": "activities/02-foundations-univariate.html#exercise-11-exploring-rainfall",
    "href": "activities/02-foundations-univariate.html#exercise-11-exploring-rainfall",
    "title": "Univariate Visualization & Summaries",
    "section": "Exercise 11: Exploring rainfall",
    "text": "Exercise 11: Exploring rainfall\nThe raintoday variable contains information about rainfall.\n\nIs this variable quantitative or categorical?\nCreate an appropriate visualization, and compute appropriate numerical summaries.\nWhat do you learn about rainfall in Perth?\n\n\n# Visualization\n\n# Numerical summaries",
    "crumbs": [
      "Univariate Visualization & Summaries"
    ]
  },
  {
    "objectID": "activities/02-foundations-univariate.html#exercise-12-exploring-temperature",
    "href": "activities/02-foundations-univariate.html#exercise-12-exploring-temperature",
    "title": "Univariate Visualization & Summaries",
    "section": "Exercise 12: Exploring temperature",
    "text": "Exercise 12: Exploring temperature\nThe maxtemp variable contains information on the daily high temperature.\n\nIs this variable quantitative or categorical?\nCreate an appropriate visualization, and compute appropriate numerical summaries.\nWhat do you learn about high temperatures in Perth?\n\n\n# Visualization\n\n# Numerical summaries",
    "crumbs": [
      "Univariate Visualization & Summaries"
    ]
  },
  {
    "objectID": "activities/02-foundations-univariate.html#exercise-13-customizing-challenge",
    "href": "activities/02-foundations-univariate.html#exercise-13-customizing-challenge",
    "title": "Univariate Visualization & Summaries",
    "section": "Exercise 13: Customizing! (CHALLENGE)",
    "text": "Exercise 13: Customizing! (CHALLENGE)\nThough you will naturally absorb some RStudio code throughout the semester, being an effective statistical thinker and ‚Äúprogrammer‚Äù does not require that we memorize all code. That would be impossible! In contrast, using the foundation you built today, do some digging online to learn how to customize your visualizations.\n\nFor the histogram below, add a title and more meaningful axis labels. Specifically, title the plot ‚ÄúDistribution of max temperatures in Perth‚Äù, change the x-axis label to ‚ÄúMaximum temperature‚Äù and y-axis label to ‚ÄúNumber of days‚Äù. HINT: Do a Google search for something like ‚Äúadd axis labels ggplot‚Äù.\n\n\n# Add a title and axis labels\nggplot(weather, aes(x = maxtemp)) + \n    geom_histogram()\n## Error: object 'weather' not found\n\n\nAdjust the code below in order to color the bars green. NOTE: Color can be an effective tool, but here it is simply gratuitous.\n\n\n# Make the bars green\nggplot(weather, aes(x = raintoday)) + \n    geom_bar()\n## Error: object 'weather' not found\n\n\nCheck out the ggplot2 cheat sheet. Try making some of the other kinds of univariate plots outlined there.\nWhat else would you like to change about your plot? Try it!",
    "crumbs": [
      "Univariate Visualization & Summaries"
    ]
  },
  {
    "objectID": "activities/02-foundations-univariate.html#exercise-14-optional-challenge",
    "href": "activities/02-foundations-univariate.html#exercise-14-optional-challenge",
    "title": "Univariate Visualization & Summaries",
    "section": "Exercise 14: Optional challenge",
    "text": "Exercise 14: Optional challenge\nAt the top of this activity, we searched for words related to some topics of interest (parents, marriage, money) and combined them into a single theme variable. It looked something like this:\n\nabby_new &lt;- abby %&gt;% \n  mutate(\n    parents = str_detect(question_only, \"mother|mama|mom|father|papa|dad\"),\n    marriage = str_detect(question_only, \"marriage|marry|married\"),\n    money = str_detect(question_only, \"money|finance\")\n  ) %&gt;%\n  rowwise() %&gt;%\n  mutate(\n    themes = c(\n      if (parents) \"parents\",\n      if (marriage) \"marriage\",\n      if (money) \"money\"\n    ) %&gt;% paste(collapse = \", \"),\n    themes = ifelse(themes == \"\", \"other\", themes)\n  ) %&gt;%\n  ungroup()\n\nCheck it out:\n\nhead(abby_new)\n## # A tibble: 6 √ó 12\n##    year month day   question_only     bing_pos afinn_overall afinn_pos afinn_neg\n##   &lt;dbl&gt; &lt;ord&gt; &lt;chr&gt; &lt;chr&gt;                &lt;dbl&gt;         &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;\n## 1  2017 Aug   30    \"i moved to the ‚Ä¶    0.75             14        16         2\n## 2  2017 Aug   30    \"under what circ‚Ä¶   NA                NA        NA        NA\n## 3  2017 Aug   28    \"i'm not a dog p‚Ä¶    0.333             5         5         0\n## 4  2017 Aug   28    \"my 62-year-old ‚Ä¶    0.143           -11         8        19\n## 5  2017 Aug   27    \"i have a friend‚Ä¶    0.222             0         7         7\n## 6  2017 Aug   27    \"i have been sel‚Ä¶    0.333            -5         2         7\n## # ‚Ñπ 4 more variables: themes &lt;chr&gt;, parents &lt;lgl&gt;, marriage &lt;lgl&gt;, money &lt;lgl&gt;\n\n\nUnderstand the code!\n\nInside mutate() the line parents = str_detect(question_only, \"mother|mama|mom|father|papa|dad\") created a new variable called parents. This variable takes on TRUE or FALSE. Explain what TRUE and FALSE mean here.\nThe themes variable combines the information from the parents, marriage, and money variables. Check out the themes for the first 3 rows / data points. Convince yourself that you understand how it corresponds to the parents, marriage, and money variables.\n\nBeyond parents, marriage, and money, what are some other topics that might pop up in the Dear Abby letters (and that you‚Äôre interested in exploring)? Modify the code below to explore those topics! Update the themes variable accordingly.\n\n\nabby_new &lt;- abby %&gt;% \n  mutate(\n    parents = str_detect(question_only, \"mother|mama|mom|father|papa|dad\"),\n    marriage = str_detect(question_only, \"marriage|marry|married\"),\n    money = str_detect(question_only, \"money|finance\")\n  ) %&gt;%\n  rowwise() %&gt;%\n  mutate(\n    themes = c(\n      if (parents) \"parents\",\n      if (marriage) \"marriage\",\n      if (money) \"money\"\n    ) %&gt;% paste(collapse = \", \"),\n    themes = ifelse(themes == \"\", \"other\", themes)\n  ) %&gt;%\n  ungroup()\n\n# Check out the raw data\nhead(abby_new)\n## # A tibble: 6 √ó 12\n##    year month day   question_only     bing_pos afinn_overall afinn_pos afinn_neg\n##   &lt;dbl&gt; &lt;ord&gt; &lt;chr&gt; &lt;chr&gt;                &lt;dbl&gt;         &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;\n## 1  2017 Aug   30    \"i moved to the ‚Ä¶    0.75             14        16         2\n## 2  2017 Aug   30    \"under what circ‚Ä¶   NA                NA        NA        NA\n## 3  2017 Aug   28    \"i'm not a dog p‚Ä¶    0.333             5         5         0\n## 4  2017 Aug   28    \"my 62-year-old ‚Ä¶    0.143           -11         8        19\n## 5  2017 Aug   27    \"i have a friend‚Ä¶    0.222             0         7         7\n## 6  2017 Aug   27    \"i have been sel‚Ä¶    0.333            -5         2         7\n## # ‚Ñπ 4 more variables: themes &lt;chr&gt;, parents &lt;lgl&gt;, marriage &lt;lgl&gt;, money &lt;lgl&gt;\n\n# Check out the number of letters belonging to each theme\nabby_new %&gt;% \n  count(themes)\n## # A tibble: 8 √ó 2\n##   themes                       n\n##   &lt;chr&gt;                    &lt;int&gt;\n## 1 marriage                    75\n## 2 marriage, money              5\n## 3 money                       21\n## 4 other                      234\n## 5 parents                    127\n## 6 parents, marriage           33\n## 7 parents, marriage, money     4\n## 8 parents, money              15",
    "crumbs": [
      "Univariate Visualization & Summaries"
    ]
  },
  {
    "objectID": "activities/02-foundations-univariate.html#exercise-1-importing-and-getting-to-know-the-data-1",
    "href": "activities/02-foundations-univariate.html#exercise-1-importing-and-getting-to-know-the-data-1",
    "title": "Univariate Visualization & Summaries",
    "section": "Exercise 1: Importing and getting to know the data",
    "text": "Exercise 1: Importing and getting to know the data\n\nNote how clicking the abby data causes both a popup pane and the command View(abby) to appear in the Console. In fact, the View() function is the underlying command that opens a dataset pane. (View() should always be entered in the Console and NOT your Quarto document.)\nEach row / case corresponds to a single question.\nColumns = variables\nTry out each function below. Identify what each function tells you about the abby data and note this in the ???:\n\n\n# First number = number of rows / cases\n# Second number = number of columns / variables\ndim(abby)\n## [1] 514   6\n\n# Number of rows (cases)\nnrow(abby)\n## [1] 514\n\n# Number of columns (variables)\nncol(abby)\n## [1] 6\n\n# View first few rows of the dataset (6 rows, by default)\nhead(abby)\n## # A tibble: 6 √ó 6\n##    year month day   question_only                                bing_pos themes\n##   &lt;dbl&gt; &lt;ord&gt; &lt;chr&gt; &lt;chr&gt;                                           &lt;dbl&gt; &lt;chr&gt; \n## 1  2017 Aug   30    \"i moved to the philippines five years ago.‚Ä¶    0.75  paren‚Ä¶\n## 2  2017 Aug   30    \"under what circumstances do you ask your a‚Ä¶   NA     money \n## 3  2017 Aug   28    \"i'm not a dog person. i'm not even an anim‚Ä¶    0.333 other \n## 4  2017 Aug   28    \"my 62-year-old father has recently started‚Ä¶    0.143 paren‚Ä¶\n## 5  2017 Aug   27    \"i have a friend, \\\"charlene,\\\" whom i met ‚Ä¶    0.222 other \n## 6  2017 Aug   27    \"i have been selected to attend a symposium‚Ä¶    0.333 other\n\n# Get all column (variable) names\nnames(abby)\n## [1] \"year\"          \"month\"         \"day\"           \"question_only\"\n## [5] \"bing_pos\"      \"themes\"\n\n\nWe can display the first 10 rows with head(abby, n = 10).",
    "crumbs": [
      "Univariate Visualization & Summaries"
    ]
  },
  {
    "objectID": "activities/02-foundations-univariate.html#exercise-2-preparing-to-summarize-and-visualize-the-data-1",
    "href": "activities/02-foundations-univariate.html#exercise-2-preparing-to-summarize-and-visualize-the-data-1",
    "title": "Univariate Visualization & Summaries",
    "section": "Exercise 2: Preparing to summarize and visualize the data",
    "text": "Exercise 2: Preparing to summarize and visualize the data\n\nThe sentiment variables are afinn_overall, afinn_pos, afinn_neg, and bing_pos, and they are quantitative. The afinn variables don‚Äôt have units but we can still get a sense of the scale by remembering that each word gets a score between -5 and 5. The bing_pos variable doesn‚Äôt have units because it‚Äôs a fraction, but we know that it ranges from 0 to 1.\ncategorical\nAppropriate visualizations:\n\nsingle quantitative variable: boxplot, histogram, density plot\nsingle categorical variable: barplot",
    "crumbs": [
      "Univariate Visualization & Summaries"
    ]
  },
  {
    "objectID": "activities/02-foundations-univariate.html#exercise-3-exploring-themes-in-the-letters-1",
    "href": "activities/02-foundations-univariate.html#exercise-3-exploring-themes-in-the-letters-1",
    "title": "Univariate Visualization & Summaries",
    "section": "Exercise 3: Exploring themes in the letters",
    "text": "Exercise 3: Exploring themes in the letters\n\nExpectations about the plot will vary\n\n\nggplot(abby, aes(x = themes)) +\n    geom_bar() +\n    theme(axis.text.x = element_text(angle = 90, vjust = 0.5, hjust=1))\n\n\n\n\n\n\n\n\n\nCounts in the table below match the barplot\n\n\n# Construct a table of counts\nabby %&gt;% \n    count(themes)\n## # A tibble: 8 √ó 2\n##   themes                       n\n##   &lt;chr&gt;                    &lt;int&gt;\n## 1 marriage                    75\n## 2 marriage, money              5\n## 3 money                       21\n## 4 other                      234\n## 5 parents                    127\n## 6 parents, marriage           33\n## 7 parents, marriage, money     4\n## 8 parents, money              15\n\n\nWhat do the plot layers do?\n\n\n# Just sets up the \"canvas\" of the plot with axis labels\nggplot(abby, aes(x = themes))\n\n\n\n\n\n\n\n\n\n# Adds the bars\nggplot(abby, aes(x = themes)) +\n    geom_bar()\n\n\n\n\n\n\n\n\n\n# Rotates the x axis labels\nggplot(abby, aes(x = themes)) +\n    geom_bar() +\n    theme(axis.text.x = element_text(angle = 90, vjust = 0.5, hjust=1))\n\n\n\n\n\n\n\n\n\n# Changes the visual theme of the plot with a white background and removes gridlines\nggplot(abby, aes(x = themes)) +\n    geom_bar() +\n    theme_classic() +\n    theme(axis.text.x = element_text(angle = 90, vjust = 0.5, hjust=1))",
    "crumbs": [
      "Univariate Visualization & Summaries"
    ]
  },
  {
    "objectID": "activities/02-foundations-univariate.html#exercise-4-exploring-sentiment-1",
    "href": "activities/02-foundations-univariate.html#exercise-4-exploring-sentiment-1",
    "title": "Univariate Visualization & Summaries",
    "section": "Exercise 4: Exploring sentiment",
    "text": "Exercise 4: Exploring sentiment\n\n\n\n\nggplot(abby, aes(x = bing_pos)) +\n    geom_boxplot()\n\n\n\n\n\n\n\n\n\nWe replace geom_boxplot() with geom_histogram() and geom_density().\n\n\n# Histogram\nggplot(abby, aes(x = bing_pos)) +\n    geom_histogram()\n\n\n\n\n\n\n\n\n# Density plot\nggplot(abby, aes(x = bing_pos)) +\n    geom_density()\n\n\n\n\n\n\n\n\n\n\nBoxplot shows min, max, median, 1st and 3rd quartile easily. (It shows median, 1st and 3rd quartile directly as lines)\nHistogram and density plot show min and max but the mean and median aren‚Äôt shown directly‚Äìwe have to roughly guess based on the peak of the distribution\n\n\n\n# Summary statistics\nabby %&gt;% \n    select(bing_pos) %&gt;% \n    summary()\n##     bing_pos     \n##  Min.   :0.0000  \n##  1st Qu.:0.1667  \n##  Median :0.3333  \n##  Mean   :0.3650  \n##  3rd Qu.:0.5000  \n##  Max.   :1.0000  \n##  NA's   :19\n\nabby %&gt;% \n    summarize(mean(bing_pos, na.rm = TRUE), median(bing_pos, na.rm = TRUE), sd(bing_pos, na.rm = TRUE))\n## # A tibble: 1 √ó 3\n##   `mean(bing_pos, na.rm = TRUE)` median(bing_pos, na.rm‚Ä¶¬π sd(bing_pos, na.rm =‚Ä¶¬≤\n##                            &lt;dbl&gt;                    &lt;dbl&gt;                  &lt;dbl&gt;\n## 1                          0.365                    0.333                  0.279\n## # ‚Ñπ abbreviated names: ¬π‚Äã`median(bing_pos, na.rm = TRUE)`,\n## #   ¬≤‚Äã`sd(bing_pos, na.rm = TRUE)`\n\n\nThe distribution of sentiment scores is roughly tri-modal, ranging from 0 to 1. There‚Äôs a group of very negative letters (with roughly 0% of words being positive), a group of very positive letters (with roughly 100% of words being positive), and a group of mostly negative letters. The typical sentiment is around 0.34.",
    "crumbs": [
      "Univariate Visualization & Summaries"
    ]
  },
  {
    "objectID": "activities/02-foundations-univariate.html#exercise-5-box-plots-vs.-histograms-vs.-density-plots-1",
    "href": "activities/02-foundations-univariate.html#exercise-5-box-plots-vs.-histograms-vs.-density-plots-1",
    "title": "Univariate Visualization & Summaries",
    "section": "Exercise 5: Box plots vs.¬†histograms vs.¬†density plots",
    "text": "Exercise 5: Box plots vs.¬†histograms vs.¬†density plots\n\nBoxplots very clearly show key summary statistics like median, 1st and 3rd quartile\nBoxplots can oversimplify by not showing the shape of the distribution.",
    "crumbs": [
      "Univariate Visualization & Summaries"
    ]
  },
  {
    "objectID": "activities/02-foundations-univariate.html#exercise-6-returning-to-our-context-looking-ahead-1",
    "href": "activities/02-foundations-univariate.html#exercise-6-returning-to-our-context-looking-ahead-1",
    "title": "Univariate Visualization & Summaries",
    "section": "Exercise 6: Returning to our context, looking ahead",
    "text": "Exercise 6: Returning to our context, looking ahead\n\nAnswers will vary",
    "crumbs": [
      "Univariate Visualization & Summaries"
    ]
  },
  {
    "objectID": "activities/02-foundations-univariate.html#exercise-9-read-in-and-get-to-know-the-weather-data-1",
    "href": "activities/02-foundations-univariate.html#exercise-9-read-in-and-get-to-know-the-weather-data-1",
    "title": "Univariate Visualization & Summaries",
    "section": "Exercise 9: Read in and get to know the weather data",
    "text": "Exercise 9: Read in and get to know the weather data\n\nweather &lt;- read_csv(\"https://raw.githubusercontent.com/Mac-STAT/data/main/weather_3_locations.csv\")",
    "crumbs": [
      "Univariate Visualization & Summaries"
    ]
  },
  {
    "objectID": "activities/02-foundations-univariate.html#exercise-10-exploring-the-data-structure-1",
    "href": "activities/02-foundations-univariate.html#exercise-10-exploring-the-data-structure-1",
    "title": "Univariate Visualization & Summaries",
    "section": "Exercise 10: Exploring the data structure",
    "text": "Exercise 10: Exploring the data structure\nCheck out the basic features of the weather data.\n\n# Examine the first six cases\nhead(weather)\n## # A tibble: 6 √ó 24\n##   date       location  mintemp maxtemp rainfall evaporation sunshine windgustdir\n##   &lt;date&gt;     &lt;chr&gt;       &lt;dbl&gt;   &lt;dbl&gt;    &lt;dbl&gt;       &lt;dbl&gt;    &lt;dbl&gt; &lt;chr&gt;      \n## 1 2020-01-01 Wollongo‚Ä¶    17.1    23.1        0          NA       NA SSW        \n## 2 2020-01-02 Wollongo‚Ä¶    17.7    24.2        0          NA       NA SSW        \n## 3 2020-01-03 Wollongo‚Ä¶    19.7    26.8        0          NA       NA NE         \n## 4 2020-01-04 Wollongo‚Ä¶    20.4    35.5        0          NA       NA SSW        \n## 5 2020-01-05 Wollongo‚Ä¶    19.8    21.4        0          NA       NA SSW        \n## 6 2020-01-06 Wollongo‚Ä¶    18.3    22.9        0          NA       NA NE         \n## # ‚Ñπ 16 more variables: windgustspeed &lt;dbl&gt;, winddir9am &lt;chr&gt;, winddir3pm &lt;chr&gt;,\n## #   windspeed9am &lt;dbl&gt;, windspeed3pm &lt;dbl&gt;, humidity9am &lt;dbl&gt;,\n## #   humidity3pm &lt;dbl&gt;, pressure9am &lt;dbl&gt;, pressure3pm &lt;dbl&gt;, cloud9am &lt;dbl&gt;,\n## #   cloud3pm &lt;dbl&gt;, temp9am &lt;dbl&gt;, temp3pm &lt;dbl&gt;, raintoday &lt;chr&gt;,\n## #   risk_mm &lt;dbl&gt;, raintomorrow &lt;chr&gt;\n\n# Find the dimensions of the data\ndim(weather)\n## [1] 2367   24\n\nA case represents a day of the year in a particular area (Hobart, Uluru, Wollongong as seen by the location variable).",
    "crumbs": [
      "Univariate Visualization & Summaries"
    ]
  },
  {
    "objectID": "activities/02-foundations-univariate.html#exercise-11-exploring-rainfall-1",
    "href": "activities/02-foundations-univariate.html#exercise-11-exploring-rainfall-1",
    "title": "Univariate Visualization & Summaries",
    "section": "Exercise 11: Exploring rainfall",
    "text": "Exercise 11: Exploring rainfall\nThe raintoday variable contains information about rainfall.\n\nraintoday is categorical (No, Yes)\nIt is more common to have no rain.\n\n\n# Visualization\nggplot(weather, aes(x = raintoday)) +\n    geom_bar()\n\n\n\n\n\n\n\n\n# Numerical summaries\nweather %&gt;% \n    count(raintoday)\n## # A tibble: 3 √ó 2\n##   raintoday     n\n##   &lt;chr&gt;     &lt;int&gt;\n## 1 No         1864\n## 2 Yes         446\n## 3 &lt;NA&gt;         57",
    "crumbs": [
      "Univariate Visualization & Summaries"
    ]
  },
  {
    "objectID": "activities/02-foundations-univariate.html#exercise-12-exploring-temperature-1",
    "href": "activities/02-foundations-univariate.html#exercise-12-exploring-temperature-1",
    "title": "Univariate Visualization & Summaries",
    "section": "Exercise 12: Exploring temperature",
    "text": "Exercise 12: Exploring temperature\nThe maxtemp variable contains information on the daily high temperature.\n\nmaxtemp is quantitative\nThe typical max temperature is around 23 degrees Celsius (with an average of 23.62 and a median of 22 degrees). The max temperatures ranged from 8.6 to 45.4 degrees. Finally, on the typical day, the max temp falls about 7.8 degrees from the mean. There are multiple modes in the distribution of max temperature‚Äîthis likely reflects the different cities in the dataset.\n\n\n# Visualization\nggplot(weather, aes(x = maxtemp)) + \n    geom_histogram()\n\n\n\n\n\n\n\n\n# Numerical summaries\nsummary(weather$maxtemp)\n##    Min. 1st Qu.  Median    Mean 3rd Qu.    Max.    NA's \n##    8.60   18.10   22.00   23.62   27.40   45.40      34\n\n# There are missing values (NAs) in this variable, so we add\n# the na.rm = TRUE argument\nweather %&gt;% \n    summarize(sd(maxtemp, na.rm = TRUE))\n## # A tibble: 1 √ó 1\n##   `sd(maxtemp, na.rm = TRUE)`\n##                         &lt;dbl&gt;\n## 1                        7.80",
    "crumbs": [
      "Univariate Visualization & Summaries"
    ]
  },
  {
    "objectID": "activities/02-foundations-univariate.html#exercise-13-customizing-challenge-1",
    "href": "activities/02-foundations-univariate.html#exercise-13-customizing-challenge-1",
    "title": "Univariate Visualization & Summaries",
    "section": "Exercise 13: Customizing! (CHALLENGE)",
    "text": "Exercise 13: Customizing! (CHALLENGE)\n\n\n\n\nggplot(weather, aes(x = maxtemp)) + \n    geom_histogram() + \n    labs(x = \"Maximum temperature\", y = \"Number of days\", title = \"Distribution of max temperatures in Perth\")\n\n\n\n\n\n\n\n\n\n\n\n\n# Make the bars green\nggplot(weather, aes(x = raintoday)) + \n    geom_bar(fill = \"green\")",
    "crumbs": [
      "Univariate Visualization & Summaries"
    ]
  },
  {
    "objectID": "activities/05-slr-model-eval.html",
    "href": "activities/05-slr-model-eval.html",
    "title": "Simple Linear Regression - Model Evaluation",
    "section": "",
    "text": "You can download the .qmd file for this activity here and open in R-studio. The rendered version is posted in the course website (Activities tab). I often experiment with the class activities (and see it in live!) and make updates, but I always post the final version before class starts. To be sure you have the most up-to-date copy, please download it once you‚Äôve settled in before class begins.",
    "crumbs": [
      "Simple Linear Regression - Model Evaluation"
    ]
  },
  {
    "objectID": "activities/05-slr-model-eval.html#learning-goals",
    "href": "activities/05-slr-model-eval.html#learning-goals",
    "title": "Simple Linear Regression - Model Evaluation",
    "section": "Learning goals",
    "text": "Learning goals\nBy the end of this lesson, you should be able to:\n\nUse residual plots to evaluate the correctness of a model\nExplain the rationale for the R-squared metric of model strength\nInterpret the R-squared metric\nThink about ethical implications of modeling by examining the impacts of biased data, power dynamics, the role of categorization, and the role of emotion and lived experience",
    "crumbs": [
      "Simple Linear Regression - Model Evaluation"
    ]
  },
  {
    "objectID": "activities/05-slr-model-eval.html#readings-and-videos",
    "href": "activities/05-slr-model-eval.html#readings-and-videos",
    "title": "Simple Linear Regression - Model Evaluation",
    "section": "Readings and videos",
    "text": "Readings and videos\nChoose either the reading or the videos to go through before class.\n\nReading: Sections 1.7, 3.7, and 3.8 in the STAT 155 Notes\n\nNote: You do not need to focus on the ‚ÄúLadder of Power‚Äù in Section 3.8. Transformations in general will be the focus of the next activity we do.\n\nVideos:\n\nModel evaluation: is the model wrong? (slides)\nModel evaluation: is the model strong? (slides)\nModel evaluation: is the model fair? (slides)\nR Code for Evaluating and Using a Linear Model",
    "crumbs": [
      "Simple Linear Regression - Model Evaluation"
    ]
  },
  {
    "objectID": "activities/05-slr-model-eval.html#model-assumptions",
    "href": "activities/05-slr-model-eval.html#model-assumptions",
    "title": "Simple Linear Regression - Model Evaluation",
    "section": "Model Assumptions",
    "text": "Model Assumptions\nOne way to think about model evaluation is to consider whether or not underlying assumptions of our regression models are being met (or not). Asking ourselves if our models are ‚Äúwrong‚Äù, ‚Äústrong‚Äù, and ‚Äúfair‚Äù approaches this from one perspective. To the first question (whether our model is wrong), recall the following four assumptions of linear regression:\n\nLinearity\nIndependence\nNormality\nEqual Variance\n\nNote that they spell ‚ÄúLINE‚Äù (how convenient!).\nBy assumptions, we mean that the above four ‚Äúthings‚Äù are needed mathematically in order for linear regression to ‚Äúwork‚Äù.\nWhereas we can check some of these assumptions using a residual plot, we need to examine the context of our data collection when checking the Independence assumption. What we mean by independence, is that the residuals in our model do not depend on one another. This may seem like an unsatisfying definition, so here are some examples:\n\nSuppose I want to understand the association between a person‚Äôs high school GPA and their college GPA. I collect data from every graduating senior, at three different high schools. If I have college GPA as my outcome, and high school GPA as my predictor, are my residuals independent? Probably not! It is reasonable to believe that students from the same high school may have similar GPAs, due to resources their high school may have had available, or specific teachers grading differently at one school or another. This is an example of clustering, where we have clusters of students within schools. The independence assumption of our linear regression model would be violated. One way to address this would be to include which high school they went to as an additional covariate in our regression model (we‚Äôll get to this with multiple linear regression), and more advanced methods are covered in a course on Correlated Data.\nSuppose I want to understand the association between a mouse‚Äôs weight and their water consumption across time. I collect data for 365 days for ten different mice, recording their weight and water consumption each day of the year. If I have weight as my predictor and water consumption as my outcome, are my residuals independent? Nope! This is an example of correlated data that is longitudinal in nature: I have multiple observations per individual (mouse) across time. A mouse‚Äôs weight one day is certainly not independent of it‚Äôs weight the following day. The independence assumption of our linear regression model would again be violated. One way to address this would be to include ‚ÄúMouse ID‚Äù as a predictor in our regression model (again, we‚Äôll get to this with multiple linear regression).\n\nAll types of data that will violate the independence assumption of linear regression will have some sort of correlation structure (within individual, across time, across space, etc.). Think about clusters. If your observations fall neatly into specific clusters, your data may violate the independence assumption of linear regression.\nFile organization: Save this file in the ‚ÄúActivities‚Äù subfolder of your ‚ÄúSTAT155‚Äù folder.",
    "crumbs": [
      "Simple Linear Regression - Model Evaluation"
    ]
  },
  {
    "objectID": "activities/05-slr-model-eval.html#exercise-1-is-the-model-correct",
    "href": "activities/05-slr-model-eval.html#exercise-1-is-the-model-correct",
    "title": "Simple Linear Regression - Model Evaluation",
    "section": "Exercise 1: Is the model correct?",
    "text": "Exercise 1: Is the model correct?\nLet‚Äôs revisit the Capital Bikeshare data:\n\n# Load packages and import data\nlibrary(readr)\nlibrary(ggplot2)\nlibrary(dplyr)\n\nbikes &lt;- read_csv(\"https://mac-stat.github.io/data/bikeshare.csv\")\n\nWe previously explored a model of daily ridership among registered users as a function of temperature:\n\n# Fit a linear model\nbike_model &lt;- lm(riders_registered ~ temp_feel, data = bikes)\n\n# Check it out\nsummary(bike_model)\n## \n## Call:\n## lm(formula = riders_registered ~ temp_feel, data = bikes)\n## \n## Residuals:\n##     Min      1Q  Median      3Q     Max \n## -3607.1  -959.2  -153.8   998.2  3304.8 \n## \n## Coefficients:\n##             Estimate Std. Error t value Pr(&gt;|t|)    \n## (Intercept) -667.916    251.608  -2.655  0.00811 ** \n## temp_feel     57.892      3.306  17.514  &lt; 2e-16 ***\n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n## \n## Residual standard error: 1310 on 729 degrees of freedom\n## Multiple R-squared:  0.2961, Adjusted R-squared:  0.2952 \n## F-statistic: 306.7 on 1 and 729 DF,  p-value: &lt; 2.2e-16\n\nPlot this relationship with both a curved and linear trend line. Based on this plot, do you think the model is correct? If not, which of the LINE assumptions does it violate?\n\n# Plot temp_feel vs riders_registered with a model trend\nggplot(bikes, aes(x = temp_feel, y = riders_registered)) + \n    geom_point() + \n    geom_smooth(method = \"lm\", se = FALSE) +\n    geom_smooth(se = FALSE, color = \"red\")",
    "crumbs": [
      "Simple Linear Regression - Model Evaluation"
    ]
  },
  {
    "objectID": "activities/05-slr-model-eval.html#exercise-2-residual-plots",
    "href": "activities/05-slr-model-eval.html#exercise-2-residual-plots",
    "title": "Simple Linear Regression - Model Evaluation",
    "section": "Exercise 2: Residual plots",
    "text": "Exercise 2: Residual plots\nPlotting the residuals vs the predictions (also called ‚Äúfitted values‚Äù) for each case can help us assess how wrong our model is. This will be a particularly important tool when evaluating models with multiple predictors. Construct the residual plot for bike_model. As with the scatterplot, this plot indicates that bike_model violates one of the LINE assumptions. Explain which assumption that is and how you can tell that from just the residual plot.\nNotes:\n\nInformation about the residuals (.resid) and predictions (.fitted) are stored within our model, thus we start our ggplot() with the model name as opposed to the raw dataset. We will rarely start ggplot() with a model instead of the data.\nWe can fix this model by adding a quadratic ‚Äútransformation term‚Äù.\n\n\n# Check out the residual plot for bike_model\nggplot(bike_model, aes(x = .fitted, y = .resid)) + \n    geom_point() + \n    geom_hline(yintercept = 0) + # Check this first!\n    geom_smooth(se = FALSE)",
    "crumbs": [
      "Simple Linear Regression - Model Evaluation"
    ]
  },
  {
    "objectID": "activities/05-slr-model-eval.html#exercise-3-whats-incorrect-about-this-model",
    "href": "activities/05-slr-model-eval.html#exercise-3-whats-incorrect-about-this-model",
    "title": "Simple Linear Regression - Model Evaluation",
    "section": "Exercise 3: What‚Äôs incorrect about this model?",
    "text": "Exercise 3: What‚Äôs incorrect about this model?\nConsider another example. The mammals data includes data on the average brain weight (g) and body weight (kg) for a variety of mammals:\n\n# Import the data\nmammals &lt;- read_csv(\"https://mac-stat.github.io/data/mammals.csv\")\n\n# Check it out\nhead(mammals)\n## # A tibble: 6 √ó 4\n##    ...1 animal            body brain\n##   &lt;dbl&gt; &lt;chr&gt;            &lt;dbl&gt; &lt;dbl&gt;\n## 1     1 Arctic fox        3.38  44.5\n## 2     2 Owl monkey        0.48  15.5\n## 3     3 Mountain beaver   1.35   8.1\n## 4     4 Cow             465    423  \n## 5     5 Grey wolf        36.3  120. \n## 6     6 Goat             27.7  115\n\nFit a model of brain vs body weight:\n\n# Construct the model\nmammal_model &lt;- lm(brain ~ body, mammals)\n\n# Check it out\nsummary(mammal_model)\n## \n## Call:\n## lm(formula = brain ~ body, data = mammals)\n## \n## Residuals:\n##     Min      1Q  Median      3Q     Max \n## -810.07  -88.52  -79.64  -13.02 2050.33 \n## \n## Coefficients:\n##             Estimate Std. Error t value Pr(&gt;|t|)    \n## (Intercept) 91.00440   43.55258    2.09   0.0409 *  \n## body         0.96650    0.04766   20.28   &lt;2e-16 ***\n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n## \n## Residual standard error: 334.7 on 60 degrees of freedom\n## Multiple R-squared:  0.8727, Adjusted R-squared:  0.8705 \n## F-statistic: 411.2 on 1 and 60 DF,  p-value: &lt; 2.2e-16\n\n\nConstruct two plots that will help us evaluate mammal_model:\n\n\n# Scatterplot of brain weight (y) vs body weight (x)\n# Include a model trend line (i.e. a representation of mammal_model)\nggplot(mammals, aes(x = body, y = brain)) + \n    geom_point() + \n    geom_smooth(method = \"lm\", se = FALSE) +\n    geom_smooth(se = FALSE, color = \"red\")\n\n\n\n\n\n\n\n\n\n# Residual plot for mammal_model\n\n\nThese two plots confirm that our model is wrong. What is wrong? That is, which of the LINE assumptions are violated? (NOTE: We again can fix this model by ‚Äútransforming‚Äù one or both of the brain and body variables.",
    "crumbs": [
      "Simple Linear Regression - Model Evaluation"
    ]
  },
  {
    "objectID": "activities/05-slr-model-eval.html#exercise-4-exploring-mammals",
    "href": "activities/05-slr-model-eval.html#exercise-4-exploring-mammals",
    "title": "Simple Linear Regression - Model Evaluation",
    "section": "Exercise 4: Exploring mammals",
    "text": "Exercise 4: Exploring mammals\nJust for fun, let‚Äôs dig into the mammals data. Discuss what you observe:\n\n# Label the points by the animal name!\n# Discuss: What 2 things are new in this code?\nggplot(mammals, aes(x = body, y = brain, label = animal)) + \n    geom_text() + \n    geom_smooth(method = \"lm\", se = FALSE) \n\n\n\n\n\n\n\n\n\n# Zoom in\nggplot(mammals, aes(x = body, y = brain, label = animal)) + \n    geom_text() + \n    lims(y = c(0, 1500), x = c(0, 600))\n\n\n\n\n\n\n\n\n\n# Zoom in more\nggplot(mammals, aes(x = body, y = brain, label = animal)) + \n    geom_text() + \n    lims(y = c(0, 500), x = c(0, 200))",
    "crumbs": [
      "Simple Linear Regression - Model Evaluation"
    ]
  },
  {
    "objectID": "activities/05-slr-model-eval.html#exercise-5-is-the-model-strong-developing-r-squared-intuition",
    "href": "activities/05-slr-model-eval.html#exercise-5-is-the-model-strong-developing-r-squared-intuition",
    "title": "Simple Linear Regression - Model Evaluation",
    "section": "Exercise 5: Is the model strong? Developing R-squared intuition",
    "text": "Exercise 5: Is the model strong? Developing R-squared intuition\nThe R-squared metric is a way to quantify the strength of a model. It measures how much variation in the outcome/response variable can be explained by the variation in the predictors.\nWhere does R-squared come from? Well, it turns out that we can partition the variance of the observed response values into the variability that‚Äôs explained by the model (the variance of the predictions) and the variability that‚Äôs left unexplained by the model (the variance of the residuals):\n\\[\\text{Var(observed) = Var(predicted) + Var(residuals)}\\]\nStrong models have residuals that don‚Äôt deviate far from 0. So the smaller the variance in the residuals (thus larger the variance in the predictions), the stronger the model. Take a look at the picture below and write a few sentences addressing the following:\n\nThe two rows of plots show a stronger and a weaker model. Just by looking at the blue trend line and the dispersion of the points about the line, which row corresponds to the stronger model? How can you tell? Which row would you expect to have a higher correlation?\nWhat is different about the variance of the residuals from the first to the second row?\n\n\n\nThe first row corresponds to the weaker model. We can tell because the points are much more dispersed from the trend line than in the second row. Recall that the correlation metric measures how closely clustered points are about a straight line of best fit, so we would expect the correlation to be lower for the first row than the second row.\nThe variance of the residuals is much lower for the second row‚Äîthe residuals are all quite small. This indicates a stronger model.\n\nPutting this together, the R-squared compares Var(predicted) to Var(response):\n\\[R^2 = \\frac{\\text{variance of predicted values}}{\\text{variance of observed response values}} = 1 - \\frac{\\text{variance of residuals}}{\\text{variance of observed response values}}\\]\n\n\n\n\n\n\nR-squared\n\n\n\n\n\n\\[\nR^2 = 1 - \\frac{SSE}{SSTO} = 1 - \\frac{\\sum (y_i - \\hat{y}_i)^2}{\\sum (y_i - \\bar{y})^2}\n\\] where \\(y_i\\) are our observed outcomes, \\(i = 1, \\dots, n\\), \\(\\hat{y}_i\\) are our fitted values/predictions, and \\(\\bar{y}\\) is our observed average outcome.",
    "crumbs": [
      "Simple Linear Regression - Model Evaluation"
    ]
  },
  {
    "objectID": "activities/05-slr-model-eval.html#exercise-6-r-squared-interpretations",
    "href": "activities/05-slr-model-eval.html#exercise-6-r-squared-interpretations",
    "title": "Simple Linear Regression - Model Evaluation",
    "section": "Exercise 6: R-squared Interpretations",
    "text": "Exercise 6: R-squared Interpretations\nRecall bikemod1 from Exercise 1, where we predicted registered riders by what the temperature felt like on a given day. Use the summary function to look out the model output for bikemod1, and interpret the \\(R^2\\) value for this model, in the context of the problem. (NOTE: \\(R^2\\) is reported in output here as ‚ÄúMultiple R-squared‚Äù).\n\n# Get R-squared\nsummary(bike_model)\n## \n## Call:\n## lm(formula = riders_registered ~ temp_feel, data = bikes)\n## \n## Residuals:\n##     Min      1Q  Median      3Q     Max \n## -3607.1  -959.2  -153.8   998.2  3304.8 \n## \n## Coefficients:\n##             Estimate Std. Error t value Pr(&gt;|t|)    \n## (Intercept) -667.916    251.608  -2.655  0.00811 ** \n## temp_feel     57.892      3.306  17.514  &lt; 2e-16 ***\n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n## \n## Residual standard error: 1310 on 729 degrees of freedom\n## Multiple R-squared:  0.2961, Adjusted R-squared:  0.2952 \n## F-statistic: 306.7 on 1 and 729 DF,  p-value: &lt; 2.2e-16\n\nMultiple R-squared: 0.2961\nInterpretation: 29.61% of the variation in number of registered riders on any given day can be explained by the variation in temperature (specifically, what temperature it ‚Äúfeels‚Äù like it is).",
    "crumbs": [
      "Simple Linear Regression - Model Evaluation"
    ]
  },
  {
    "objectID": "activities/05-slr-model-eval.html#exercise-7-further-exploring-r-squared",
    "href": "activities/05-slr-model-eval.html#exercise-7-further-exploring-r-squared",
    "title": "Simple Linear Regression - Model Evaluation",
    "section": "Exercise 7: Further exploring R-squared",
    "text": "Exercise 7: Further exploring R-squared\nIn this exercise, we‚Äôll look at data from a synthetic dataset called Anscombe‚Äôs quartet. Load the data in as follows, and look at the first few rows:\n\ndata(anscombe)\n\n# Look at the first few rows\nhead(anscombe)\n##   x1 x2 x3 x4   y1   y2    y3   y4\n## 1 10 10 10  8 8.04 9.14  7.46 6.58\n## 2  8  8  8  8 6.95 8.14  6.77 5.76\n## 3 13 13 13  8 7.58 8.74 12.74 7.71\n## 4  9  9  9  8 8.81 8.77  7.11 8.84\n## 5 11 11 11  8 8.33 9.26  7.81 8.47\n## 6 14 14 14  8 9.96 8.10  8.84 7.04\n\nThe anscombe data is actually 4 datasets in one: x1 and y1 go together, and so forth. Examine the coefficient estimates (in the ‚ÄúEstimate‚Äù column of the ‚ÄúCoefficients:‚Äù part) and the ‚ÄúMultiple R-squared‚Äù value on the second to last line. What do you notice? How do these models compare?\n\nanscombe_mod1 &lt;- lm(y1 ~ x1, data = anscombe)\nanscombe_mod2 &lt;- lm(y2 ~ x2, data = anscombe)\nanscombe_mod3 &lt;- lm(y3 ~ x3, data = anscombe)\nanscombe_mod4 &lt;- lm(y4 ~ x4, data = anscombe)\n\nsummary(anscombe_mod1)\n## \n## Call:\n## lm(formula = y1 ~ x1, data = anscombe)\n## \n## Residuals:\n##      Min       1Q   Median       3Q      Max \n## -1.92127 -0.45577 -0.04136  0.70941  1.83882 \n## \n## Coefficients:\n##             Estimate Std. Error t value Pr(&gt;|t|)   \n## (Intercept)   3.0001     1.1247   2.667  0.02573 * \n## x1            0.5001     0.1179   4.241  0.00217 **\n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n## \n## Residual standard error: 1.237 on 9 degrees of freedom\n## Multiple R-squared:  0.6665, Adjusted R-squared:  0.6295 \n## F-statistic: 17.99 on 1 and 9 DF,  p-value: 0.00217\nsummary(anscombe_mod2)\n## \n## Call:\n## lm(formula = y2 ~ x2, data = anscombe)\n## \n## Residuals:\n##     Min      1Q  Median      3Q     Max \n## -1.9009 -0.7609  0.1291  0.9491  1.2691 \n## \n## Coefficients:\n##             Estimate Std. Error t value Pr(&gt;|t|)   \n## (Intercept)    3.001      1.125   2.667  0.02576 * \n## x2             0.500      0.118   4.239  0.00218 **\n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n## \n## Residual standard error: 1.237 on 9 degrees of freedom\n## Multiple R-squared:  0.6662, Adjusted R-squared:  0.6292 \n## F-statistic: 17.97 on 1 and 9 DF,  p-value: 0.002179\nsummary(anscombe_mod3)\n## \n## Call:\n## lm(formula = y3 ~ x3, data = anscombe)\n## \n## Residuals:\n##     Min      1Q  Median      3Q     Max \n## -1.1586 -0.6146 -0.2303  0.1540  3.2411 \n## \n## Coefficients:\n##             Estimate Std. Error t value Pr(&gt;|t|)   \n## (Intercept)   3.0025     1.1245   2.670  0.02562 * \n## x3            0.4997     0.1179   4.239  0.00218 **\n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n## \n## Residual standard error: 1.236 on 9 degrees of freedom\n## Multiple R-squared:  0.6663, Adjusted R-squared:  0.6292 \n## F-statistic: 17.97 on 1 and 9 DF,  p-value: 0.002176\nsummary(anscombe_mod4)\n## \n## Call:\n## lm(formula = y4 ~ x4, data = anscombe)\n## \n## Residuals:\n##    Min     1Q Median     3Q    Max \n## -1.751 -0.831  0.000  0.809  1.839 \n## \n## Coefficients:\n##             Estimate Std. Error t value Pr(&gt;|t|)   \n## (Intercept)   3.0017     1.1239   2.671  0.02559 * \n## x4            0.4999     0.1178   4.243  0.00216 **\n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n## \n## Residual standard error: 1.236 on 9 degrees of freedom\n## Multiple R-squared:  0.6667, Adjusted R-squared:  0.6297 \n## F-statistic:    18 on 1 and 9 DF,  p-value: 0.002165\n\n\nAll of these models have close to the same intercept, slope, and R-squared!\n\nNow take a look at the following scatterplots of the 4 pairs of variables. What do you notice? What takeaway can we draw from this exercise?\n\nggplot(anscombe, aes(x = x1, y = y1)) +\n    geom_point() + \n    geom_smooth(method = \"lm\", color = \"red\", se = FALSE)\n\n\n\n\n\n\n\n\nggplot(anscombe, aes(x = x2, y = y2)) +\n    geom_point() + \n    geom_smooth(method = \"lm\", color = \"red\", se = FALSE)\n\n\n\n\n\n\n\n\nggplot(anscombe, aes(x = x3, y = y3)) +\n    geom_point() + \n    geom_smooth(method = \"lm\", color = \"red\", se = FALSE)\n\n\n\n\n\n\n\n\nggplot(anscombe, aes(x = x4, y = y4)) +\n    geom_point() + \n    geom_smooth(method = \"lm\", color = \"red\", se = FALSE)\n\n\n\n\n\n\n\n\n\nx2 and y2: The scatterplot is clearly curved‚Äîa ‚Äúlinear‚Äù regression model with squared terms, for example, would be more appropriate for this data. (We‚Äôll talk more about ways to handle nonlinear relationships soon!)\nx3 and y3: There is a very clear outlier at about x3 = 13 that we would want to dig into to better understand the context. After that investigation, we might consider removing this outlier and refitting the model.\nx4 and y4: There is clearly something strange going on with most of the cases having an x4 value of exactly 8. We would not want to jump straight into modeling. Instead, we should dig deeper to find out more about this data.\n\n\nComplete Exercise 8-9 after the class.",
    "crumbs": [
      "Simple Linear Regression - Model Evaluation"
    ]
  },
  {
    "objectID": "activities/05-slr-model-eval.html#exercise-8-biased-data-biased-results-example-1",
    "href": "activities/05-slr-model-eval.html#exercise-8-biased-data-biased-results-example-1",
    "title": "Simple Linear Regression - Model Evaluation",
    "section": "Exercise 8: Biased data, biased results: example 1",
    "text": "Exercise 8: Biased data, biased results: example 1\nIn the above exercises, we focused on exploring our first 2 model evaluation questions: Is it correct? Is it strong? In the next exercises, let‚Äôs explore the third question: Is it fair?\nData are not neutral. Data can reflect personal biases, institutional biases, power dynamics, societal biases, the limits of our knowledge, etc. And biased data can lead to biased analyses. Consider the example of a large company that developed a model / algorithm to review the r√©sum√©s of applicants for software developer & other tech positions. The model then gave each applicant a score indicating their hire-ability or potential for success at the company. You can think of this model as something like:\n\\[E[\\text{potential | r√©sum√© features}] = \\beta_0 + \\beta_1 (\\text{r√©sum√© features})\\]\nSkim this Reuter‚Äôs article about the company‚Äôs r√©sum√© model.\n\nExplain why the data used by this model are not neutral.\nWhat are the potential implications, personal or societal, of the results produced from this biased data?",
    "crumbs": [
      "Simple Linear Regression - Model Evaluation"
    ]
  },
  {
    "objectID": "activities/05-slr-model-eval.html#exercise-9-biased-data-biased-results-example-2",
    "href": "activities/05-slr-model-eval.html#exercise-9-biased-data-biased-results-example-2",
    "title": "Simple Linear Regression - Model Evaluation",
    "section": "Exercise 9: Biased data, biased results: example 2",
    "text": "Exercise 9: Biased data, biased results: example 2\nWhen working with categorical variables, we‚Äôve seen that our units of observation fall into neat groups. Reality isn‚Äôt so discrete. For example, check out questions 6 and 9 on page 2 of the 2020 US Census. With your group, discuss the following:\n\nWhat are a couple of issues you see with these questions?\nWhat impact might this type of data collection have on a subsequent analysis of the census responses and the policies it might inform?\nCan you think of a better way to write these questions while still preserving the privacy of respondents?\n\nFOR A DEEPER DISCUSSION: Read Chapter 4 of Data Feminism on ‚ÄúWhat gets counted counts‚Äù.",
    "crumbs": [
      "Simple Linear Regression - Model Evaluation"
    ]
  },
  {
    "objectID": "activities/05-slr-model-eval.html#exercise-10-data-visualization-drill",
    "href": "activities/05-slr-model-eval.html#exercise-10-data-visualization-drill",
    "title": "Simple Linear Regression - Model Evaluation",
    "section": "Exercise 10: Data & visualization drill",
    "text": "Exercise 10: Data & visualization drill\n\nLet‚Äôs practice some data wrangling with our peaks data. In addition to some basic functions (e.g.¬†head()), use the tidyverse functions we‚Äôve been accumulating: select(), summarize(), filter().\n\n\n# How many hikes are in the data set?\n\n\n# Show just the peak's name and its elevation for the first 6 hikes\n___ %&gt;% \n  ___ %&gt;% \n  head()\n\n# Calculate the average hiking time\n\n\n# Show data for the hikes that are *at least* 17 miles long\n\n\n# Calculate the average hiking time for hikes that are at least 17 miles long\n# HINT: You'll need to use 2 tidyverse functions!\n___ %&gt;% \n  ___ %&gt;% \n  ___\n\n# Calculate the average hiking time for hikes with an \"easy\" rating\n# HINT: You'll need to use 2 tidyverse functions again!\n\n## Error in parse(text = input): &lt;text&gt;:5:2: unexpected input\n## 4: # Show just the peak's name and its elevation for the first 6 hikes\n## 5: __\n##     ^\n\n\nLet‚Äôs practice some visualization!\n\n\n# Construct a visualization of hike rating\n\n\n# Construct a visualization of hike length\n\n\n# Construct a visualization of the relationship between hiking time and length (distance)\n\n\nSummarize, in words, what you learned from each plot above.",
    "crumbs": [
      "Simple Linear Regression - Model Evaluation"
    ]
  },
  {
    "objectID": "activities/05-slr-model-eval.html#exercise-10-types-of-models",
    "href": "activities/05-slr-model-eval.html#exercise-10-types-of-models",
    "title": "Simple Linear Regression - Model Evaluation",
    "section": "Exercise 10: Types of models",
    "text": "Exercise 10: Types of models\n\nIs it possible to build a model that‚Äôs correct but weak? If not, explain. If yes, sketch an example of what this might look like.\nIs it possible to build a model that‚Äôs wrong but strong? If not, explain. If yes, sketch an example of what this might look like.",
    "crumbs": [
      "Simple Linear Regression - Model Evaluation"
    ]
  },
  {
    "objectID": "activities/05-slr-model-eval.html#exercise-11-model-practice",
    "href": "activities/05-slr-model-eval.html#exercise-11-model-practice",
    "title": "Simple Linear Regression - Model Evaluation",
    "section": "Exercise 11: Model practice",
    "text": "Exercise 11: Model practice\nLet‚Äôs explore the temperature data for Hobart, including the temperature in celsius at 3pm and 9am:\n\ntemps &lt;- read_csv(\"https://mac-stat.github.io/data/weather_3_locations.csv\") %&gt;% \n  filter(location == \"Hobart\") %&gt;% \n  select(date, temp3pm, temp9am)\n\nhead(temps)\n## # A tibble: 6 √ó 3\n##   date       temp3pm temp9am\n##   &lt;date&gt;       &lt;dbl&gt;   &lt;dbl&gt;\n## 1 2020-01-01    22.1    17.4\n## 2 2020-01-02    19      19  \n## 3 2020-01-03    19.7    16.2\n## 4 2020-01-04    20.6    19.9\n## 5 2020-01-05    21.9    14.2\n## 6 2020-01-06    20.8    15.3\n\n\nPart a\nBetween temp3pm and temp9am, which is the response variable and which is the predictor?\n\n\nPart b\nVisualize the relationship between these 2 variables, including representations of the raw data and the simple linear regression model. Describe your observations.\n\n\nPart c\nBuild the linear regression model. Write out the estimated model formula and interpret the 2 coefficients.",
    "crumbs": [
      "Simple Linear Regression - Model Evaluation"
    ]
  },
  {
    "objectID": "activities/05-slr-model-eval.html#exercise-1-is-the-model-correct-1",
    "href": "activities/05-slr-model-eval.html#exercise-1-is-the-model-correct-1",
    "title": "Simple Linear Regression - Model Evaluation",
    "section": "Exercise 1: Is the model correct?",
    "text": "Exercise 1: Is the model correct?\nThe red curved trend line shows a clear downward trend around 85 degrees, which contextually makes plenty of sense‚Äîextremely hot days would naturally see less riders. Overall the combination of the upward trend and downward trend makes for a curved relationship that is not captured well by a straight line of best fit. Specifically, a simple linear regression model would violate the Linearity assumption.\n\n# Load packages and import data\nlibrary(readr)\nlibrary(ggplot2)\nlibrary(dplyr)\n\nbikes &lt;- read_csv(\"https://mac-stat.github.io/data/bikeshare.csv\")\n\nggplot(bikes, aes(x = temp_feel, y = riders_registered)) + \n    geom_point() + \n    geom_smooth(se = FALSE, color = \"red\") +\n    geom_smooth(method = \"lm\", se = FALSE)",
    "crumbs": [
      "Simple Linear Regression - Model Evaluation"
    ]
  },
  {
    "objectID": "activities/05-slr-model-eval.html#exercise-2-residual-plots-1",
    "href": "activities/05-slr-model-eval.html#exercise-2-residual-plots-1",
    "title": "Simple Linear Regression - Model Evaluation",
    "section": "Exercise 2: Residual plots",
    "text": "Exercise 2: Residual plots\nThe residual plot shows a lingering trend in the residuals‚Äîthe blue curve traces the trend in the residuals, and it does not lie flat on the y = 0 line. This again suggests that the Linearity assumption is violated.\n\nbike_model &lt;- lm(riders_registered ~ temp_feel, data = bikes)\n\n# Check out the residual plot for bike_model\nggplot(bike_model, aes(x = .fitted, y = .resid)) + \n    geom_point() + \n    geom_hline(yintercept = 0) +\n    geom_smooth(se = FALSE)",
    "crumbs": [
      "Simple Linear Regression - Model Evaluation"
    ]
  },
  {
    "objectID": "activities/05-slr-model-eval.html#exercise-3-whats-incorrect-about-this-model-1",
    "href": "activities/05-slr-model-eval.html#exercise-3-whats-incorrect-about-this-model-1",
    "title": "Simple Linear Regression - Model Evaluation",
    "section": "Exercise 3: What‚Äôs incorrect about this model?",
    "text": "Exercise 3: What‚Äôs incorrect about this model?\n\n# Import the data\nmammals &lt;- read_csv(\"https://mac-stat.github.io/data/mammals.csv\")\n\n# Check it out\nhead(mammals)\n## # A tibble: 6 √ó 4\n##    ...1 animal            body brain\n##   &lt;dbl&gt; &lt;chr&gt;            &lt;dbl&gt; &lt;dbl&gt;\n## 1     1 Arctic fox        3.38  44.5\n## 2     2 Owl monkey        0.48  15.5\n## 3     3 Mountain beaver   1.35   8.1\n## 4     4 Cow             465    423  \n## 5     5 Grey wolf        36.3  120. \n## 6     6 Goat             27.7  115\n\n# Construct the model\nmammal_model &lt;- lm(brain ~ body, mammals)\n\n# Check it out\nsummary(mammal_model)\n## \n## Call:\n## lm(formula = brain ~ body, data = mammals)\n## \n## Residuals:\n##     Min      1Q  Median      3Q     Max \n## -810.07  -88.52  -79.64  -13.02 2050.33 \n## \n## Coefficients:\n##             Estimate Std. Error t value Pr(&gt;|t|)    \n## (Intercept) 91.00440   43.55258    2.09   0.0409 *  \n## body         0.96650    0.04766   20.28   &lt;2e-16 ***\n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n## \n## Residual standard error: 334.7 on 60 degrees of freedom\n## Multiple R-squared:  0.8727, Adjusted R-squared:  0.8705 \n## F-statistic: 411.2 on 1 and 60 DF,  p-value: &lt; 2.2e-16\n\n\n\n\n\n# Scatterplot of brain weight (y) vs body weight (x)\n# Include a model trend line (i.e. a representation of mammal_model)\nggplot(mammals, aes(y = brain, x = body)) + \n  geom_point() + \n  geom_smooth(method = \"lm\", se = FALSE)\n\n\n\n\n\n\n\n\n# Residual plot for mammal_model\nggplot(mammal_model, aes(x = .fitted, y = .resid)) + \n    geom_point() + \n    geom_hline(yintercept = 0) +\n    geom_smooth(se = FALSE)\n\n\n\n\n\n\n\n\n\nThe biggest issue here is that the assumption of equal variance is violated. There‚Äôs much greater variability in the residuals as the predictions increase. This is because there‚Äôs much greater variability in the brain weights (y) as body weights (x) increase.",
    "crumbs": [
      "Simple Linear Regression - Model Evaluation"
    ]
  },
  {
    "objectID": "activities/05-slr-model-eval.html#exercise-4-exploring-mammals-1",
    "href": "activities/05-slr-model-eval.html#exercise-4-exploring-mammals-1",
    "title": "Simple Linear Regression - Model Evaluation",
    "section": "Exercise 4: Exploring mammals",
    "text": "Exercise 4: Exploring mammals\nAnswers will vary.",
    "crumbs": [
      "Simple Linear Regression - Model Evaluation"
    ]
  },
  {
    "objectID": "activities/05-slr-model-eval.html#exercise-5-is-the-model-strong-developing-r-squared-intuition-1",
    "href": "activities/05-slr-model-eval.html#exercise-5-is-the-model-strong-developing-r-squared-intuition-1",
    "title": "Simple Linear Regression - Model Evaluation",
    "section": "Exercise 5: Is the model strong? Developing R-squared intuition",
    "text": "Exercise 5: Is the model strong? Developing R-squared intuition\nThe R-squared metric is a way to quantify the strength of a model. It measures how much variation in the outcome/response variable can be explained by the model.\nWhere does R-squared come from? Well, it turns out that we can partition the variance of the observed response values into the variability that‚Äôs explained by the model (the variance of the predictions) and the variability that‚Äôs left unexplained by the model (the variance of the residuals):\n\\[\\text{Var(observed) = Var(predicted) + Var(residuals)}\\]\n‚ÄúGood‚Äù models have residuals that don‚Äôt deviate far from 0. So the smaller the variance in the residuals (thus larger the variance in the predictions), the stronger the model. Take a look at the picture below and write a few sentences addressing the following:\n\nThe first row corresponds to the weaker model. We can tell because the points are much more dispersed from the trend line than in the second row. Recall that the correlation metric measures how closely clustered points are about a straight line of best fit, so we would expect the correlation to be lower for the first row than the second row.\nThe variance of the residuals is much lower for the second row‚Äîthe residuals are all quite small. This indicates a stronger model.",
    "crumbs": [
      "Simple Linear Regression - Model Evaluation"
    ]
  },
  {
    "objectID": "activities/05-slr-model-eval.html#exercise-6-r-squared-interpretations-1",
    "href": "activities/05-slr-model-eval.html#exercise-6-r-squared-interpretations-1",
    "title": "Simple Linear Regression - Model Evaluation",
    "section": "Exercise 6: R-squared Interpretations",
    "text": "Exercise 6: R-squared Interpretations\n\nsummary(bike_model)\n## \n## Call:\n## lm(formula = riders_registered ~ temp_feel, data = bikes)\n## \n## Residuals:\n##     Min      1Q  Median      3Q     Max \n## -3607.1  -959.2  -153.8   998.2  3304.8 \n## \n## Coefficients:\n##             Estimate Std. Error t value Pr(&gt;|t|)    \n## (Intercept) -667.916    251.608  -2.655  0.00811 ** \n## temp_feel     57.892      3.306  17.514  &lt; 2e-16 ***\n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n## \n## Residual standard error: 1310 on 729 degrees of freedom\n## Multiple R-squared:  0.2961, Adjusted R-squared:  0.2952 \n## F-statistic: 306.7 on 1 and 729 DF,  p-value: &lt; 2.2e-16\n\nMultiple R-squared: 0.2961\nInterpretation: 29.61% of the variation in number of registered riders on any given day can be explained by the variation in temperature (specifically, what temperature it ‚Äúfeels‚Äù like it is).",
    "crumbs": [
      "Simple Linear Regression - Model Evaluation"
    ]
  },
  {
    "objectID": "activities/05-slr-model-eval.html#exercise-7-further-exploring-r-squared-1",
    "href": "activities/05-slr-model-eval.html#exercise-7-further-exploring-r-squared-1",
    "title": "Simple Linear Regression - Model Evaluation",
    "section": "Exercise 7: Further exploring R-squared",
    "text": "Exercise 7: Further exploring R-squared\nIn this exercise, we‚Äôll look at data from a synthetic dataset called Anscombe‚Äôs quartet. Load the data in as follows, and look at the first few rows:\n\ndata(anscombe)\n\n# Look at the first few rows\nhead(anscombe)\n##   x1 x2 x3 x4   y1   y2    y3   y4\n## 1 10 10 10  8 8.04 9.14  7.46 6.58\n## 2  8  8  8  8 6.95 8.14  6.77 5.76\n## 3 13 13 13  8 7.58 8.74 12.74 7.71\n## 4  9  9  9  8 8.81 8.77  7.11 8.84\n## 5 11 11 11  8 8.33 9.26  7.81 8.47\n## 6 14 14 14  8 9.96 8.10  8.84 7.04\n\nAll of these models have close to the same intercept, slope, and R-squared!\n\nanscombe_mod1 &lt;- lm(y1 ~ x1, data = anscombe)\nanscombe_mod2 &lt;- lm(y2 ~ x2, data = anscombe)\nanscombe_mod3 &lt;- lm(y3 ~ x3, data = anscombe)\nanscombe_mod4 &lt;- lm(y4 ~ x4, data = anscombe)\n\nsummary(anscombe_mod1)\n## \n## Call:\n## lm(formula = y1 ~ x1, data = anscombe)\n## \n## Residuals:\n##      Min       1Q   Median       3Q      Max \n## -1.92127 -0.45577 -0.04136  0.70941  1.83882 \n## \n## Coefficients:\n##             Estimate Std. Error t value Pr(&gt;|t|)   \n## (Intercept)   3.0001     1.1247   2.667  0.02573 * \n## x1            0.5001     0.1179   4.241  0.00217 **\n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n## \n## Residual standard error: 1.237 on 9 degrees of freedom\n## Multiple R-squared:  0.6665, Adjusted R-squared:  0.6295 \n## F-statistic: 17.99 on 1 and 9 DF,  p-value: 0.00217\nsummary(anscombe_mod2)\n## \n## Call:\n## lm(formula = y2 ~ x2, data = anscombe)\n## \n## Residuals:\n##     Min      1Q  Median      3Q     Max \n## -1.9009 -0.7609  0.1291  0.9491  1.2691 \n## \n## Coefficients:\n##             Estimate Std. Error t value Pr(&gt;|t|)   \n## (Intercept)    3.001      1.125   2.667  0.02576 * \n## x2             0.500      0.118   4.239  0.00218 **\n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n## \n## Residual standard error: 1.237 on 9 degrees of freedom\n## Multiple R-squared:  0.6662, Adjusted R-squared:  0.6292 \n## F-statistic: 17.97 on 1 and 9 DF,  p-value: 0.002179\nsummary(anscombe_mod3)\n## \n## Call:\n## lm(formula = y3 ~ x3, data = anscombe)\n## \n## Residuals:\n##     Min      1Q  Median      3Q     Max \n## -1.1586 -0.6146 -0.2303  0.1540  3.2411 \n## \n## Coefficients:\n##             Estimate Std. Error t value Pr(&gt;|t|)   \n## (Intercept)   3.0025     1.1245   2.670  0.02562 * \n## x3            0.4997     0.1179   4.239  0.00218 **\n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n## \n## Residual standard error: 1.236 on 9 degrees of freedom\n## Multiple R-squared:  0.6663, Adjusted R-squared:  0.6292 \n## F-statistic: 17.97 on 1 and 9 DF,  p-value: 0.002176\nsummary(anscombe_mod4)\n## \n## Call:\n## lm(formula = y4 ~ x4, data = anscombe)\n## \n## Residuals:\n##    Min     1Q Median     3Q    Max \n## -1.751 -0.831  0.000  0.809  1.839 \n## \n## Coefficients:\n##             Estimate Std. Error t value Pr(&gt;|t|)   \n## (Intercept)   3.0017     1.1239   2.671  0.02559 * \n## x4            0.4999     0.1178   4.243  0.00216 **\n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n## \n## Residual standard error: 1.236 on 9 degrees of freedom\n## Multiple R-squared:  0.6667, Adjusted R-squared:  0.6297 \n## F-statistic:    18 on 1 and 9 DF,  p-value: 0.002165\n\nBut when we look at the scatterplots, they all look substantially different, and we would want to approach our modeling differently for each one:\n\nx1 and y1: A linear model seems appropriate for this data.\nx2 and y2: The scatterplot is clearly curved‚Äîa ‚Äúlinear‚Äù regression model with squared terms, for example, would be more appropriate for this data. (We‚Äôll talk more about ways to handle nonlinear relationships soon!)\nx3 and y3: There is a very clear outlier at about x3 = 13 that we would want to dig into to better understand the context. After that investigation, we might consider removing this outlier and refitting the model.\nx4 and y4: There is clearly something strange going on with most of the cases having an x4 value of exactly 8. We would not want to jump straight into modeling. Instead, we should dig deeper to find out more about this data.\n\n\nggplot(anscombe, aes(x = x1, y = y1)) +\n    geom_point() + \n    geom_smooth(method = \"lm\", color = \"red\", se = FALSE)\n\n\n\n\n\n\n\n\nggplot(anscombe, aes(x = x2, y = y2)) +\n    geom_point() + \n    geom_smooth(method = \"lm\", color = \"red\", se = FALSE)\n\n\n\n\n\n\n\n\nggplot(anscombe, aes(x = x3, y = y3)) +\n    geom_point() + \n    geom_smooth(method = \"lm\", color = \"red\", se = FALSE)\n\n\n\n\n\n\n\n\nggplot(anscombe, aes(x = x4, y = y4)) +\n    geom_point() + \n    geom_smooth(method = \"lm\", color = \"red\", se = FALSE)",
    "crumbs": [
      "Simple Linear Regression - Model Evaluation"
    ]
  },
  {
    "objectID": "activities/05-slr-model-eval.html#exercises-8---9",
    "href": "activities/05-slr-model-eval.html#exercises-8---9",
    "title": "Simple Linear Regression - Model Evaluation",
    "section": "Exercises 8 - 9",
    "text": "Exercises 8 - 9\nNo solutions for these exercises. These require longer discussions, not discrete answers.",
    "crumbs": [
      "Simple Linear Regression - Model Evaluation"
    ]
  },
  {
    "objectID": "activities/05-slr-model-eval.html#exercise-10-data-visualization-drill-1",
    "href": "activities/05-slr-model-eval.html#exercise-10-data-visualization-drill-1",
    "title": "Simple Linear Regression - Model Evaluation",
    "section": "Exercise 10: Data & visualization drill",
    "text": "Exercise 10: Data & visualization drill\n\n\n\n\n# STEP 1: Load packages and data, and examine data structure\nlibrary(tidyverse)\npeaks &lt;- read_csv(\"https://mac-stat.github.io/data/high_peaks.csv\")\nhead(peaks)\n## # A tibble: 6 √ó 7\n##   peak           elevation difficulty ascent length  time rating   \n##   &lt;chr&gt;              &lt;dbl&gt;      &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt;    \n## 1 Mt. Marcy           5344          5   3166   14.8  10   moderate \n## 2 Algonquin Peak      5114          5   2936    9.6   9   moderate \n## 3 Mt. Haystack        4960          7   3570   17.8  12   difficult\n## 4 Mt. Skylight        4926          7   4265   17.9  15   difficult\n## 5 Whiteface Mtn.      4867          4   2535   10.4   8.5 easy     \n## 6 Dix Mtn.            4857          5   2800   13.2  10   moderate\ndim(peaks)\n## [1] 46  7\n\n# How many hikes are in the data set?\nnrow(peaks)\n## [1] 46\n\n# Show just the peak's name and its elevation for the first 6 hikes\npeaks %&gt;% \n  select(peak, elevation) %&gt;% \n  head()\n## # A tibble: 6 √ó 2\n##   peak           elevation\n##   &lt;chr&gt;              &lt;dbl&gt;\n## 1 Mt. Marcy           5344\n## 2 Algonquin Peak      5114\n## 3 Mt. Haystack        4960\n## 4 Mt. Skylight        4926\n## 5 Whiteface Mtn.      4867\n## 6 Dix Mtn.            4857\n\n# Calculate the average hiking time\npeaks %&gt;% \n  summarize(mean(time))\n## # A tibble: 1 √ó 1\n##   `mean(time)`\n##          &lt;dbl&gt;\n## 1         10.7\n\n# Show data for the hikes that are at least 17 miles long\npeaks %&gt;% \n  filter(length &gt;= 17)\n## # A tibble: 7 √ó 7\n##   peak          elevation difficulty ascent length  time rating   \n##   &lt;chr&gt;             &lt;dbl&gt;      &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt;    \n## 1 Mt. Haystack       4960          7   3570   17.8  12   difficult\n## 2 Mt. Skylight       4926          7   4265   17.9  15   difficult\n## 3 Mt. Redfield       4606          7   3225   17.5  14   difficult\n## 4 Panther Peak       4442          6   3762   17.6  13.5 moderate \n## 5 Mt. Donaldson      4140          7   3490   17    17   difficult\n## 6 Mt. Emmons         4040          7   3490   18    18   difficult\n## 7 Cliff Mtn.         3960          6   2160   17.2  12   moderate\n\n# Calculate the average hiking time for hikes that are more than 14 miles long\npeaks %&gt;% \n  filter(length &gt;= 17) %&gt;% \n  summarize(mean(time))\n## # A tibble: 1 √ó 1\n##   `mean(time)`\n##          &lt;dbl&gt;\n## 1         14.5\n\n# Calculate the average hiking time for hikes with an \"easy\" rating\npeaks %&gt;% \n  filter(rating == \"easy\") %&gt;% \n  summarize(mean(time))\n## # A tibble: 1 √ó 1\n##   `mean(time)`\n##          &lt;dbl&gt;\n## 1            8\n\n\n\n\n\n# Construct a visualization of hike rating\npeaks %&gt;% \n  ggplot(aes(x = rating)) + \n  geom_bar()\n\n\n\n\n\n\n\n\n# Construct a visualization of hike length\n# a histogram or boxplot would also work\n# BUT a bar plot would NOT be appropriate\npeaks %&gt;% \n  ggplot(aes(x = length)) + \n  geom_density()\n\n\n\n\n\n\n\n\n# Construct a visualization of the relationship of hiking time with length (distance)\npeaks %&gt;% \n  ggplot(aes(y = time, x = length)) + \n  geom_point()\n\n\n\n\n\n\n\n\n\nobservations\n\nThe majority of hikes are rated as ‚Äúmoderate‚Äù, with ‚Äúdifficult‚Äù hikes being the least numerous.\nHike lengths are roughly normally distributed, with a slight left skew, around an average hike length of 12 miles and ranging from as short as ~6 miles to as long as ~18 miles.\nThere‚Äôs a strong, positive association between hiking time and hike length ‚Äì the longer the hike, the longer it tends to take to complete the hike.",
    "crumbs": [
      "Simple Linear Regression - Model Evaluation"
    ]
  },
  {
    "objectID": "activities/05-slr-model-eval.html#exercise-11-types-of-models",
    "href": "activities/05-slr-model-eval.html#exercise-11-types-of-models",
    "title": "Simple Linear Regression - Model Evaluation",
    "section": "Exercise 11: Types of models",
    "text": "Exercise 11: Types of models\n\nYes. Below is an example:\n\n\n\n\n\n\n\n\n\n\n\nNo.¬†It‚Äôs possible to have a strong relationship between Y and X, but we model that relationship incorrectly (e.g.¬†if they have a quadratic relationship but we model it with a line). But it‚Äôs impossible for a wrong model to be strong.",
    "crumbs": [
      "Simple Linear Regression - Model Evaluation"
    ]
  },
  {
    "objectID": "activities/05-slr-model-eval.html#exercise-11-model-practice-1",
    "href": "activities/05-slr-model-eval.html#exercise-11-model-practice-1",
    "title": "Simple Linear Regression - Model Evaluation",
    "section": "Exercise 11: Model practice",
    "text": "Exercise 11: Model practice\n\ntemps &lt;- read_csv(\"https://mac-stat.github.io/data/weather_3_locations.csv\") %&gt;% \n  filter(location == \"Hobart\") %&gt;% \n  select(date, temp3pm, temp9am)\n\nhead(temps)\n## # A tibble: 6 √ó 3\n##   date       temp3pm temp9am\n##   &lt;date&gt;       &lt;dbl&gt;   &lt;dbl&gt;\n## 1 2020-01-01    22.1    17.4\n## 2 2020-01-02    19      19  \n## 3 2020-01-03    19.7    16.2\n## 4 2020-01-04    20.6    19.9\n## 5 2020-01-05    21.9    14.2\n## 6 2020-01-06    20.8    15.3\n\n\nPart a\ntemp3pm = response, temp9am = predictor. it doesn‚Äôt make contextual sense to predict 9am temperature from 3pm temperature (without time travel).\n\n\nPart b\nThere is a relatively strong, positive, linear assocation between these variables ‚Äì the warmer it is at 9am, the warmer we expect it to be at 3pm.\n\ntemps %&gt;% \n  ggplot(aes(y = temp3pm, x = temp9am)) + \n  geom_point() + \n  geom_smooth(method = \"lm\", se = FALSE)\n\n\n\n\n\n\n\n\n\n\nPart c\nE[temp3pm | temp9am] = 5.21 + 0.88 temp9am\n\nWhen the 9am temperature is 0 degrees, the expected 3pm temperature is 5.21 degrees. Or, among days that are 0 degrees at 9am, the average 3pm temperature is 5.21 degrees.\nFor every 1 degree increase in 3pm temperature, the average / expected 3pm temperature increases by 0.88 degrees.\n\n\ntemp_model &lt;- lm(temp3pm ~ temp9am, temps)\nsummary(temp_model)\n## \n## Call:\n## lm(formula = temp3pm ~ temp9am, data = temps)\n## \n## Residuals:\n##      Min       1Q   Median       3Q      Max \n## -10.2361  -1.8142  -0.1131   1.6880  17.0775 \n## \n## Coefficients:\n##             Estimate Std. Error t value Pr(&gt;|t|)    \n## (Intercept)  5.20618    0.32983   15.78   &lt;2e-16 ***\n## temp9am      0.88022    0.02448   35.95   &lt;2e-16 ***\n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n## \n## Residual standard error: 2.963 on 785 degrees of freedom\n##   (2 observations deleted due to missingness)\n## Multiple R-squared:  0.6222, Adjusted R-squared:  0.6217 \n## F-statistic:  1293 on 1 and 785 DF,  p-value: &lt; 2.2e-16",
    "crumbs": [
      "Simple Linear Regression - Model Evaluation"
    ]
  },
  {
    "objectID": "activities/07-slr-cat-predictor.html",
    "href": "activities/07-slr-cat-predictor.html",
    "title": "Simple Linear Regression - Categorical Predictor",
    "section": "",
    "text": "You can download the .qmd file for this activity here and open in R-studio. The rendered version is posted in the course website (Activities tab). I often experiment with the class activities (and see it in live!) and make updates, but I always post the final version before class starts. To be sure you have the most up-to-date copy, please download it once you‚Äôve settled in before class begins.",
    "crumbs": [
      "Simple Linear Regression - Categorical Predictor"
    ]
  },
  {
    "objectID": "activities/07-slr-cat-predictor.html#learning-goals",
    "href": "activities/07-slr-cat-predictor.html#learning-goals",
    "title": "Simple Linear Regression - Categorical Predictor",
    "section": "Learning goals",
    "text": "Learning goals\nBy the end of this lesson, you should be able to:\n\nWrite a model formula for a simple linear regression model with a categorical predictor using indicator variables\nInterpret the coefficients in a simple linear regression model with a categorical predictor",
    "crumbs": [
      "Simple Linear Regression - Categorical Predictor"
    ]
  },
  {
    "objectID": "activities/07-slr-cat-predictor.html#readings-and-videos",
    "href": "activities/07-slr-cat-predictor.html#readings-and-videos",
    "title": "Simple Linear Regression - Categorical Predictor",
    "section": "Readings and videos",
    "text": "Readings and videos\nComplete both the reading and the videos to go through before class.\n\nReading: Section 3.9 in the STAT 155 Notes only up through section 3.9.1 Indicator Variables\nVideos:\n\nSimple linear regression: categorical predictor (slides)\nR Code for Categorical Predictors\n\n\nFile organization: Save this file in the ‚ÄúActivities‚Äù subfolder of your ‚ÄúSTAT155‚Äù folder.",
    "crumbs": [
      "Simple Linear Regression - Categorical Predictor"
    ]
  },
  {
    "objectID": "activities/07-slr-cat-predictor.html#exercise-1-get-to-know-the-data",
    "href": "activities/07-slr-cat-predictor.html#exercise-1-get-to-know-the-data",
    "title": "Simple Linear Regression - Categorical Predictor",
    "section": "Exercise 1: Get to know the data",
    "text": "Exercise 1: Get to know the data\nWrite R code to answer the following:\n\n\nHow many cases and variables do we have? What does a case represent?\n\n\nWhat do the first few rows of the data look like?\n\n\nConstruct and interpret two different visualizations of the price variable.\n\n\nConstruct and interpret a visualization of the cut variable.\n\n\n\ndim(diamonds)\n## [1] 53940    10\n\nhead(diamonds)\n## # A tibble: 6 √ó 10\n##   carat cut       color clarity depth table price     x     y     z\n##   &lt;dbl&gt; &lt;fct&gt;     &lt;fct&gt; &lt;fct&gt;   &lt;dbl&gt; &lt;dbl&gt; &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n## 1  0.23 Ideal     E     SI2      61.5    55   326  3.95  3.98  2.43\n## 2  0.21 Premium   E     SI1      59.8    61   326  3.89  3.84  2.31\n## 3  0.23 Good      E     VS1      56.9    65   327  4.05  4.07  2.31\n## 4  0.29 Premium   I     VS2      62.4    58   334  4.2   4.23  2.63\n## 5  0.31 Good      J     SI2      63.3    58   335  4.34  4.35  2.75\n## 6  0.24 Very Good J     VVS2     62.8    57   336  3.94  3.96  2.48\n\n# Visualize price (outcome variable)\nggplot(diamonds, aes(x = price)) +\n    geom_histogram()\n\n\n\n\n\n\n\nggplot(diamonds, aes(y = price)) +\n    geom_boxplot()\n\n\n\n\n\n\n\ndiamonds %&gt;%\n    summarize(mean(price), median(price), sd(price))\n## # A tibble: 1 √ó 3\n##   `mean(price)` `median(price)` `sd(price)`\n##           &lt;dbl&gt;           &lt;dbl&gt;       &lt;dbl&gt;\n## 1         3933.            2401       3989.\n\n# Visualize cut (predictor variable)\nggplot(diamonds, aes(x = cut)) +\n    geom_bar()\n\n\n\n\n\n\n\ndiamonds %&gt;% \n    count(cut)\n## # A tibble: 5 √ó 2\n##   cut           n\n##   &lt;fct&gt;     &lt;int&gt;\n## 1 Fair       1610\n## 2 Good       4906\n## 3 Very Good 12082\n## 4 Premium   13791\n## 5 Ideal     21551",
    "crumbs": [
      "Simple Linear Regression - Categorical Predictor"
    ]
  },
  {
    "objectID": "activities/07-slr-cat-predictor.html#exercise-2-visualizations",
    "href": "activities/07-slr-cat-predictor.html#exercise-2-visualizations",
    "title": "Simple Linear Regression - Categorical Predictor",
    "section": "Exercise 2: Visualizations",
    "text": "Exercise 2: Visualizations\nStart by visualizing this relationship of interest, that between price and cut.\n\nThe appropriate plot depends upon the type of variables we‚Äôre plotting. When exploring the relationship between a quantitative response and a quantitative predictor, a scatterplot was an effective choice. After running the code below, explain why a scatterplot is not effective for exploring the relationship between the outcome price and categorical cut predictor.\n\n\n# Try a scatterplot\nggplot(diamonds, aes(y = price, x = cut)) + \n    geom_point()\n\n\n\n\n\n\n\n\n\nResponse: Put your response here.\n\n\nSeparately run each chunk below, with two plots. Comment (#) on what changes in the code / output.\n\nb.1.\n\n# Univariate boxplot\nggplot(diamonds, aes(y = price)) + \n    geom_boxplot()\n\n\n\n\n\n\n\n\n\nResponse: Put your response here.\n\nb.2.\n\n# ???\nggplot(diamonds, aes(y = price, x = cut)) + \n    geom_boxplot()\n\n\n\n\n\n\n\n\n\nResponse: Put your response here.\n\nb.3.\n\n# Univariate density plot\nggplot(diamonds, aes(x = price)) + \n    geom_density()\n\n\n\n\n\n\n\n\n\nResponse: Put your response here.\n\nb.4.\n\n# Comparisons in density plot\nggplot(diamonds, aes(x = price, color = cut)) + \n    geom_density()\n\n\n\n\n\n\n\n\n\nResponse: Put your response here.\n\nb.5.\n\n# Univariate histogram\nggplot(diamonds, aes(x = price)) + \n    geom_histogram()\n\n\n\n\n\n\n\n\n\nResponse: Put your response here.\n\nb.6. What‚Äôs the difference between this and b4 density plot comparison? Can we now interpret the relationship between price and cut as we could in density plot? Why not?\n\n# ???\nggplot(diamonds, aes(x = price)) + \n    geom_histogram() + \n    facet_wrap(~ cut)\n\n\n\n\n\n\n\n\n\nResponse: Put your response here.\n\n\nDo you notice anything interesting about the relationship between price and cut? What do you think might be happening here?\n\n\nResponse: Put your response here.",
    "crumbs": [
      "Simple Linear Regression - Categorical Predictor"
    ]
  },
  {
    "objectID": "activities/07-slr-cat-predictor.html#exercise-3-numerical-summaries",
    "href": "activities/07-slr-cat-predictor.html#exercise-3-numerical-summaries",
    "title": "Simple Linear Regression - Categorical Predictor",
    "section": "Exercise 3: Numerical summaries",
    "text": "Exercise 3: Numerical summaries\nLet‚Äôs follow up our plots with some numerical summaries.\n\nTo warm up, first calculate the mean price across all diamonds.\n\n\ndiamonds %&gt;% \n     summarize(mean(price))\n## # A tibble: 1 √ó 1\n##   `mean(price)`\n##           &lt;dbl&gt;\n## 1         3933.\n\n\nTo summarize the trends we observed in the grouped plots above, we can calculate the mean price for each type of cut. This requires the inclusion of the group_by() function:\n\n\n# Calculate mean price by cut\ndiamonds %&gt;% \n    group_by(cut) %&gt;% \n    summarize(mean(price))\n## # A tibble: 5 √ó 2\n##   cut       `mean(price)`\n##   &lt;fct&gt;             &lt;dbl&gt;\n## 1 Fair              4359.\n## 2 Good              3929.\n## 3 Very Good         3982.\n## 4 Premium           4584.\n## 5 Ideal             3458.\n\n\nExamine the group mean measurements. can you match these numbers up with what you see in the plots?\nBased on the results above, we can see that, on average, diamonds with a ‚ÄúFair‚Äù cut tend to cost more than higher-quality cuts. Let‚Äôs construct a new variable named cutFair, using on the following criteria:\n\n\ncutFair = 1 if the diamond is of Fair cut\ncutFair = 0 otherwise (any other value of cut (Good, Very Good, Premium, Ideal))\n\nThe ifelse function allows to create a new variable from an existing one, based on whether or not the values in that variable meet a certain ‚Äúcondition‚Äù (remember, you can always look up function documentation in R by typing ?ifelse in the Console, and hitting enter!).\nFill in the following code to create cutFair. The condition was given to you already. Try to use this to complete the code.\n\n# In the first blank, put what value cutFair should have if the condition is \"met\", or TRUE\n# In the second blank, put what value cutFair should have if the condition is \"not met\", or FALSE\ndiamonds &lt;- diamonds %&gt;%\n  mutate(cutFair=ifelse(cut == \"Fair\", 1, 0))\n\nVariables like cutFair that are coded as 0/1 to numerically indicate if a categorical variable is at a particular state are known as an indicator variable. You will sometimes see these referred to as a ‚Äúbinary variable‚Äù or ‚Äúdichotomous variable‚Äù; you may also encounter the term ‚Äúdummy variable‚Äù in older statistical literature.\n\nNow, let‚Äôs calculate the group means based on the new cutFair indicator variable:\n\n\ndiamonds %&gt;% \n    group_by(cutFair) %&gt;% \n    summarize(mean(price))\n## # A tibble: 2 √ó 2\n##   cutFair `mean(price)`\n##     &lt;dbl&gt;         &lt;dbl&gt;\n## 1       0         3920.\n## 2       1         4359.",
    "crumbs": [
      "Simple Linear Regression - Categorical Predictor"
    ]
  },
  {
    "objectID": "activities/07-slr-cat-predictor.html#exercise-4-modeling-trend-using-a-categorical-predictor-with-exactly-2-categories",
    "href": "activities/07-slr-cat-predictor.html#exercise-4-modeling-trend-using-a-categorical-predictor-with-exactly-2-categories",
    "title": "Simple Linear Regression - Categorical Predictor",
    "section": "Exercise 4: Modeling trend using a categorical predictor with exactly 2 categories",
    "text": "Exercise 4: Modeling trend using a categorical predictor with exactly 2 categories\nNext, let‚Äôs model the trend in the relationship between the cutFair and price variables using a simple linear regression model:\n\n# Construct the model\ndiamond_mod0 &lt;- lm(price ~ cutFair, data = diamonds)\n\n# Summarize the model\nsummary(diamond_mod0)\n## \n## Call:\n## lm(formula = price ~ cutFair, data = diamonds)\n## \n## Residuals:\n##    Min     1Q Median     3Q    Max \n##  -4022  -2977  -1529   1391  14903 \n## \n## Coefficients:\n##             Estimate Std. Error t value Pr(&gt;|t|)    \n## (Intercept)  3919.69      17.44  224.80  &lt; 2e-16 ***\n## cutFair       439.06     100.93    4.35 1.36e-05 ***\n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n## \n## Residual standard error: 3989 on 53938 degrees of freedom\n## Multiple R-squared:  0.0003507,  Adjusted R-squared:  0.0003322 \n## F-statistic: 18.93 on 1 and 53938 DF,  p-value: 1.362e-05\n\nCompare these results to the output of exercise 3e. What do you notice? How do you interpret the intercept and cutFair coefficient terms from this model?",
    "crumbs": [
      "Simple Linear Regression - Categorical Predictor"
    ]
  },
  {
    "objectID": "activities/07-slr-cat-predictor.html#exercise-5-modeling-trend-using-a-categorical-predictor-with-2-categories",
    "href": "activities/07-slr-cat-predictor.html#exercise-5-modeling-trend-using-a-categorical-predictor-with-2-categories",
    "title": "Simple Linear Regression - Categorical Predictor",
    "section": "Exercise 5: Modeling trend using a categorical predictor with >2 categories",
    "text": "Exercise 5: Modeling trend using a categorical predictor with &gt;2 categories\nUsing a single binary predictor like the cutFair indicator variable is useful when there are two clearly delineated categories. However, the cut variable actually contains 5 categories! Because we‚Äôve collapsed all non-Fair classifications into a single category (i.e.¬†cutFair = 0), the model above can‚Äôt tell us anything about the difference in expected price between, say, Premium and Ideal cuts. The good news is that it is very straightforward to model categorical predictors with &gt;2 categories. We can do this by using the cut variable as our predictor:\n\n# Construct the model\ndiamond_mod &lt;- lm(price ~ cut, data = diamonds)\n\n# Summarize the model\ncoef(summary(diamond_mod))\n##               Estimate Std. Error   t value     Pr(&gt;|t|)\n## (Intercept)  4358.7578   98.78795 44.122361 0.000000e+00\n## cutGood      -429.8933  113.84940 -3.775982 1.595493e-04\n## cutVery Good -376.9979  105.16422 -3.584849 3.375707e-04\n## cutPremium    225.4999  104.39521  2.160060 3.077240e-02\n## cutIdeal     -901.2158  102.41155 -8.799943 1.408406e-18\n\n\nEven though we specified a single predictor variable in the model, we are seeing 4 coefficient estimates‚Äìwhy do you think this is the case?\n\nWe are seeing 4 coefficient estimates because each category is being assigned to a separate indicator variable‚ÄìcutGood = 1 when cut == \"Good\" and 0 otherwise, cutVery Good = 1 when `cut == ‚ÄúVery Good‚Äù and 0 otherwise, and so on.\nNOTE: We see 4 indicator variables (for Good, Very Good, Premium, and Ideal), but we do not see cutFair in the model output. This is because Fair is the reference level of the cut variable (it‚Äôs first alphabetically).\n\nAfter examining the summary table output from the code chunk above, complete the model formula:\n\n\n\nE[price | cut] = 4358.7578 - 429.8933 cutGood - 376.9979 cutVery Good + 225.4999 cutPremium - 901.2158 cutIdeal",
    "crumbs": [
      "Simple Linear Regression - Categorical Predictor"
    ]
  },
  {
    "objectID": "activities/07-slr-cat-predictor.html#exercise-6-making-sense-of-the-model",
    "href": "activities/07-slr-cat-predictor.html#exercise-6-making-sense-of-the-model",
    "title": "Simple Linear Regression - Categorical Predictor",
    "section": "Exercise 6: Making sense of the model",
    "text": "Exercise 6: Making sense of the model\nRecall our model: E[price | cut] = 4358.7578 - 429.8933 cutGood - 376.9979 cutVery Good + 225.4999 cutPremium - 901.2158 cutIdeal\n\nUse the model formula to calculate the expected/typical price for diamonds of Good cut.\n\n\nExpected/typical price for diamonds of Good cut:\n\nE[price | cut] = 4358.7578 - 429.8933 * 1 - 376.9979 * 0 + 225.4999 * 0 - 901.2158 * 0 = 4358.7578 - 429.8933 = $3928.865\n\npredict(diamond_mod, newdata = data.frame(cut = \"Good\"))\n##        1 \n## 3928.864\n\n\nSimilarly, calculate the expected/typical price for diamonds of Fair cut.\n\n\nExpected/typical price for diamonds of Fair cut:\n\nE[price | cut] = 4358.7578 - 429.8933 * 0 - 376.9979 * 0 + 225.4999 * 0 - 901.2158 * 0 = $4358.7578\n\npredict(diamond_mod, newdata = data.frame(cut = \"Fair\"))\n##        1 \n## 4358.758\n\n\nRe-examine these 2 calculations. Where have you seen these numbers before?!\n\nThese come from our group mean calculations in Exercise 3b! The predicted value for diamonds of Fair cut is also the same as what we obtained using the SLR model in exercise 4 with only a single cutFair indicator variable.",
    "crumbs": [
      "Simple Linear Regression - Categorical Predictor"
    ]
  },
  {
    "objectID": "activities/07-slr-cat-predictor.html#exercise-7-interpreting-coefficients",
    "href": "activities/07-slr-cat-predictor.html#exercise-7-interpreting-coefficients",
    "title": "Simple Linear Regression - Categorical Predictor",
    "section": "Exercise 7: Interpreting coefficients",
    "text": "Exercise 7: Interpreting coefficients\nRecall that our model formula is not a formula for a line. Thus we can‚Äôt interpret the coefficients as ‚Äúslopes‚Äù as we have before. Taking this into account and reflecting upon your calculations above‚Ä¶\n\nInterpret the intercept coefficient (4358.7578) in terms of the data context. Make sure to use non-causal language, include units, and talk about averages rather than individual cases.\n\n\nThe average price of a Fair cut diamonds is $4358.7578.\n\n\nInterpret the cutGood and cutVery Good coefficients (-429.8933 and -376.9979) in terms of the data context. Hint: where did you use these value in the prediction calculations above?\n\nInterpretation of cutGood coefficient: On average, Good cut diamonds are worth $429.89 less than Fair cut diamonds.\nInterpretation of cutVery Good coefficient: On average, Very Good cut diamonds are worth $377.00 less than Fair cut diamonds.",
    "crumbs": [
      "Simple Linear Regression - Categorical Predictor"
    ]
  },
  {
    "objectID": "activities/07-slr-cat-predictor.html#exercise-8-modeling-choices-challenge",
    "href": "activities/07-slr-cat-predictor.html#exercise-8-modeling-choices-challenge",
    "title": "Simple Linear Regression - Categorical Predictor",
    "section": "Exercise 8: Modeling choices (CHALLENGE)",
    "text": "Exercise 8: Modeling choices (CHALLENGE)\nWhy do we fit this model in this way (using 4 indicator variables cutGood, cutVery Good, cutPremium, cutIdeal)? Instead, suppose that we created a single variable cutCat that gave each category a numerical value: 0 for Fair, 1 for Good, 2 for Very Good, 3 for Premium, and 4 for Ideal.\nHow would this change things? What are the pros and cons of each approach?",
    "crumbs": [
      "Simple Linear Regression - Categorical Predictor"
    ]
  },
  {
    "objectID": "activities/07-slr-cat-predictor.html#render-your-work",
    "href": "activities/07-slr-cat-predictor.html#render-your-work",
    "title": "Simple Linear Regression - Categorical Predictor",
    "section": "Render your work",
    "text": "Render your work\n\nClick the ‚ÄúRender‚Äù button in the menu bar for this pane (blue arrow pointing right). This will create an HTML file containing all of the directions, code, and responses from this activity. A preview of the HTML will appear in the browser.\nScroll through and inspect the document to check that your work translated to the HTML format correctly.\nClose the browser tab.\nGo to the ‚ÄúBackground Jobs‚Äù pane in RStudio and click the Stop button to end the rendering process.\nNavigate to your ‚ÄúActivities‚Äù subfolder within your ‚ÄúSTAT155‚Äù folder and locate the HTML file. You can open it again in your browser to double check.",
    "crumbs": [
      "Simple Linear Regression - Categorical Predictor"
    ]
  },
  {
    "objectID": "activities/07-slr-cat-predictor.html#exercise-9-the-least-squares-criterion",
    "href": "activities/07-slr-cat-predictor.html#exercise-9-the-least-squares-criterion",
    "title": "Simple Linear Regression - Categorical Predictor",
    "section": "Exercise 9: The least squares criterion",
    "text": "Exercise 9: The least squares criterion\nThe coefficient estimates diamond_mod were selected in the same way as when our predictor is quantitative: by minimizing the sum of the squared residuals. Use this model to calculate the residual for the first diamond in the dataset:\n\n# Observed data on the first diamond\ndiamonds %&gt;% \n  select(price, cut) %&gt;% \n  head(1)\n## # A tibble: 1 √ó 2\n##   price cut  \n##   &lt;int&gt; &lt;fct&gt;\n## 1   326 Ideal",
    "crumbs": [
      "Simple Linear Regression - Categorical Predictor"
    ]
  },
  {
    "objectID": "activities/07-slr-cat-predictor.html#exercise-10-diamond-color",
    "href": "activities/07-slr-cat-predictor.html#exercise-10-diamond-color",
    "title": "Simple Linear Regression - Categorical Predictor",
    "section": "Exercise 10: Diamond color",
    "text": "Exercise 10: Diamond color\nConsider modeling price by color.\n\nBefore creating a visualization that shows the relationship between price and color, write down what you expect the plot to look like. Then construct and interpret an appropriate plot.\nCompute the average price for each color.\nFit an appropriate linear model with lm() and display a short summary of the model.\nWrite out the model formula from the above summary.\nWhich color is the reference level? How can you tell from the model summary?\nInterpret the intercept and two other coefficients from the model in terms of the data context.",
    "crumbs": [
      "Simple Linear Regression - Categorical Predictor"
    ]
  },
  {
    "objectID": "activities/07-slr-cat-predictor.html#exercise-11-diamond-clarity",
    "href": "activities/07-slr-cat-predictor.html#exercise-11-diamond-clarity",
    "title": "Simple Linear Regression - Categorical Predictor",
    "section": "Exercise 11: Diamond clarity",
    "text": "Exercise 11: Diamond clarity\nRepeat the steps from the previous exercise for the clarity variable.",
    "crumbs": [
      "Simple Linear Regression - Categorical Predictor"
    ]
  },
  {
    "objectID": "activities/07-slr-cat-predictor.html#exercise-12-evaluating-model-strength",
    "href": "activities/07-slr-cat-predictor.html#exercise-12-evaluating-model-strength",
    "title": "Simple Linear Regression - Categorical Predictor",
    "section": "Exercise 12: Evaluating model strength",
    "text": "Exercise 12: Evaluating model strength\nLet‚Äôs study some penguin data!\n\ndata(penguins)\npenguins &lt;- penguins %&gt;% \n  filter(!is.na(sex), !is.na(species))\n\nWe‚Äôll focus on 3 variables with the goal of predicting flipper_len:\n\nflipper_len = the length of the penguin‚Äôs flippers (arms) in mm\nspecies = Adelie, Chinstrap, or Gentoo\nsex = female, male\n\nAnd build 2 models of flipper_len:\n\n# Model of flipper_len by sex\nflipper_model_1 &lt;- lm(flipper_len ~ sex, data = penguins)\nflipper_model_2 &lt;- lm(flipper_len ~ species, data = penguins)\n\nJust as with models that use quantitative predictors, it‚Äôs important to evaluate our (eventual) models of flipper_len by species and sex: are they correct? strong? fair?\nLet‚Äôs start with strength. How strong are these models? What‚Äôs the best predictor? Let‚Äôs explore.\n\nBased on the boxplots below, what is the stronger predictor of flipper_len: sex or species?\n\n\npenguins %&gt;% \n  ggplot(aes(y = flipper_len, x = sex)) + \n  geom_boxplot()\n\n\n\n\n\n\n\n\n\npenguins %&gt;% \n  ggplot(aes(y = flipper_len, x = species)) + \n  geom_boxplot()\n\n\n\n\n\n\n\n\n\nWe used 2 approaches to measuring the strength of the simple linear regression models in previous activities: correlation and R-squared. Unfortunately, we get errors when we try to calculate the correlation between flipper_len and sex, and between flipper_len and species. Why?\n\n\npenguins %&gt;% \n  summarize(cor(sex, flipper_len))\n## Error in `summarize()`:\n## ‚Ñπ In argument: `cor(sex, flipper_len)`.\n## Caused by error in `cor()`:\n## ! 'x' must be numeric\npenguins %&gt;% \n  summarize(cor(species, flipper_len))\n## Error in `summarize()`:\n## ‚Ñπ In argument: `cor(species, flipper_len)`.\n## Caused by error in `cor()`:\n## ! 'x' must be numeric\n\n\nLuckily, R-squared works no matter whether a predictor is quantitative or categorical! Interpret and compare the R-squared values for our separate models of flipper_len by sex and species. Which is the stronger predictor? Does this match your answer in part a?\n\n\n# Model of flipper_len by sex\nsummary(flipper_model_1)\n## \n## Call:\n## lm(formula = flipper_len ~ sex, data = penguins)\n## \n## Residuals:\n##     Min      1Q  Median      3Q     Max \n## -26.506 -10.364  -4.364  12.636  26.494 \n## \n## Coefficients:\n##             Estimate Std. Error t value Pr(&gt;|t|)    \n## (Intercept)  197.364      1.057 186.792  &lt; 2e-16 ***\n## sexmale        7.142      1.488   4.801 2.39e-06 ***\n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n## \n## Residual standard error: 13.57 on 331 degrees of freedom\n## Multiple R-squared:  0.06511,    Adjusted R-squared:  0.06229 \n## F-statistic: 23.05 on 1 and 331 DF,  p-value: 2.391e-06\n\n# Model of flipper_len by species\nsummary(flipper_model_2)\n## \n## Call:\n## lm(formula = flipper_len ~ species, data = penguins)\n## \n## Residuals:\n##      Min       1Q   Median       3Q      Max \n## -18.1027  -4.8235  -0.1027   4.7647  19.8973 \n## \n## Coefficients:\n##                  Estimate Std. Error t value Pr(&gt;|t|)    \n## (Intercept)      190.1027     0.5522  344.25  &lt; 2e-16 ***\n## speciesChinstrap   5.7208     0.9796    5.84 1.25e-08 ***\n## speciesGentoo     27.1326     0.8241   32.92  &lt; 2e-16 ***\n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n## \n## Residual standard error: 6.673 on 330 degrees of freedom\n## Multiple R-squared:  0.7747, Adjusted R-squared:  0.7734 \n## F-statistic: 567.4 on 2 and 330 DF,  p-value: &lt; 2.2e-16",
    "crumbs": [
      "Simple Linear Regression - Categorical Predictor"
    ]
  },
  {
    "objectID": "activities/07-slr-cat-predictor.html#exercise-13-evaluating-model-correctness",
    "href": "activities/07-slr-cat-predictor.html#exercise-13-evaluating-model-correctness",
    "title": "Simple Linear Regression - Categorical Predictor",
    "section": "Exercise 13: Evaluating model correctness",
    "text": "Exercise 13: Evaluating model correctness\nLet‚Äôs just focus on the stronger of our 2 models, that of flipper_len by species (flipper_model_2), and ask: is it correct (not wrong)? Recall that when our predictor was quantitative, residual plots provided some insight. Check out the residual plots below. They look a little goofy! Explain the goofiness (what‚Äôs happening here) and describe what you learn about the model‚Äôs ‚Äúcorrectness‚Äù. Is the model ‚Äúcorrect‚Äù?\n\n# Residual plot\nflipper_model_2 %&gt;% \n  ggplot(aes(x = .fitted, y = .resid)) + \n  geom_point() + \n  geom_hline(yintercept = 0)\n\n\n\n\n\n\n\n\n\n# Residual plot using boxes!\nflipper_model_2 %&gt;% \n  ggplot(aes(x = .fitted, y = .resid, group = .fitted)) + \n  geom_boxplot() + \n  geom_hline(yintercept = 0)",
    "crumbs": [
      "Simple Linear Regression - Categorical Predictor"
    ]
  },
  {
    "objectID": "activities/07-slr-cat-predictor.html#exercise-14-really-understand-how-the-coefficients-work",
    "href": "activities/07-slr-cat-predictor.html#exercise-14-really-understand-how-the-coefficients-work",
    "title": "Simple Linear Regression - Categorical Predictor",
    "section": "Exercise 14: Really understand how the coefficients work",
    "text": "Exercise 14: Really understand how the coefficients work\nNext, consider the model of flipper_len by island. The average flipper_len of penguins on each island is calculated below:\n\npenguins %&gt;% \n  group_by(island) %&gt;% \n  summarize(mean(flipper_len))\n## # A tibble: 3 √ó 2\n##   island    `mean(flipper_len)`\n##   &lt;fct&gt;                   &lt;dbl&gt;\n## 1 Biscoe                   210.\n## 2 Dream                    193.\n## 3 Torgersen                192.\n\n\nUsing just the above averages and your understanding of categorical predictors, fill in the model coefficients and indicator variables below. Do NOT use lm() yet!!!\n\nE[flipper_len | island] = ___ +/- ___ island??? +/- ___ island???\n\nCheck your work to Part a.\n\n\nflipper_model_3 &lt;- lm(flipper_len ~ island, penguins)\nsummary(flipper_model_3)\n## \n## Call:\n## lm(formula = flipper_len ~ island, data = penguins)\n## \n## Residuals:\n##     Min      1Q  Median      3Q     Max \n## -37.558  -5.532   1.468   7.442  21.442 \n## \n## Coefficients:\n##                 Estimate Std. Error t value Pr(&gt;|t|)    \n## (Intercept)      209.558      0.879 238.410   &lt;2e-16 ***\n## islandDream      -16.371      1.340 -12.214   &lt;2e-16 ***\n## islandTorgersen  -18.026      1.858  -9.702   &lt;2e-16 ***\n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n## \n## Residual standard error: 11.22 on 330 degrees of freedom\n## Multiple R-squared:  0.3628, Adjusted R-squared:  0.3589 \n## F-statistic: 93.94 on 2 and 330 DF,  p-value: &lt; 2.2e-16",
    "crumbs": [
      "Simple Linear Regression - Categorical Predictor"
    ]
  },
  {
    "objectID": "activities/07-slr-cat-predictor.html#exercise-1-get-to-know-the-data-1",
    "href": "activities/07-slr-cat-predictor.html#exercise-1-get-to-know-the-data-1",
    "title": "Simple Linear Regression - Categorical Predictor",
    "section": "Exercise 1: Get to know the data",
    "text": "Exercise 1: Get to know the data\n\nA case represents a single diamond.\nThe distribution of price is right skewed with considerable high outliers. The right skew is evidenced by the mean price ($3932) being much higher than the median price ($2401).\nMost diamonds in this data are of Good cut or better. Ideal cut diamonds are the most common with each succesive grade being the next most common.\n\n\ndim(diamonds)\n## [1] 53940    11\n\nhead(diamonds)\n## # A tibble: 6 √ó 11\n##   carat cut       color clarity depth table price     x     y     z cutFair\n##   &lt;dbl&gt; &lt;fct&gt;     &lt;fct&gt; &lt;fct&gt;   &lt;dbl&gt; &lt;dbl&gt; &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;   &lt;dbl&gt;\n## 1  0.23 Ideal     E     SI2      61.5    55   326  3.95  3.98  2.43       0\n## 2  0.21 Premium   E     SI1      59.8    61   326  3.89  3.84  2.31       0\n## 3  0.23 Good      E     VS1      56.9    65   327  4.05  4.07  2.31       0\n## 4  0.29 Premium   I     VS2      62.4    58   334  4.2   4.23  2.63       0\n## 5  0.31 Good      J     SI2      63.3    58   335  4.34  4.35  2.75       0\n## 6  0.24 Very Good J     VVS2     62.8    57   336  3.94  3.96  2.48       0\n\n# Visualize price (outcome variable)\nggplot(diamonds, aes(x = price)) +\n    geom_histogram()\n\n\n\n\n\n\n\nggplot(diamonds, aes(y = price)) +\n    geom_boxplot()\n\n\n\n\n\n\n\ndiamonds %&gt;%\n    summarize(mean(price), median(price), sd(price))\n## # A tibble: 1 √ó 3\n##   `mean(price)` `median(price)` `sd(price)`\n##           &lt;dbl&gt;           &lt;dbl&gt;       &lt;dbl&gt;\n## 1         3933.            2401       3989.\n\n# Visualize cut (predictor variable)\nggplot(diamonds, aes(x = cut)) +\n    geom_bar()\n\n\n\n\n\n\n\ndiamonds %&gt;% \n    count(cut)\n## # A tibble: 5 √ó 2\n##   cut           n\n##   &lt;fct&gt;     &lt;int&gt;\n## 1 Fair       1610\n## 2 Good       4906\n## 3 Very Good 12082\n## 4 Premium   13791\n## 5 Ideal     21551",
    "crumbs": [
      "Simple Linear Regression - Categorical Predictor"
    ]
  },
  {
    "objectID": "activities/07-slr-cat-predictor.html#exercise-2-visualizations-1",
    "href": "activities/07-slr-cat-predictor.html#exercise-2-visualizations-1",
    "title": "Simple Linear Regression - Categorical Predictor",
    "section": "Exercise 2: Visualizations",
    "text": "Exercise 2: Visualizations\nStart by visualizing this relationship of interest, that between price and cut.\n\nWe just don‚Äôt see anything clearly on a scatterplot. With the small number of unique values of the predictor variable, all of the points are bunched up on each other.\n\n\n# Try a scatterplot\nggplot(diamonds, aes(y = price, x = cut)) + \n    geom_point()\n\n\n\n\n\n\n\n\n\nSeparately run each chunk below, with two plots. Comment (#) on what changes in the code / output.\n\n\n# Univariate boxplot\nggplot(diamonds, aes(y = price)) + \n    geom_boxplot()\n\n\n\n\n\n\n\n\n\n# Separate boxes by category\nggplot(diamonds, aes(y = price, x = cut)) + \n    geom_boxplot()\n\n\n\n\n\n\n\n\n\n# Univariate density plot\nggplot(diamonds, aes(x = price)) + \n    geom_density()\n\n\n\n\n\n\n\n\n\n# Separate density plots by category\nggplot(diamonds, aes(x = price, color = cut)) + \n    geom_density()\n\n\n\n\n\n\n\n\n\n# Univariate histogram\nggplot(diamonds, aes(x = price)) + \n    geom_histogram()\n\n\n\n\n\n\n\n\n\n# Separate histograms by category\nggplot(diamonds, aes(x = price)) + \n    geom_histogram() + \n    facet_wrap(~ cut)\n\n\n\n\n\n\n\n\n\nThe relationship between price and cut seems to be opposite what we would expect. The diamonds with the best cut (Ideal) have the lowest average price, and the ones with the worst cut (Fair) are woth the most. Maybe something else is different between the diamonds with the best and worst cuts‚Ä¶size maybe?",
    "crumbs": [
      "Simple Linear Regression - Categorical Predictor"
    ]
  },
  {
    "objectID": "activities/07-slr-cat-predictor.html#exercise-3-numerical-summaries-1",
    "href": "activities/07-slr-cat-predictor.html#exercise-3-numerical-summaries-1",
    "title": "Simple Linear Regression - Categorical Predictor",
    "section": "Exercise 3: Numerical summaries",
    "text": "Exercise 3: Numerical summaries\nLet‚Äôs follow up our plots with some numerical summaries.\n\nMean price across all diamonds:\n\n\ndiamonds %&gt;% \n    summarize(mean(price))\n## # A tibble: 1 √ó 1\n##   `mean(price)`\n##           &lt;dbl&gt;\n## 1         3933.\n\n\nMean price for each type of cut:\n\n\ndiamonds %&gt;% \n    group_by(cut) %&gt;% \n    summarize(mean(price))\n## # A tibble: 5 √ó 2\n##   cut       `mean(price)`\n##   &lt;fct&gt;             &lt;dbl&gt;\n## 1 Fair              4359.\n## 2 Good              3929.\n## 3 Very Good         3982.\n## 4 Premium           4584.\n## 5 Ideal             3458.\n\n\nGroup means should reflect what you see in the plots (easiest to see in the boxplots)\nCreate our new cutFair variable:\n\n\ndiamonds &lt;- diamonds %&gt;%\n  mutate(cutFair=ifelse(cut == \"Fair\", 1, 0))\n\n\nCalculate the group means based on this new variable\n\n\ndiamonds %&gt;% \n    group_by(cutFair) %&gt;% \n    summarize(mean(price))\n## # A tibble: 2 √ó 2\n##   cutFair `mean(price)`\n##     &lt;dbl&gt;         &lt;dbl&gt;\n## 1       0         3920.\n## 2       1         4359.",
    "crumbs": [
      "Simple Linear Regression - Categorical Predictor"
    ]
  },
  {
    "objectID": "activities/07-slr-cat-predictor.html#exercise-4-modeling-trend-using-a-categorical-predictor-with-exactly-2-categories-1",
    "href": "activities/07-slr-cat-predictor.html#exercise-4-modeling-trend-using-a-categorical-predictor-with-exactly-2-categories-1",
    "title": "Simple Linear Regression - Categorical Predictor",
    "section": "Exercise 4: Modeling trend using a categorical predictor with exactly 2 categories",
    "text": "Exercise 4: Modeling trend using a categorical predictor with exactly 2 categories\n\n# Construct the model\ndiamond_mod0 &lt;- lm(price ~ cutFair, data = diamonds)\n\n# Summarize the model\ncoef(summary(diamond_mod0))\n##              Estimate Std. Error    t value     Pr(&gt;|t|)\n## (Intercept) 3919.6946    17.4367 224.795616 0.000000e+00\n## cutFair      439.0632   100.9269   4.350309 1.361951e-05\n\nThe intercept is the expected value (mean) of the price for all diamonds with a cut quality that isn‚Äôt Fair (Good, Very Good, Premium, or Ideal, i.e.¬†when cutFair = 0)‚Äìthe same as we saw in exercise 3e.\n\nWhen we add the intercept and coefficient for cutFair, we get 3919.69 + 439.06 = 4358.75‚Äìthis is the mean price for all diamonds with a Fair cut quality that we saw in exercise 3e! Therefore, the coefficient of cutFair (439.06) is interpreted as the difference between the mean value of diamonds with a Fair cut quality and the mean value of diamonds with a higher cut quality.",
    "crumbs": [
      "Simple Linear Regression - Categorical Predictor"
    ]
  },
  {
    "objectID": "activities/07-slr-cat-predictor.html#exercise-5-modeling-trend-using-a-categorical-predictor-with-2-categories-1",
    "href": "activities/07-slr-cat-predictor.html#exercise-5-modeling-trend-using-a-categorical-predictor-with-2-categories-1",
    "title": "Simple Linear Regression - Categorical Predictor",
    "section": "Exercise 5: Modeling trend using a categorical predictor with >2 categories",
    "text": "Exercise 5: Modeling trend using a categorical predictor with &gt;2 categories\n\n# Construct the model\ndiamond_mod &lt;- lm(price ~ cut, data = diamonds)\n\n# Summarize the model\ncoef(summary(diamond_mod))\n##               Estimate Std. Error   t value     Pr(&gt;|t|)\n## (Intercept)  4358.7578   98.78795 44.122361 0.000000e+00\n## cutGood      -429.8933  113.84940 -3.775982 1.595493e-04\n## cutVery Good -376.9979  105.16422 -3.584849 3.375707e-04\n## cutPremium    225.4999  104.39521  2.160060 3.077240e-02\n## cutIdeal     -901.2158  102.41155 -8.799943 1.408406e-18\n\n\nWe are seeing 4 coefficient estimates because each category is being assigned to a separate indicator variable‚ÄìcutGood = 1 when cut == \"Good\" and 0 otherwise, cutVery Good = 1 when `cut == ‚ÄúVery Good‚Äù and 0 otherwise, and so on.\nE[price | cut] = 4358.7578 - 429.8933 cutGood - 376.9979 cutVery Good + 225.4999 cutPremium - 901.2158 cutIdeal",
    "crumbs": [
      "Simple Linear Regression - Categorical Predictor"
    ]
  },
  {
    "objectID": "activities/07-slr-cat-predictor.html#exercise-6-making-sense-of-the-model-1",
    "href": "activities/07-slr-cat-predictor.html#exercise-6-making-sense-of-the-model-1",
    "title": "Simple Linear Regression - Categorical Predictor",
    "section": "Exercise 6: Making sense of the model",
    "text": "Exercise 6: Making sense of the model\n\nExpected/typical price for diamonds of Good cut:\n\nE[price | cut] = 4358.7578 - 429.8933 * 1 - 376.9979 * 0 + 225.4999 * 0 - 901.2158 * 0 = 4358.7578 - 429.8933 = $3928.865\n\npredict(diamond_mod, newdata = data.frame(cut = \"Good\"))\n##        1 \n## 3928.864\n\n\nExpected/typical price for diamonds of Fair cut:\n\nE[price | cut] = 4358.7578 - 429.8933 * 0 - 376.9979 * 0 + 225.4999 * 0 - 901.2158 * 0 = $4358.7578\n\npredict(diamond_mod, newdata = data.frame(cut = \"Fair\"))\n##        1 \n## 4358.758\n\n\nThese come from our group mean calculations in Exercise 3b! The predicted value for diamonds of Fair cut is also the same as what we obtained using the SLR model in exercise 4 with only a single cutFair indicator variable.",
    "crumbs": [
      "Simple Linear Regression - Categorical Predictor"
    ]
  },
  {
    "objectID": "activities/07-slr-cat-predictor.html#exercise-7-interpreting-coefficients-1",
    "href": "activities/07-slr-cat-predictor.html#exercise-7-interpreting-coefficients-1",
    "title": "Simple Linear Regression - Categorical Predictor",
    "section": "Exercise 7: Interpreting coefficients",
    "text": "Exercise 7: Interpreting coefficients\nRecall that our model formula is not a formula for a line. Thus we can‚Äôt interpret the coefficients as ‚Äúslopes‚Äù as we have before. Taking this into account and reflecting upon your calculations above‚Ä¶\n\nThe average price of a Fair cut diamonds is $4358.7578.\n\nInterpretation of cutGood coefficient: On average, Good cut diamonds are worth $429.89 less than Fair cut diamonds.\nInterpretation of cutVery Good coefficient: On average, Very Good cut diamonds are worth $377.00 less than Fair cut diamonds.",
    "crumbs": [
      "Simple Linear Regression - Categorical Predictor"
    ]
  },
  {
    "objectID": "activities/07-slr-cat-predictor.html#exercise-8-modeling-choices-challenge-1",
    "href": "activities/07-slr-cat-predictor.html#exercise-8-modeling-choices-challenge-1",
    "title": "Simple Linear Regression - Categorical Predictor",
    "section": "Exercise 8: Modeling choices (CHALLENGE)",
    "text": "Exercise 8: Modeling choices (CHALLENGE)\nWhy do we fit this model in this way (using 4 indicator variables cutGood, cutVery Good, cutPremium, cutIdeal)? Instead, suppose that we created a single variable cutCat that gave each category a numerical value: 0 for Fair, 1 for Good, 2 for Very Good, 3 for Premium, and 4 for Ideal.\n\nIf we used 0-4 instead of creating indicator variables, we would be constraining the change from 0 to 1, from 1 to 2, etc. to always be of the same magnitude. That is, a 1 unit change in the cut variable would always have the same change in price in our model.\nUsing separate indicator variables allows the difference between subsequent categories to be different, which allows our model to be a bit more nuanced. It is possible to take nuance too far though. For example, in our previous investigations of bikeshare data, we modeled ridership versus temperature. We treated temperature as a quantitative predictor. Imagine if we had created an indicator variable for each unique temperature in the data‚Äîthat would be so many variables! Having so many variables creates a very complex model which can be hard to make sense of. (These ideas are addressed further in STAT 253: Statistical Machine Learning!)",
    "crumbs": [
      "Simple Linear Regression - Categorical Predictor"
    ]
  },
  {
    "objectID": "activities/07-slr-cat-predictor.html#exercise-9-the-least-squares-criterion-1",
    "href": "activities/07-slr-cat-predictor.html#exercise-9-the-least-squares-criterion-1",
    "title": "Simple Linear Regression - Categorical Predictor",
    "section": "Exercise 9: The least squares criterion",
    "text": "Exercise 9: The least squares criterion\n\n# Observed price = 326\ndiamonds %&gt;% \n  select(price, cut) %&gt;% \n  head(1)\n## # A tibble: 1 √ó 2\n##   price cut  \n##   &lt;int&gt; &lt;fct&gt;\n## 1   326 Ideal\n\n# Predicted price = 3457.542\npredict(diamond_mod, newdata = data.frame(cut = \"Ideal\"))\n##        1 \n## 3457.542\n\n# Residual\n326 - 3457.542\n## [1] -3131.542",
    "crumbs": [
      "Simple Linear Regression - Categorical Predictor"
    ]
  },
  {
    "objectID": "activities/07-slr-cat-predictor.html#exercise-10-diamond-color-1",
    "href": "activities/07-slr-cat-predictor.html#exercise-10-diamond-color-1",
    "title": "Simple Linear Regression - Categorical Predictor",
    "section": "Exercise 10: Diamond color",
    "text": "Exercise 10: Diamond color\nConsider modeling price by color.\n\nThe best color diamonds are J, and worst are D. We would expect D diamonds to have the lowest price and increase steadily as we get to J. This is in fact what we see in the boxplots.\n\n\nggplot(diamonds, aes(x = color, y = price)) +\n    geom_boxplot()\n\n\n\n\n\n\n\n\ndiamonds %&gt;% \n    group_by(color) %&gt;% \n    summarize(mean(price))\n## # A tibble: 7 √ó 2\n##   color `mean(price)`\n##   &lt;fct&gt;         &lt;dbl&gt;\n## 1 D             3170.\n## 2 E             3077.\n## 3 F             3725.\n## 4 G             3999.\n## 5 H             4487.\n## 6 I             5092.\n## 7 J             5324.\n\n\nWe fit a linear model and obtain the model formula: E[price | color] = 3169.95 - 93.20 colorE + 554.93 colorF + 829.18 colorG + 1316.72 colorH + 1921.92 colorI + 2153.86 colorJ\n\n\ndiamond_mod2 &lt;- lm(price ~ color, data = diamonds)\n\ncoef(summary(diamond_mod2))\n##               Estimate Std. Error   t value      Pr(&gt;|t|)\n## (Intercept) 3169.95410   47.70694 66.446391  0.000000e+00\n## colorE       -93.20162   62.04724 -1.502107  1.330752e-01\n## colorF       554.93230   62.38527  8.895246  6.004834e-19\n## colorG       829.18158   60.34470 13.740751  6.836340e-43\n## colorH      1316.71510   64.28715 20.481777  7.074714e-93\n## colorI      1921.92086   71.55308 26.860072 7.078041e-158\n## colorJ      2153.86392   88.13203 24.439060 3.414906e-131\n\n\nColor D is the reference level because we don‚Äôt see its indicator variable in the model output.\nInterpretation of the intercept: Diamonds with D color cost $3169.95 on average.\nInterpretation of the colorE coefficient: Diamonds with E color cost $93.20 less than D color diamonds on average.\nInterpretation of the colorF coefficient: Diamonds with F color cost $554.93 more than D color diamonds on average.",
    "crumbs": [
      "Simple Linear Regression - Categorical Predictor"
    ]
  },
  {
    "objectID": "activities/07-slr-cat-predictor.html#exercise-11-diamond-clarity-1",
    "href": "activities/07-slr-cat-predictor.html#exercise-11-diamond-clarity-1",
    "title": "Simple Linear Regression - Categorical Predictor",
    "section": "Exercise 11: Diamond clarity",
    "text": "Exercise 11: Diamond clarity\nWe see the unexpected result that diamonds of better clarity (VS1 and higher) have lower average prices. In fact the best clarity diamonds (VVS1 and IF) have the lowest average prices. What might be going on? What if the most clear diamonds were also quite small‚Ä¶\n\nggplot(diamonds, aes(x = clarity, y = price)) +\n    geom_boxplot()\n\n\n\n\n\n\n\n\ndiamonds %&gt;% \n    group_by(clarity) %&gt;% \n    summarize(mean(price))\n## # A tibble: 8 √ó 2\n##   clarity `mean(price)`\n##   &lt;fct&gt;           &lt;dbl&gt;\n## 1 I1              3924.\n## 2 SI2             5063.\n## 3 SI1             3996.\n## 4 VS2             3925.\n## 5 VS1             3839.\n## 6 VVS2            3284.\n## 7 VVS1            2523.\n## 8 IF              2865.\n\ndiamond_mod3 &lt;- lm(price ~ clarity, data = diamonds)\n\ncoef(summary(diamond_mod3))\n##                  Estimate Std. Error      t value      Pr(&gt;|t|)\n## (Intercept)  3924.1686910   144.5619 27.145247517 3.513547e-161\n## claritySI2   1138.8599147   150.2746  7.578526239  3.550711e-14\n## claritySI1     71.8324571   148.6049  0.483378837  6.288287e-01\n## clarityVS2      0.8207037   148.8672  0.005512992  9.956013e-01\n## clarityVS1    -84.7132999   150.9746 -0.561109670  5.747251e-01\n## clarityVVS2  -640.4316203   154.7737 -4.137858008  3.510944e-05\n## clarityVVS1 -1401.0540535   158.5401 -8.837224284  1.010097e-18\n## clarityIF   -1059.3295848   171.8990 -6.162510636  7.210567e-10",
    "crumbs": [
      "Simple Linear Regression - Categorical Predictor"
    ]
  },
  {
    "objectID": "activities/07-slr-cat-predictor.html#exercise-12-evaluating-model-strength-1",
    "href": "activities/07-slr-cat-predictor.html#exercise-12-evaluating-model-strength-1",
    "title": "Simple Linear Regression - Categorical Predictor",
    "section": "Exercise 12: Evaluating model strength",
    "text": "Exercise 12: Evaluating model strength\n\nflipper_model_1 &lt;- lm(flipper_len ~ sex, data = penguins)\nflipper_model_2 &lt;- lm(flipper_len ~ species, data = penguins)\n\n\nPart a\nspecies appears to be the stronger predictor - the flipper_len values are more distinct between species (there‚Äôs less overlap in the boxes)\n\ndata(penguins)\npenguins &lt;- penguins %&gt;% \n  filter(!is.na(sex), !is.na(species))\n\npenguins %&gt;% \n  ggplot(aes(y = flipper_len, x = sex)) + \n  geom_boxplot()\n\n\n\n\n\n\n\n\npenguins %&gt;% \n  ggplot(aes(y = flipper_len, x = species)) + \n  geom_boxplot()\n\n\n\n\n\n\n\n\n\n\nPart b\nCorrelation only works when both variables in the pair are quantitative, but sex and species are categorical.\n\n\nPart c\nUsing sex: R-squared = 0.06511\nUsing species: R-squared = 0.7782. Thus species is the stronger predictor of flipper_len. It explains roughly 78% of the variability in flipper length from penguin to penguin.\n\nsummary(flipper_model_1)\n## \n## Call:\n## lm(formula = flipper_len ~ sex, data = penguins)\n## \n## Residuals:\n##     Min      1Q  Median      3Q     Max \n## -26.506 -10.364  -4.364  12.636  26.494 \n## \n## Coefficients:\n##             Estimate Std. Error t value Pr(&gt;|t|)    \n## (Intercept)  197.364      1.057 186.792  &lt; 2e-16 ***\n## sexmale        7.142      1.488   4.801 2.39e-06 ***\n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n## \n## Residual standard error: 13.57 on 331 degrees of freedom\n## Multiple R-squared:  0.06511,    Adjusted R-squared:  0.06229 \n## F-statistic: 23.05 on 1 and 331 DF,  p-value: 2.391e-06\nsummary(flipper_model_2)\n## \n## Call:\n## lm(formula = flipper_len ~ species, data = penguins)\n## \n## Residuals:\n##      Min       1Q   Median       3Q      Max \n## -18.1027  -4.8235  -0.1027   4.7647  19.8973 \n## \n## Coefficients:\n##                  Estimate Std. Error t value Pr(&gt;|t|)    \n## (Intercept)      190.1027     0.5522  344.25  &lt; 2e-16 ***\n## speciesChinstrap   5.7208     0.9796    5.84 1.25e-08 ***\n## speciesGentoo     27.1326     0.8241   32.92  &lt; 2e-16 ***\n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n## \n## Residual standard error: 6.673 on 330 degrees of freedom\n## Multiple R-squared:  0.7747, Adjusted R-squared:  0.7734 \n## F-statistic: 567.4 on 2 and 330 DF,  p-value: &lt; 2.2e-16",
    "crumbs": [
      "Simple Linear Regression - Categorical Predictor"
    ]
  },
  {
    "objectID": "activities/07-slr-cat-predictor.html#exercise-13-evaluating-model-correctness-1",
    "href": "activities/07-slr-cat-predictor.html#exercise-13-evaluating-model-correctness-1",
    "title": "Simple Linear Regression - Categorical Predictor",
    "section": "Exercise 13: Evaluating model correctness",
    "text": "Exercise 13: Evaluating model correctness\nSince there are only 3 different species, there are only 3 possible predictions of flipper_len in this model (one per species). Thus each of the 3 groups on the x-axis corresponds to a species. Within each species, the observed flipper_len deviates from the species-based prediction, thus so too do the residuals. That‚Äôs why the points in each group have different y coordinates.\nOverall, our model doesn‚Äôt seem too wrong! No matter the x value (species!), the residuals are normally balanced above and below 0 with roughly equal variance in each species.\n\n# Residual plot\nflipper_model_2 %&gt;% \n  ggplot(aes(x = .fitted, y = .resid)) + \n  geom_point() + \n  geom_hline(yintercept = 0)\n\n\n\n\n\n\n\n# Residual plot using boxes!\nflipper_model_2 %&gt;% \n  ggplot(aes(x = .fitted, y = .resid, group = .fitted)) + \n  geom_boxplot() + \n  geom_hline(yintercept = 0)",
    "crumbs": [
      "Simple Linear Regression - Categorical Predictor"
    ]
  },
  {
    "objectID": "activities/07-slr-cat-predictor.html#exercise-14-really-understand-how-the-coefficients-work-1",
    "href": "activities/07-slr-cat-predictor.html#exercise-14-really-understand-how-the-coefficients-work-1",
    "title": "Simple Linear Regression - Categorical Predictor",
    "section": "Exercise 14: Really understand how the coefficients work",
    "text": "Exercise 14: Really understand how the coefficients work\n\npenguins %&gt;% \n  group_by(island) %&gt;% \n  summarize(mean(flipper_len))\n## # A tibble: 3 √ó 2\n##   island    `mean(flipper_len)`\n##   &lt;fct&gt;                   &lt;dbl&gt;\n## 1 Biscoe                   210.\n## 2 Dream                    193.\n## 3 Torgersen                192.\n\n\nE[flipper_len | island] = 209.56 - 16.37 islandDream - 18.03 islandTorgersen\n\nWhy?\n\nThe island terms are islandDream and islandTorgersen since Biscoe is the reference.\nThe intercept is the average flipper length for the reference, Biscoe.\nThe islandDream coefficient reflects the fact that the average flipper length on Dream is 16.37mm lower than that on Biscoe (193.1870 - 209.5583)\nThe islandTorgersen coefficient reflects the fact that the average flipper length on Dream is 18.03mm lower than that on Biscoe (191.5319 - 209.5583)\n\n\n\n\n\nflipper_model_3 &lt;- lm(flipper_len ~ island, penguins)\nsummary(flipper_model_3)\n## \n## Call:\n## lm(formula = flipper_len ~ island, data = penguins)\n## \n## Residuals:\n##     Min      1Q  Median      3Q     Max \n## -37.558  -5.532   1.468   7.442  21.442 \n## \n## Coefficients:\n##                 Estimate Std. Error t value Pr(&gt;|t|)    \n## (Intercept)      209.558      0.879 238.410   &lt;2e-16 ***\n## islandDream      -16.371      1.340 -12.214   &lt;2e-16 ***\n## islandTorgersen  -18.026      1.858  -9.702   &lt;2e-16 ***\n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n## \n## Residual standard error: 11.22 on 330 degrees of freedom\n## Multiple R-squared:  0.3628, Adjusted R-squared:  0.3589 \n## F-statistic: 93.94 on 2 and 330 DF,  p-value: &lt; 2.2e-16",
    "crumbs": [
      "Simple Linear Regression - Categorical Predictor"
    ]
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "STAT 155: Introduction to Statistical Modeling",
    "section": "",
    "text": "STAT 155: Introduction to Statistical Modeling\nMacalester College, Spring 2026\nStatistics is not just about theories & numbers ‚Äî it‚Äôs about making sense of the world.\n\n\n\n\n\n\nüìñ A Thought from H. G. Wells\n\n\n\n\n‚ÄúStatistical thinking will one day be as necessary for efficient citizenship as the ability to read and write.‚Äù\n\n\n\nWelcome to the world of Statistics! In this course, you‚Äôll learn how to analyze data, test research hypotheses, and make predictions in ways that matter.\nInstructor: Md Mutasim Billah  Class meeting times:\n\nSection 06: M/W/F 09:40-10:40am, THTR 203\nSection 07: M/W/F 10:50-11:50pm, THTR 203\n\nInstructor‚Äôs drop-in (office) hours:\n\nLocation: My office (OLRI 234)\nTimes: M/W: 12:00pm - 12:30pm (in-person), Tuesday/Thursday: 2pm - 3pm (Over zoom, password: 123456)\nBy Appointment: I‚Äôm also happy to meet one-on-one if my normal drop-in hours don‚Äôt work for you. Shoot me an email and we can arrange it over zoom, password: 123456.\nEmail Response Time: I do my best to reply to emails promptly during weekdays. Please note that messages sent after 3:00 pm or on weekends may take longer to receive a response.\n\n\nSTAT 155 Preceptor Office Hours: There is a link to a Google Calendar containing all preceptor office hours available at the top of the course Moodle page!\nR/RStudio Preceptor Office Hours: Available on the MSCS Events google calendar (not to be used for questions about course content, only Data and R-related things!)\n\nThis course website will be updated throughout the semester with new activities, assignments, and announcements, so please bookmark this page if you are enrolled in the course!\nIf you find any typos, bugs, dead links, or have other questions, please email mbillah@macalester.edu"
  },
  {
    "objectID": "schedule.html",
    "href": "schedule.html",
    "title": "Course Schedule",
    "section": "",
    "text": "See the rough schedule of the course, which is subject to change as needed. I will post the updated weekly schedule here. Any urgent announcements will be made over email (moodle)."
  }
]